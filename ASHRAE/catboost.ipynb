{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'catboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import catboost as cbt\n",
    "from sklearn import metrics, base\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import datetime\n",
    "import gc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "from meteocalc import Temp, dew_point, heat_index, wind_chill, feels_like\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "DATA_PATH = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.07 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 73.8%\n"
     ]
    }
   ],
   "source": [
    "building = pd.read_csv(DATA_PATH + 'building_metadata.csv')\n",
    "building = reduce_mem_usage(building,use_float16=True)\n",
    "\n",
    "# def fill_building(df):\n",
    "#     year_built_filler = pd.DataFrame(df.groupby(['site_id'])['year_built'].median(),columns=[\"year_built\"])\n",
    "#     df.update(year_built_filler,overwrite=False)\n",
    "    \n",
    "#     floor_count_filler = pd.DataFrame(df.groupby(['site_id'])['floor_count'].median(),columns=[\"floor_count\"])\n",
    "#     df.update(floor_count_filler,overwrite=False)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# building = fill_building(building)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 616.95 MB\n",
      "Memory usage after optimization is: 173.90 MB\n",
      "Decreased by 71.8%\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "train = reduce_mem_usage(train,use_float16=True)\n",
    "bad_rows = pd.read_csv('../rows_to_drop.csv')\n",
    "train = train.drop(index=bad_rows['0'].values)\n",
    "train = train.merge(building, left_on='building_id',right_on='building_id',how='left')\n",
    "train.loc[(train['meter'] == 0) & (train['site_id'] == 0), 'meter_reading'] = train['meter_reading'] * 0.2931"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_weather_dataset(weather):\n",
    "\n",
    "    zone_dict={0:4,1:0,2:7,3:4,4:7,5:0,6:4,7:4,8:4,9:5,10:7,11:4,12:0,13:5,14:4,15:4} \n",
    "\n",
    "    def set_localtime(df):\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        for sid, zone in zone_dict.items():\n",
    "            sids = df.site_id == sid\n",
    "            df.loc[sids, 'timestamp'] = df[sids].timestamp - pd.offsets.Hour(zone)\n",
    "        df['timestamp'] = df['timestamp'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    set_localtime(weather)\n",
    "    \n",
    "    # Find Missing Dates\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_date = datetime.datetime.strptime(weather['timestamp'].min(),time_format)\n",
    "    end_date = datetime.datetime.strptime(weather['timestamp'].max(),time_format)\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n",
    "    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n",
    "\n",
    "    missing_hours = []\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather[weather['site_id'] == site_id]['timestamp'])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n",
    "        new_rows['site_id'] = site_id\n",
    "        weather = pd.concat([weather,new_rows])\n",
    "\n",
    "        weather = weather.reset_index(drop=True)           \n",
    "\n",
    "    # Add new Features\n",
    "    weather[\"datetime\"] = pd.to_datetime(weather[\"timestamp\"])\n",
    "    weather[\"day\"] = weather[\"datetime\"].dt.day\n",
    "    weather[\"week\"] = weather[\"datetime\"].dt.week\n",
    "    weather[\"month\"] = weather[\"datetime\"].dt.month\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "    weather = weather.set_index(['site_id','day','month'])\n",
    "\n",
    "    air_temperature_filler = pd.DataFrame(weather.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "    weather.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "    cloud_coverage_filler = weather.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "    weather.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "    due_temperature_filler = pd.DataFrame(weather.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "    weather.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "    sea_level_filler = weather.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "    weather.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "    wind_direction_filler =  pd.DataFrame(weather.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "    weather.update(wind_direction_filler,overwrite=False)\n",
    "    \n",
    "    wind_speed_filler =  pd.DataFrame(weather.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "    weather.update(wind_speed_filler,overwrite=False)\n",
    "\n",
    "    precip_depth_filler = weather.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "    weather.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "    weather = weather.reset_index()\n",
    "    weather = weather.drop(['datetime','day','week','month'],axis=1)\n",
    "    \n",
    "    weather.loc[weather['dew_temperature'] > weather['air_temperature'], 'dew_temperature'] = weather['air_temperature']\n",
    "    weather['humidity'] = 100*(np.exp((17.625*weather['dew_temperature'])/(243.04+weather['dew_temperature']))/np.exp((17.625*weather['air_temperature'])/(243.04+weather['air_temperature'])))\n",
    "    \n",
    "    weather['heat_index'] = weather.apply(lambda x: heat_index(temperature=x['air_temperature'], humidity=x['humidity']), axis=1)\n",
    "    weather['heat_index'] = weather['heat_index'].apply(lambda x: round(x, 2))\n",
    "    weather['feels_like'] = weather.apply(lambda x: feels_like(temperature=x['air_temperature'], humidity=x['humidity'], wind_speed=x['wind_speed']), axis=1)\n",
    "    weather['feels_like'] = weather['feels_like'].apply(lambda x: round(x, 2))\n",
    "    \n",
    "    return weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_holiday2(dataframe):\n",
    "    # site 0\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/05/2016')\n",
    "    date2 = pd.date_range(start='3/21/2016', end='3/27/2016')\n",
    "    date3 = pd.date_range(start='11/24/2016', end='11/25/2016')\n",
    "    date4 = pd.date_range(start='12/19/2016',end='12/31/2016')\n",
    "    date5 = pd.date_range(start='1/1/2017', end='1/08/2017')\n",
    "    date6 = pd.date_range(start='3/13/2017', end='3/17/2017')\n",
    "    date7 = pd.date_range(start='11/23/2017', end='11/24/2017')\n",
    "    date8 = pd.date_range(start='12/19/2017',end='12/31/2017')\n",
    "    date9 = pd.date_range(start='1/1/2018', end='1/07/2018')\n",
    "    date10 = pd.date_range(start='3/12/2018', end='3/18/2018')\n",
    "    date11 = pd.date_range(start='11/22/2018', end='11/23/2018')\n",
    "    date12 = pd.date_range(start='12/19/2018',end='12/31/2018')\n",
    "    site0_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12)\n",
    "    sids = dataframe.site_id == 0\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site0_hol))), 'is_holiday'] = 1\n",
    "    # site 1\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/03/2016')\n",
    "    date2 = pd.date_range(start='3/25/2016', end='3/30/2016')\n",
    "    date3 = pd.date_range(start='12/24/2016',end='12/31/2016')\n",
    "    date4 = pd.date_range(start='1/1/2017', end='1/02/2017')\n",
    "    date5 = pd.date_range(start='3/13/2017', end='3/19/2017')\n",
    "    date6 = pd.date_range(start='12/23/2017',end='12/31/2017')\n",
    "    date7 = pd.date_range(start='3/29/2018', end='4/04/2018')\n",
    "    date8 = pd.date_range(start='12/22/2018',end='12/31/2018')\n",
    "    site1_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8)\n",
    "    sids = dataframe.site_id == 1\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site1_hol))), 'is_holiday'] = 1\n",
    "    # site 2\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/12/2016')\n",
    "    date2 = pd.date_range(start='3/12/2016', end='3/20/2016')\n",
    "    date3 = pd.date_range(start='11/24/2016', end='11/27/2016')\n",
    "    date4 = pd.date_range(start='12/26/2016',end='12/27/2016')\n",
    "    date5 = pd.date_range(start='1/1/2017', end='1/9/2017')\n",
    "    date6 = pd.date_range(start='3/11/2017', end='3/19/2017')\n",
    "    date7 = pd.date_range(start='11/23/2017', end='11/26/2017')\n",
    "    date8 = pd.date_range(start='12/25/2017',end='12/26/2017')\n",
    "    date9 = pd.date_range(start='1/1/2018', end='1/09/2018')\n",
    "    date10 = pd.date_range(start='3/5/2018', end='3/9/2018')\n",
    "    date11 = pd.date_range(start='11/22/2018', end='11/25/2018')\n",
    "    date12 = pd.date_range(start='12/24/2018',end='12/25/2018')\n",
    "    site2_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12)\n",
    "    sids = dataframe.site_id == 2\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site2_hol))), 'is_holiday'] = 1\n",
    "    # site 4\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/11/2016')\n",
    "    date2 = pd.date_range(start='3/21/2016', end='3/25/2016')\n",
    "    date3 = pd.date_range(start='11/24/2016', end='11/25/2016')\n",
    "    date4 = pd.date_range(start='12/26/2016',end='12/27/2016')\n",
    "    date5 = pd.date_range(start='1/1/2017', end='1/9/2017')\n",
    "    date6 = pd.date_range(start='3/27/2017', end='3/31/2017')\n",
    "    date7 = pd.date_range(start='11/23/2017', end='11/26/2017')\n",
    "    date8 = pd.date_range(start='12/25/2017',end='12/26/2017')\n",
    "    date9 = pd.date_range(start='12/29/2017',end='12/29/2017')\n",
    "    date10 = pd.date_range(start='1/1/2018', end='1/08/2018')\n",
    "    date11 = pd.date_range(start='3/26/2018', end='3/30/2018')\n",
    "    date12 = pd.date_range(start='11/22/2018', end='11/23/2018')\n",
    "    date13 = pd.date_range(start='12/24/2018',end='12/25/2018')\n",
    "    date14 = pd.date_range(start='12/31/2018',end='12/31/2018')\n",
    "    site4_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12).union(date13).union(date14)\n",
    "    sids = dataframe.site_id == 4\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site4_hol))), 'is_holiday'] = 1\n",
    "    # site 5\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/3/2016')\n",
    "    date2 = pd.date_range(start='3/19/2016', end='4/17/2016')\n",
    "    date3 = pd.date_range(start='6/11/2016', end='8/21/2016')\n",
    "    date4 = pd.date_range(start='1/1/2017',end='1/8/2017')\n",
    "    date5 = pd.date_range(start='3/23/2017', end='4/23/2017')\n",
    "    date6 = pd.date_range(start='6/17/2017', end='8/20/2017')\n",
    "    date7 = pd.date_range(start='1/1/2018',end='1/7/2018')\n",
    "    date8 = pd.date_range(start='3/17/2018', end='4/15/2018')\n",
    "    date9 = pd.date_range(start='6/16/2018', end='8/19/2018')\n",
    "    site5_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9)\n",
    "    sids = dataframe.site_id == 5\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site5_hol))), 'is_holiday'] = 1\n",
    "    # site 7&11\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/06/2016')\n",
    "    date2 = pd.date_range(start='2/29/2016', end='3/4/2016')\n",
    "    date3 = pd.date_range(start='11/24/2016', end='11/25/2016')\n",
    "    date4 = pd.date_range(start='12/23/2016',end='12/31/2016')\n",
    "    date5 = pd.date_range(start='1/1/2017', end='1/03/2017')\n",
    "    date6 = pd.date_range(start='2/27/2017', end='3/3/2017')\n",
    "    date7 = pd.date_range(start='11/23/2017', end='11/24/2017')\n",
    "    date8 = pd.date_range(start='12/23/2017',end='12/31/2017')\n",
    "    date9 = pd.date_range(start='1/1/2018', end='1/07/2018')\n",
    "    date10 = pd.date_range(start='3/5/2018', end='3/9/2018')\n",
    "    date11 = pd.date_range(start='11/22/2018', end='11/23/2018')\n",
    "    date12 = pd.date_range(start='12/24/2018',end='12/31/2018')\n",
    "    site_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12)\n",
    "    sids = dataframe.site_id == 7\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site_hol))), 'is_holiday'] = 1\n",
    "    sids = dataframe.site_id == 11\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site_hol))), 'is_holiday'] = 1\n",
    "    # site 9\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/06/2016')\n",
    "    date2 = pd.date_range(start='3/14/2016', end='3/19/2016')\n",
    "    date3 = pd.date_range(start='11/23/2016', end='11/26/2016')\n",
    "    date4 = pd.date_range(start='12/23/2016',end='12/31/2016')\n",
    "    date5 = pd.date_range(start='1/1/2017', end='1/04/2017')\n",
    "    date6 = pd.date_range(start='3/13/2017', end='3/18/2017')\n",
    "    date7 = pd.date_range(start='11/22/2017', end='11/25/2017')\n",
    "    date8 = pd.date_range(start='12/23/2017',end='12/31/2017')\n",
    "    date9 = pd.date_range(start='1/1/2018', end='1/03/2018')\n",
    "    date10 = pd.date_range(start='3/12/2018', end='3/17/2018')\n",
    "    date11 = pd.date_range(start='11/21/2018', end='11/24/2018')\n",
    "    date12 = pd.date_range(start='12/24/2018',end='12/31/2018')\n",
    "    site9_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12)\n",
    "    sids = dataframe.site_id == 9\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site9_hol))), 'is_holiday'] = 1\n",
    "    # site 10\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/10/2016')\n",
    "    date2 = pd.date_range(start='3/5/2016', end='3/13/2016')\n",
    "    date3 = pd.date_range(start='11/24/2016', end='11/25/2016')\n",
    "    date4 = pd.date_range(start='12/23/2016',end='12/31/2016')\n",
    "    date5 = pd.date_range(start='1/1/2017', end='1/08/2017')\n",
    "    date6 = pd.date_range(start='3/4/2017', end='3/12/2017')\n",
    "    date7 = pd.date_range(start='11/23/2017', end='11/24/2017')\n",
    "    date8 = pd.date_range(start='12/24/2017',end='12/31/2017')\n",
    "    date9 = pd.date_range(start='1/1/2018', end='1/07/2018')\n",
    "    date10 = pd.date_range(start='3/3/2018', end='3/11/2018')\n",
    "    date11 = pd.date_range(start='11/22/2018', end='11/23/2018')\n",
    "    date12 = pd.date_range(start='12/24/2018',end='12/31/2018')\n",
    "    site10_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12)\n",
    "    sids = dataframe.site_id == 10\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site10_hol))), 'is_holiday'] = 1\n",
    "    # site 13\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/12/2016')\n",
    "    date2 = pd.date_range(start='3/7/2016', end='3/11/2016')\n",
    "    date3 = pd.date_range(start='10/27/2016', end='10/28/2016')\n",
    "    date4 = pd.date_range(start='11/24/2016', end='11/25/2016')\n",
    "    date5 = pd.date_range(start='12/23/2016',end='12/30/2016')\n",
    "    date6 = pd.date_range(start='1/1/2017',end='1/10/2017')\n",
    "    date7 = pd.date_range(start='3/6/2017', end='3/10/2017')\n",
    "    date8 = pd.date_range(start='10/26/2017', end='10/27/2017')\n",
    "    date9 = pd.date_range(start='11/23/2017', end='11/24/2017')\n",
    "    date10 = pd.date_range(start='12/25/2017',end='12/29/2017')\n",
    "    date11 = pd.date_range(start='1/1/2018',end='1/9/2018')\n",
    "    date12 = pd.date_range(start='3/5/2018', end='3/9/2018')\n",
    "    date13 = pd.date_range(start='10/25/2018', end='10/26/2018')\n",
    "    date14 = pd.date_range(start='11/22/2018', end='11/23/2018')\n",
    "    date15 = pd.date_range(start='12/25/2018',end='12/26/2018')\n",
    "    date16 = pd.date_range(start='12/31/2018',end='12/31/2018')\n",
    "    site13_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12).union(date13).union(date14).union(date15).union(date16)\n",
    "    sids = dataframe.site_id == 13\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site13_hol))), 'is_holiday'] = 1\n",
    "    # site 14\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/03/2016')\n",
    "    date2 = pd.date_range(start='3/5/2016', end='3/13/2016')\n",
    "    date3 = pd.date_range(start='11/23/2016', end='11/27/2016')\n",
    "    date4 = pd.date_range(start='1/1/2017', end='1/02/2017')\n",
    "    date5 = pd.date_range(start='3/4/2017', end='3/12/2017')\n",
    "    date6 = pd.date_range(start='11/22/2017', end='11/26/2017')\n",
    "    date7 = pd.date_range(start='1/1/2018', end='1/02/2018')\n",
    "    date8 = pd.date_range(start='3/3/2018', end='3/11/2018')\n",
    "    date9 = pd.date_range(start='11/21/2018', end='11/25/2018')\n",
    "    site14_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9)\n",
    "    sids = dataframe.site_id == 14\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site14_hol))), 'is_holiday'] = 1\n",
    "    # site 15\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/03/2016')\n",
    "    date2 = pd.date_range(start='2/13/2016',end='2/17/2016')\n",
    "    date3 = pd.date_range(start='3/26/2016', end='4/3/2016')\n",
    "    date4 = pd.date_range(start='10/8/2016', end='10/11/2016')\n",
    "    date5 = pd.date_range(start='11/23/2016', end='11/27/2016')\n",
    "    date6 = pd.date_range(start='12/26/2016',end='12/31/2016')\n",
    "    date7 = pd.date_range(start='1/1/2017',end='1/02/2017')\n",
    "    date8 = pd.date_range(start='2/18/2017',end='2/21/2017')\n",
    "    date9 = pd.date_range(start='4/1/2017', end='4/9/2017')\n",
    "    date10 = pd.date_range(start='10/7/2017', end='10/10/2017')\n",
    "    date11 = pd.date_range(start='11/22/2017', end='11/26/2017')\n",
    "    date12 = pd.date_range(start='12/25/2017',end='12/31/2017')\n",
    "    date13 = pd.date_range(start='2/17/2018',end='2/20/2018')\n",
    "    date14 = pd.date_range(start='3/31/2018', end='4/8/2018')\n",
    "    date15 = pd.date_range(start='10/6/2018', end='10/9/2018')\n",
    "    date16 = pd.date_range(start='11/21/2018', end='11/25/2018')\n",
    "    date17 = pd.date_range(start='12/24/2018',end='12/31/2018')\n",
    "    site15_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12).union(date13).union(date14).union(date15).union(date16).union(date17)\n",
    "    sids = dataframe.site_id == 15\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site15_hol))), 'is_holiday'] = 1\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_engineering(df):\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df.sort_values(\"timestamp\")\n",
    "    df.reset_index(drop=True)\n",
    "    \n",
    "    # Add more features\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "    df[\"dayofweek\"] = df[\"timestamp\"].dt.weekday\n",
    "    \n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['month'].replace((1, 2, 3, 12), 1, inplace = True)\n",
    "    df['month'].replace((6, 7, 8, 9), 3, inplace = True)\n",
    "    df['month'].replace((10, 11, 4, 5), 2, inplace = True)\n",
    "    \n",
    "    us_holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "                  \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "                  \"2017-01-02\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "                  \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "                  \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "                  \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "                  \"2019-01-01\"]\n",
    "    uk_holidays = [\"2016-01-01\", \"2016-01-02\", \"2016-03-25\", \"2016-03-28\", \"2016-05-02\", \n",
    "                   \"2016-05-30\", \"2016-08-29\", \"2016-12-25\", \"2016-12-26\", \"2016-12-27\", \n",
    "                   \"2017-01-01\", \"2017-01-02\", \"2017-04-14\", \"2017-04-17\", \"2017-05-01\", \n",
    "                   \"2017-05-29\", \"2017-08-28\", \"2017-12-25\", \"2017-12-26\", \n",
    "                   \"2018-01-01\", \"2018-01-02\", \"2018-03-30\", \"2018-04-02\", \"2018-05-07\", \n",
    "                   \"2018-05-28\", \"2018-08-27\", \"2018-12-25\", \"2018-12-26\",\n",
    "                   \"2019-01-01\"]\n",
    "    ir_holidays = [\"2016-01-01\", \"2016-01-02\", \"2016-03-17\", \"2016-03-28\", \"2016-05-02\", \n",
    "                   \"2016-06-02\", \"2016-08-01\", \"2016-10-31\", \"2016-12-26\", \"2016-12-27\", \n",
    "                   \"2017-01-01\", \"2017-01-02\", \"2017-03-17\", \"2017-04-14\", \"2017-05-01\", \n",
    "                   \"2017-06-05\", \"2017-08-07\", \"2017-10-30\", \"2017-12-25\", \"2017-12-26\", \n",
    "                   \"2018-01-01\", \"2018-01-02\", \"2018-03-19\", \"2018-04-02\", \"2018-05-07\", \n",
    "                   \"2018-06-04\", \"2018-08-06\", \"2018-10-29\", \"2018-12-25\", \"2018-12-26\",\n",
    "                   \"2019-01-01\"]\n",
    "\n",
    "    def set_holiday():\n",
    "        df[\"is_holiday\"] = 0\n",
    "        df.loc[df.dayofweek.isin([5,6]), 'is_holiday'] = 1\n",
    "        us_zone = [0,2,3,4,6,7,8,9,10,11,13,14,15]\n",
    "        for sid in us_zone:\n",
    "            sids = df.site_id == sid\n",
    "            df.loc[(sids) & (df.timestamp.isin(pd.to_datetime(us_holidays))), 'is_holiday'] = 1\n",
    "        uk_zone = [1,5]\n",
    "        for sid in uk_zone:\n",
    "            sids = df.site_id == sid\n",
    "            df.loc[(sids) & (df.timestamp.isin(pd.to_datetime(uk_holidays))), 'is_holiday'] = 1\n",
    "        sids = df.site_id == 12\n",
    "        df.loc[(sids) & (df.timestamp.isin(pd.to_datetime(ir_holidays))), 'is_holiday'] = 1\n",
    "        return df\n",
    "    \n",
    "    df = set_holiday()\n",
    "    df = set_holiday2(df)\n",
    "    df['square_feet'] =  np.log1p(df['square_feet'])\n",
    "    \n",
    "    # Remove Unused Columns\n",
    "    drop = [\"timestamp\",\"sea_level_pressure\",\"wind_direction\",\"wind_speed\",\"year_built\",\"floor_count\"]\n",
    "    df = df.drop(drop, axis=1)\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Weather Information\n",
    "\n",
    "I'm using [this kernel](https://www.kaggle.com/aitude/ashrae-missing-weather-data-handling) to handle missing weather information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 12.88 MB\n",
      "Memory usage after optimization is: 3.46 MB\n",
      "Decreased by 73.1%\n"
     ]
    }
   ],
   "source": [
    "weather = pd.read_csv(DATA_PATH + 'weather_train.csv')\n",
    "weather = fill_weather_dataset(weather)\n",
    "weather = reduce_mem_usage(weather,use_float16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df, window):\n",
    "    df['ave_temp'] = df['air_temperature']/2 + df['dew_temperature']/2\n",
    "    \n",
    "    feature_cols = [\"air_temperature\", \"dew_temperature\", \"cloud_coverage\", \"precip_depth_1_hr\",\"ave_temp\",\n",
    "                    \"humidity\",'heat_index']\n",
    "    df_site = df.groupby(\"site_id\")\n",
    "\n",
    "    df_rolled = df_site[feature_cols].rolling(window=window, min_periods=0)\n",
    "\n",
    "    df_mean = df_rolled.mean().reset_index().astype(np.float16)\n",
    "    df_min = df_rolled.min().reset_index().astype(np.float16)\n",
    "    df_std = df_rolled.std().reset_index().astype(np.float16)\n",
    "    df_var = df_rolled.var().reset_index().astype(np.float16)\n",
    "\n",
    "    for feature in feature_cols:\n",
    "        df[f\"{feature}_mean_lag{window}\"] = df_mean[feature]\n",
    "        df[f\"{feature}_min_lag{window}\"] = df_min[feature]\n",
    "        df[f\"{feature}_std_lag{window}\"] = df_std[feature]\n",
    "        \n",
    "    for feature in feature_cols:\n",
    "        df[f\"{feature}_diff1\"] = df[feature].diff(1) \n",
    "        \n",
    "    return df\n",
    "\n",
    "weather = create_lag_features(weather, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(weather,how='left',left_on=['site_id','timestamp'],right_on=['site_id','timestamp'])\n",
    "del weather\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = features_engineering(train)\n",
    "\n",
    "meter_encodage={0:0,3:1,1:2,2:3}\n",
    "train['meter']=[meter_encodage[c] for c in train['meter']]\n",
    "encodage={'Religious worship':1,'Warehouse/storage':2,'Technology/science':3,'Other':4,'Retail':5,'Parking':6,\\\n",
    "         'Lodging/residential':7,'Manufacturing/industrial':8,'Public services':9,'Food sales and service':10,\\\n",
    "         'Entertainment/public assembly':11,'Utility':12,'Office':13,'Healthcare':14,'Services':15,\\\n",
    "         'Education':16}\n",
    "train['primary_use']=[encodage[c] for c in train['primary_use']]\n",
    "train[\"puse_hour\"] = train[\"primary_use\"].astype(str) + '_' + train[\"hour\"].astype(str)\n",
    "train[\"holiday_hour\"] = train[\"is_holiday\"].astype(str) + '_' + train[\"hour\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_categorical_encoders(train, features):\n",
    "    encoders = {}\n",
    "    for feature in features:\n",
    "        train[feature] = train[feature].fillna('missing')\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(train[feature].values)\n",
    "        le_dict = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "        encoders[feature] = le_dict\n",
    "    return encoders\n",
    "\n",
    "def encode_categorical_features(df, features, encoders):\n",
    "    for f in features:\n",
    "        df[f] = df[f].fillna('missing')\n",
    "        df[f] = df[f].map(encoders[f])\n",
    "\n",
    "encoded_features = [\"puse_hour\",\"holiday_hour\"] \n",
    "encoders = generate_categorical_encoders(train, encoded_features)\n",
    "encode_categorical_features(train, encoded_features, encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features & Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.log1p(train[\"meter_reading\"])\n",
    "features = train.drop('meter_reading', axis = 1)\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  KFOLD LIGHTGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "0:\tlearn: 4.5196859\ttest: 4.5479451\tbest: 4.5479451 (0)\ttotal: 9.94s\tremaining: 1d 3h 36m 38s\n",
      "50:\tlearn: 1.0101499\ttest: 1.0683300\tbest: 1.0683300 (50)\ttotal: 7m 43s\tremaining: 1d 1h 6m 51s\n",
      "100:\tlearn: 0.8732475\ttest: 0.8973422\tbest: 0.8973422 (100)\ttotal: 13m 48s\tremaining: 22h 32m 58s\n",
      "150:\tlearn: 0.8301929\ttest: 0.8562147\tbest: 0.8562147 (150)\ttotal: 19m 40s\tremaining: 21h 23m 33s\n",
      "200:\tlearn: 0.8006051\ttest: 0.8342902\tbest: 0.8342902 (200)\ttotal: 25m 11s\tremaining: 20h 28m 21s\n",
      "250:\tlearn: 0.7804377\ttest: 0.8201181\tbest: 0.8201181 (250)\ttotal: 30m 51s\tremaining: 19h 58m 45s\n",
      "300:\tlearn: 0.7642053\ttest: 0.8090940\tbest: 0.8090940 (300)\ttotal: 36m 26s\tremaining: 19h 34m 13s\n",
      "350:\tlearn: 0.7498673\ttest: 0.8005068\tbest: 0.8005068 (350)\ttotal: 42m 16s\tremaining: 19h 22m 14s\n",
      "400:\tlearn: 0.7385512\ttest: 0.7949244\tbest: 0.7949244 (400)\ttotal: 48m 2s\tremaining: 19h 10m 9s\n",
      "450:\tlearn: 0.7287592\ttest: 0.7897310\tbest: 0.7897168 (449)\ttotal: 53m 37s\tremaining: 18h 55m 19s\n",
      "500:\tlearn: 0.7198239\ttest: 0.7864006\tbest: 0.7864006 (500)\ttotal: 59m 32s\tremaining: 18h 48m 51s\n",
      "550:\tlearn: 0.7124897\ttest: 0.7829561\tbest: 0.7829561 (550)\ttotal: 1h 5m 23s\tremaining: 18h 41m 27s\n",
      "600:\tlearn: 0.7048815\ttest: 0.7796343\tbest: 0.7796343 (600)\ttotal: 1h 11m 15s\tremaining: 18h 34m 28s\n",
      "650:\tlearn: 0.6979272\ttest: 0.7771385\tbest: 0.7771385 (650)\ttotal: 1h 17m 23s\tremaining: 18h 31m 21s\n",
      "700:\tlearn: 0.6927053\ttest: 0.7752954\tbest: 0.7752415 (699)\ttotal: 1h 23m 19s\tremaining: 18h 25m 24s\n",
      "750:\tlearn: 0.6877482\ttest: 0.7731110\tbest: 0.7731110 (750)\ttotal: 1h 28m 58s\tremaining: 18h 15m 43s\n",
      "800:\tlearn: 0.6821154\ttest: 0.7710627\tbest: 0.7710627 (800)\ttotal: 1h 34m 48s\tremaining: 18h 8m 50s\n",
      "850:\tlearn: 0.6775328\ttest: 0.7701792\tbest: 0.7701792 (850)\ttotal: 1h 41m 5s\tremaining: 18h 6m 44s\n",
      "900:\tlearn: 0.6734874\ttest: 0.7687990\tbest: 0.7687990 (900)\ttotal: 1h 47m 20s\tremaining: 18h 3m 59s\n",
      "950:\tlearn: 0.6694558\ttest: 0.7679072\tbest: 0.7679072 (950)\ttotal: 1h 53m 35s\tremaining: 18h 55s\n",
      "1000:\tlearn: 0.6656316\ttest: 0.7668234\tbest: 0.7668234 (1000)\ttotal: 1h 59m 44s\tremaining: 17h 56m 25s\n",
      "1050:\tlearn: 0.6616157\ttest: 0.7656159\tbest: 0.7656159 (1050)\ttotal: 2h 5m 29s\tremaining: 17h 48m 34s\n",
      "1100:\tlearn: 0.6578549\ttest: 0.7649101\tbest: 0.7647000 (1094)\ttotal: 2h 12m 6s\tremaining: 17h 47m 44s\n",
      "1150:\tlearn: 0.6545117\ttest: 0.7641176\tbest: 0.7641176 (1150)\ttotal: 2h 18m\tremaining: 17h 41m 3s\n",
      "1200:\tlearn: 0.6514880\ttest: 0.7632796\tbest: 0.7632755 (1199)\ttotal: 2h 23m 36s\tremaining: 17h 32m 5s\n",
      "1250:\tlearn: 0.6485352\ttest: 0.7629766\tbest: 0.7628606 (1247)\ttotal: 2h 29m 28s\tremaining: 17h 25m 19s\n",
      "1300:\tlearn: 0.6459388\ttest: 0.7627147\tbest: 0.7625786 (1278)\ttotal: 2h 34m 58s\tremaining: 17h 16m 10s\n",
      "1350:\tlearn: 0.6432795\ttest: 0.7620574\tbest: 0.7620574 (1350)\ttotal: 2h 40m 34s\tremaining: 17h 8m\n",
      "1400:\tlearn: 0.6406918\ttest: 0.7619127\tbest: 0.7619127 (1400)\ttotal: 2h 46m 18s\tremaining: 17h 45s\n",
      "1450:\tlearn: 0.6379113\ttest: 0.7614628\tbest: 0.7614403 (1448)\ttotal: 2h 51m 55s\tremaining: 16h 52m 57s\n",
      "1500:\tlearn: 0.6353988\ttest: 0.7611598\tbest: 0.7611598 (1500)\ttotal: 2h 57m 28s\tremaining: 16h 44m 55s\n",
      "1550:\tlearn: 0.6331634\ttest: 0.7607823\tbest: 0.7607823 (1550)\ttotal: 3h 2m 58s\tremaining: 16h 36m 43s\n",
      "1600:\tlearn: 0.6309348\ttest: 0.7606399\tbest: 0.7606117 (1595)\ttotal: 3h 8m 27s\tremaining: 16h 28m 40s\n",
      "1650:\tlearn: 0.6288398\ttest: 0.7603975\tbest: 0.7603975 (1650)\ttotal: 3h 13m 58s\tremaining: 16h 20m 54s\n",
      "1700:\tlearn: 0.6269000\ttest: 0.7601296\tbest: 0.7601296 (1700)\ttotal: 3h 19m 7s\tremaining: 16h 11m 32s\n",
      "1750:\tlearn: 0.6247129\ttest: 0.7599090\tbest: 0.7598270 (1732)\ttotal: 3h 24m 25s\tremaining: 16h 3m 1s\n",
      "1800:\tlearn: 0.6226772\ttest: 0.7596944\tbest: 0.7596944 (1800)\ttotal: 3h 29m 58s\tremaining: 15h 55m 54s\n",
      "1850:\tlearn: 0.6206715\ttest: 0.7596744\tbest: 0.7596543 (1849)\ttotal: 3h 35m 32s\tremaining: 15h 48m 54s\n",
      "1900:\tlearn: 0.6187984\ttest: 0.7597045\tbest: 0.7596258 (1851)\ttotal: 3h 40m 52s\tremaining: 15h 41m\n",
      "bestTest = 0.7596257645\n",
      "bestIteration = 1851\n",
      "Shrink model to first 1852 iterations.\n",
      "fold 1\n",
      "0:\tlearn: 4.5313059\ttest: 4.4899890\tbest: 4.4899890 (0)\ttotal: 8.11s\tremaining: 22h 30m 46s\n",
      "50:\tlearn: 0.9971234\ttest: 1.0198882\tbest: 1.0198882 (50)\ttotal: 6m 43s\tremaining: 21h 51m 57s\n",
      "100:\tlearn: 0.8575738\ttest: 0.9107018\tbest: 0.9107018 (100)\ttotal: 12m 9s\tremaining: 19h 51m 41s\n",
      "150:\tlearn: 0.8160244\ttest: 0.8853413\tbest: 0.8853413 (150)\ttotal: 17m 33s\tremaining: 19h 5m 35s\n",
      "200:\tlearn: 0.7867804\ttest: 0.8682530\tbest: 0.8682530 (200)\ttotal: 22m 46s\tremaining: 18h 30m 6s\n",
      "250:\tlearn: 0.7655050\ttest: 0.8607013\tbest: 0.8607013 (250)\ttotal: 28m 10s\tremaining: 18h 14m 31s\n",
      "300:\tlearn: 0.7475887\ttest: 0.8540054\tbest: 0.8535504 (296)\ttotal: 34m\tremaining: 18h 15m 54s\n",
      "350:\tlearn: 0.7340020\ttest: 0.8497704\tbest: 0.8497704 (350)\ttotal: 39m 37s\tremaining: 18h 9m 15s\n",
      "400:\tlearn: 0.7213964\ttest: 0.8469026\tbest: 0.8467174 (385)\ttotal: 45m 16s\tremaining: 18h 3m 48s\n",
      "450:\tlearn: 0.7110040\ttest: 0.8458262\tbest: 0.8457607 (423)\ttotal: 51m 19s\tremaining: 18h 6m 39s\n",
      "500:\tlearn: 0.7024550\ttest: 0.8440393\tbest: 0.8440393 (500)\ttotal: 57m 11s\tremaining: 18h 4m 29s\n",
      "550:\tlearn: 0.6944118\ttest: 0.8431205\tbest: 0.8431060 (517)\ttotal: 1h 2m 59s\tremaining: 18h 22s\n",
      "600:\tlearn: 0.6872483\ttest: 0.8411865\tbest: 0.8411865 (600)\ttotal: 1h 8m 33s\tremaining: 17h 52m 18s\n",
      "650:\tlearn: 0.6806684\ttest: 0.8399905\tbest: 0.8399272 (635)\ttotal: 1h 14m 15s\tremaining: 17h 46m 25s\n",
      "700:\tlearn: 0.6750475\ttest: 0.8396589\tbest: 0.8390150 (678)\ttotal: 1h 19m 56s\tremaining: 17h 40m 24s\n",
      "bestTest = 0.8390150086\n",
      "bestIteration = 678\n",
      "Shrink model to first 679 iterations.\n",
      "fold 2\n",
      "0:\tlearn: 4.5175366\ttest: 4.5568486\tbest: 4.5568486 (0)\ttotal: 7.97s\tremaining: 22h 8m 33s\n",
      "50:\tlearn: 0.9975689\ttest: 1.0810449\tbest: 1.0810449 (50)\ttotal: 6m 59s\tremaining: 22h 45m 32s\n",
      "100:\tlearn: 0.8624247\ttest: 0.9279112\tbest: 0.9279112 (100)\ttotal: 12m 33s\tremaining: 20h 30m 15s\n",
      "150:\tlearn: 0.8224329\ttest: 0.8897133\tbest: 0.8897133 (150)\ttotal: 18m 9s\tremaining: 19h 44m 31s\n",
      "200:\tlearn: 0.7943664\ttest: 0.8666710\tbest: 0.8666710 (200)\ttotal: 23m 34s\tremaining: 19h 9m 40s\n",
      "250:\tlearn: 0.7743477\ttest: 0.8519420\tbest: 0.8519420 (250)\ttotal: 29m 4s\tremaining: 18h 49m 20s\n",
      "300:\tlearn: 0.7586560\ttest: 0.8423742\tbest: 0.8423742 (300)\ttotal: 35m\tremaining: 18h 47m 59s\n",
      "350:\tlearn: 0.7452542\ttest: 0.8342946\tbest: 0.8342920 (349)\ttotal: 41m\tremaining: 18h 47m 17s\n",
      "400:\tlearn: 0.7339055\ttest: 0.8283114\tbest: 0.8283114 (400)\ttotal: 46m 53s\tremaining: 18h 42m 20s\n",
      "450:\tlearn: 0.7236621\ttest: 0.8217891\tbest: 0.8217891 (450)\ttotal: 52m 53s\tremaining: 18h 39m 51s\n",
      "500:\tlearn: 0.7147731\ttest: 0.8173107\tbest: 0.8173107 (500)\ttotal: 58m 55s\tremaining: 18h 37m 21s\n",
      "550:\tlearn: 0.7062404\ttest: 0.8120199\tbest: 0.8120199 (550)\ttotal: 1h 5m 13s\tremaining: 18h 38m 27s\n",
      "600:\tlearn: 0.6987743\ttest: 0.8080001\tbest: 0.8079720 (599)\ttotal: 1h 11m 6s\tremaining: 18h 32m\n",
      "650:\tlearn: 0.6919311\ttest: 0.8047870\tbest: 0.8047870 (650)\ttotal: 1h 17m\tremaining: 18h 26m 1s\n",
      "700:\tlearn: 0.6860842\ttest: 0.8020383\tbest: 0.8020383 (700)\ttotal: 1h 22m 48s\tremaining: 18h 18m 32s\n",
      "750:\tlearn: 0.6808908\ttest: 0.8000968\tbest: 0.8000968 (750)\ttotal: 1h 28m 44s\tremaining: 18h 12m 56s\n",
      "800:\tlearn: 0.6758268\ttest: 0.7975308\tbest: 0.7975308 (800)\ttotal: 1h 34m 50s\tremaining: 18h 9m 15s\n",
      "850:\tlearn: 0.6711670\ttest: 0.7954787\tbest: 0.7954787 (850)\ttotal: 1h 40m 46s\tremaining: 18h 3m 26s\n",
      "900:\tlearn: 0.6670322\ttest: 0.7934264\tbest: 0.7934264 (900)\ttotal: 1h 46m 45s\tremaining: 17h 58m 12s\n",
      "950:\tlearn: 0.6630041\ttest: 0.7919248\tbest: 0.7919248 (950)\ttotal: 1h 52m 27s\tremaining: 17h 50m\n",
      "1000:\tlearn: 0.6589450\ttest: 0.7910346\tbest: 0.7909309 (994)\ttotal: 1h 58m 12s\tremaining: 17h 42m 37s\n",
      "1050:\tlearn: 0.6546054\ttest: 0.7896478\tbest: 0.7896478 (1050)\ttotal: 2h 4m 6s\tremaining: 17h 36m 49s\n",
      "1100:\tlearn: 0.6507165\ttest: 0.7879816\tbest: 0.7879816 (1100)\ttotal: 2h 10m 12s\tremaining: 17h 32m 23s\n",
      "1150:\tlearn: 0.6475010\ttest: 0.7868767\tbest: 0.7868739 (1149)\ttotal: 2h 15m 42s\tremaining: 17h 23m 21s\n",
      "1200:\tlearn: 0.6443390\ttest: 0.7859247\tbest: 0.7859247 (1200)\ttotal: 2h 21m 50s\tremaining: 17h 19m 9s\n",
      "1250:\tlearn: 0.6413372\ttest: 0.7848341\tbest: 0.7848335 (1248)\ttotal: 2h 27m 13s\tremaining: 17h 9m 36s\n",
      "1300:\tlearn: 0.6383476\ttest: 0.7837075\tbest: 0.7837075 (1300)\ttotal: 2h 32m 54s\tremaining: 17h 2m 25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350:\tlearn: 0.6357203\ttest: 0.7829199\tbest: 0.7829133 (1349)\ttotal: 2h 38m 17s\tremaining: 16h 53m 23s\n",
      "1400:\tlearn: 0.6330179\ttest: 0.7824025\tbest: 0.7824025 (1400)\ttotal: 2h 43m 57s\tremaining: 16h 46m 20s\n",
      "1450:\tlearn: 0.6304312\ttest: 0.7818164\tbest: 0.7818164 (1450)\ttotal: 2h 49m 19s\tremaining: 16h 37m 38s\n",
      "1500:\tlearn: 0.6279402\ttest: 0.7808969\tbest: 0.7808969 (1500)\ttotal: 2h 54m 52s\tremaining: 16h 30m 11s\n",
      "1550:\tlearn: 0.6255904\ttest: 0.7804038\tbest: 0.7804038 (1550)\ttotal: 3h 19s\tremaining: 16h 22m 19s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-636d75b2e71f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                   \u001b[0mmax_leaves\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1280\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_data_in_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m220\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_bin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                   early_stopping_rounds=50,eval_metric='RMSE',cat_features=categorical_features)\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpu/lib/python3.7/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   3359\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                          save_snapshot, snapshot_file, snapshot_interval, init_model)\n\u001b[0m\u001b[1;32m   3362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpu/lib/python3.7/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_oblivious\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpu/lib/python3.7/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "categorical_features = [\"meter\",\"primary_use\",\"building_id\",\"site_id\",\"dayofweek\",\"hour\",\"puse_hour\",\"holiday_hour\"] \n",
    "\n",
    "folds = 5\n",
    "kf = KFold(n_splits=folds)\n",
    "# kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "models, scores = [], []\n",
    "val_score = np.zeros(len(features))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(kf.split(features,features['month'])):  #\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    train_features = features.loc[trn_idx]\n",
    "    train_target = target.loc[trn_idx]\n",
    "    \n",
    "    test_features = features.loc[val_idx]\n",
    "    test_target = target.loc[val_idx]\n",
    "    \n",
    "    model = cbt.CatBoostRegressor(iterations=10000,learning_rate=0.05,max_depth=12,verbose=50,task_type='GPU',\n",
    "                                  max_leaves=1280,min_data_in_leaf=220,max_bin=255,\n",
    "                                  early_stopping_rounds=50,eval_metric='RMSE',cat_features=categorical_features)\n",
    "    model.fit(train_features, train_target, eval_set=(test_features,test_target))\n",
    "    models.append(model)\n",
    "    \n",
    "    val_score[val_idx] = model.predict(test_features)\n",
    "    \n",
    "    del train_features, train_target, test_features, test_target\n",
    "    gc.collect()\n",
    "    \n",
    "print(np.sqrt(mean_squared_error(val_score, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8153449159626204\n",
    "0.8150488208941825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del features, target, val_score, bad_rows, scores\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(DATA_PATH + 'weather_test.csv')\n",
    "weather = fill_weather_dataset(weather)\n",
    "weather = reduce_mem_usage(weather, use_float16 = True)\n",
    "weather = create_lag_features(weather, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "test = reduce_mem_usage(test, use_float16 = True)\n",
    "\n",
    "test = test.merge(building,left_on='building_id',right_on='building_id',how='left')\n",
    "submission = test[[\"row_id\",'meter','site_id']]\n",
    "test.drop(\"row_id\", axis=1, inplace=True)\n",
    "test = test.merge(weather,how='left',on=['timestamp','site_id'])\n",
    "\n",
    "test = features_engineering(test)\n",
    "test['meter']=[meter_encodage[c] for c in test['meter']]\n",
    "test['primary_use']=[encodage[c] for c in test['primary_use']]\n",
    "test[\"puse_hour\"] = test[\"primary_use\"].astype(str) + '_' + test[\"hour\"].astype(str)\n",
    "test[\"holiday_hour\"] = test[\"is_holiday\"].astype(str) + '_' + test[\"hour\"].astype(str)\n",
    "encode_categorical_features(test, encoded_features, encoders)\n",
    "del weather, building\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "set_size = len(test)\n",
    "batch_size = set_size // iterations\n",
    "meter_reading = []\n",
    "for i in tqdm(range(iterations)):\n",
    "    pos = i*batch_size\n",
    "    fold_preds = [np.expm1(model.predict(test.iloc[pos:pos+batch_size])) for model in models]\n",
    "    meter_reading.extend(np.mean(fold_preds, axis=0))\n",
    "    del pos, fold_preds\n",
    "\n",
    "print(len(meter_reading))\n",
    "assert len(meter_reading) == set_size\n",
    "del test, models\n",
    "\n",
    "submission['meter_reading'] = np.clip(meter_reading, a_min=0, a_max=None)\n",
    "submission.loc[(submission['meter'] == 0) & (submission['site_id'] == 0), 'meter_reading'] = submission['meter_reading'] * 3.4118\n",
    "submission = submission.drop(['meter','site_id'], axis = 1)\n",
    "submission.to_csv(f'Results/{save_file}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
