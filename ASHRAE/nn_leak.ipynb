{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'nn_leak/'\n",
    "filename = 'nn_leak'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/nguyenphuhien/anaconda3/envs/gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nguyenphuhien/anaconda3/envs/gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nguyenphuhien/anaconda3/envs/gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nguyenphuhien/anaconda3/envs/gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nguyenphuhien/anaconda3/envs/gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nguyenphuhien/anaconda3/envs/gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nguyenphuhien/anaconda3/envs/gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nguyenphuhien/anaconda3/envs/gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nguyenphuhien/anaconda3/envs/gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nguyenphuhien/anaconda3/envs/gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nguyenphuhien/anaconda3/envs/gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nguyenphuhien/anaconda3/envs/gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import datetime\n",
    "import gc\n",
    "from meteocalc import Temp, dew_point, heat_index, wind_chill, feels_like\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization, Flatten, PReLU, CuDNNLSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.losses import mean_squared_error as mse_loss\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_weather_dataset(weather):\n",
    "\n",
    "    zone_dict={0:4,1:0,2:7,3:4,4:7,5:0,6:4,7:4,8:4,9:5,10:7,11:4,12:0,13:5,14:4,15:4} \n",
    "\n",
    "    def set_localtime(df):\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        for sid, zone in zone_dict.items():\n",
    "            sids = df.site_id == sid\n",
    "            df.loc[sids, 'timestamp'] = df[sids].timestamp - pd.offsets.Hour(zone)\n",
    "        df['timestamp'] = df['timestamp'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    set_localtime(weather)\n",
    "    \n",
    "    # Find Missing Dates\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_date = datetime.datetime.strptime(weather['timestamp'].min(),time_format)\n",
    "    end_date = datetime.datetime.strptime(weather['timestamp'].max(),time_format)\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n",
    "    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n",
    "\n",
    "    missing_hours = []\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather[weather['site_id'] == site_id]['timestamp'])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n",
    "        new_rows['site_id'] = site_id\n",
    "        weather = pd.concat([weather,new_rows])\n",
    "\n",
    "        weather = weather.reset_index(drop=True)           \n",
    "\n",
    "    # Add new Features\n",
    "    weather[\"datetime\"] = pd.to_datetime(weather[\"timestamp\"])\n",
    "    weather[\"day\"] = weather[\"datetime\"].dt.day\n",
    "    weather[\"week\"] = weather[\"datetime\"].dt.week\n",
    "    weather[\"month\"] = weather[\"datetime\"].dt.month\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "    weather = weather.set_index(['site_id','day','month'])\n",
    "\n",
    "    air_temperature_filler = pd.DataFrame(weather.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "    weather.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "    cloud_coverage_filler = weather.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "    weather.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "    due_temperature_filler = pd.DataFrame(weather.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "    weather.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "    sea_level_filler = weather.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "    weather.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "    wind_direction_filler =  pd.DataFrame(weather.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "    weather.update(wind_direction_filler,overwrite=False)\n",
    "    \n",
    "    wind_speed_filler =  pd.DataFrame(weather.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "    weather.update(wind_speed_filler,overwrite=False)\n",
    "\n",
    "    precip_depth_filler = weather.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "    weather.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "    weather = weather.reset_index()\n",
    "    weather = weather.drop(['datetime','day','week','month'],axis=1)\n",
    "    \n",
    "    weather.loc[weather['dew_temperature'] > weather['air_temperature'], 'dew_temperature'] = weather['air_temperature']\n",
    "    weather['humidity'] = 100*(np.exp((17.625*weather['dew_temperature'])/(243.04+weather['dew_temperature']))/np.exp((17.625*weather['air_temperature'])/(243.04+weather['air_temperature'])))\n",
    "    \n",
    "    weather['heat_index'] = weather.apply(lambda x: heat_index(temperature=x['air_temperature'], humidity=x['humidity']), axis=1)\n",
    "    weather['heat_index'] = weather['heat_index'].apply(lambda x: round(x, 2))\n",
    "    weather['feels_like'] = weather.apply(lambda x: feels_like(temperature=x['air_temperature'], humidity=x['humidity'], wind_speed=x['wind_speed']), axis=1)\n",
    "    weather['feels_like'] = weather['feels_like'].apply(lambda x: round(x, 2))\n",
    "    \n",
    "    return weather\n",
    "\n",
    "weather_train = pd.read_csv(\"../weather_train.csv\")\n",
    "weather_train = fill_weather_dataset(weather_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df, window):\n",
    "    df['ave_temp'] = df['air_temperature']/2 + df['dew_temperature']/2\n",
    "    \n",
    "    feature_cols = [\"air_temperature\", \"dew_temperature\", \"cloud_coverage\", \"precip_depth_1_hr\",\"ave_temp\",\n",
    "                    \"humidity\",'heat_index']\n",
    "    df_site = df.groupby(\"site_id\")\n",
    "\n",
    "    df_rolled = df_site[feature_cols].rolling(window=window, min_periods=0)\n",
    "\n",
    "    df_mean = df_rolled.mean().reset_index().astype(np.float16)\n",
    "    df_min = df_rolled.min().reset_index().astype(np.float16)\n",
    "    df_std = df_rolled.std().reset_index().astype(np.float16)\n",
    "\n",
    "    for feature in feature_cols:\n",
    "        df[f\"{feature}_mean_lag{window}\"] = df_mean[feature]\n",
    "        df[f\"{feature}_min_lag{window}\"] = df_min[feature]\n",
    "        df[f\"{feature}_std_lag{window}\"] = df_std[feature]\n",
    "    \n",
    "    if window == 6:\n",
    "        for feature in feature_cols:\n",
    "            df[f\"{feature}_diff1\"] = df[feature].diff(1)\n",
    "        \n",
    "    return df\n",
    "\n",
    "weather_train = create_lag_features(weather_train, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_test = pd.read_csv('../weather_test.csv')\n",
    "weather_test = fill_weather_dataset(weather_test)\n",
    "weather_test = create_lag_features(weather_test, 6)\n",
    "weather = pd.concat([weather_train, weather_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_holiday2(dataframe):\n",
    "    # site 0\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/05/2016')\n",
    "    date2 = pd.date_range(start='3/21/2016', end='3/27/2016')\n",
    "    date3 = pd.date_range(start='11/24/2016', end='11/25/2016')\n",
    "    date4 = pd.date_range(start='12/19/2016',end='12/31/2016')\n",
    "    date5 = pd.date_range(start='1/1/2017', end='1/08/2017')\n",
    "    date6 = pd.date_range(start='3/13/2017', end='3/17/2017')\n",
    "    date7 = pd.date_range(start='11/23/2017', end='11/24/2017')\n",
    "    date8 = pd.date_range(start='12/19/2017',end='12/31/2017')\n",
    "    date9 = pd.date_range(start='1/1/2018', end='1/07/2018')\n",
    "    date10 = pd.date_range(start='3/12/2018', end='3/18/2018')\n",
    "    date11 = pd.date_range(start='11/22/2018', end='11/23/2018')\n",
    "    date12 = pd.date_range(start='12/19/2018',end='12/31/2018')\n",
    "    site0_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12)\n",
    "    sids = dataframe.site_id == 0\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site0_hol))), 'is_holiday'] = 1\n",
    "    # site 1\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/03/2016')\n",
    "    date2 = pd.date_range(start='3/25/2016', end='3/30/2016')\n",
    "    date3 = pd.date_range(start='12/24/2016',end='12/31/2016')\n",
    "    date4 = pd.date_range(start='1/1/2017', end='1/02/2017')\n",
    "    date5 = pd.date_range(start='3/13/2017', end='3/19/2017')\n",
    "    date6 = pd.date_range(start='12/23/2017',end='12/31/2017')\n",
    "    date7 = pd.date_range(start='3/29/2018', end='4/04/2018')\n",
    "    date8 = pd.date_range(start='12/22/2018',end='12/31/2018')\n",
    "    site1_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8)\n",
    "    sids = dataframe.site_id == 1\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site1_hol))), 'is_holiday'] = 1\n",
    "    # site 2\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/12/2016')\n",
    "    date2 = pd.date_range(start='3/12/2016', end='3/20/2016')\n",
    "    date3 = pd.date_range(start='11/24/2016', end='11/27/2016')\n",
    "    date4 = pd.date_range(start='12/26/2016',end='12/27/2016')\n",
    "    date5 = pd.date_range(start='1/1/2017', end='1/9/2017')\n",
    "    date6 = pd.date_range(start='3/11/2017', end='3/19/2017')\n",
    "    date7 = pd.date_range(start='11/23/2017', end='11/26/2017')\n",
    "    date8 = pd.date_range(start='12/25/2017',end='12/26/2017')\n",
    "    date9 = pd.date_range(start='1/1/2018', end='1/09/2018')\n",
    "    date10 = pd.date_range(start='3/5/2018', end='3/9/2018')\n",
    "    date11 = pd.date_range(start='11/22/2018', end='11/25/2018')\n",
    "    date12 = pd.date_range(start='12/24/2018',end='12/25/2018')\n",
    "    site2_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12)\n",
    "    sids = dataframe.site_id == 2\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site2_hol))), 'is_holiday'] = 1\n",
    "    # site 4\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/11/2016')\n",
    "    date2 = pd.date_range(start='3/21/2016', end='3/25/2016')\n",
    "    date3 = pd.date_range(start='11/24/2016', end='11/25/2016')\n",
    "    date4 = pd.date_range(start='12/26/2016',end='12/27/2016')\n",
    "    date5 = pd.date_range(start='1/1/2017', end='1/9/2017')\n",
    "    date6 = pd.date_range(start='3/27/2017', end='3/31/2017')\n",
    "    date7 = pd.date_range(start='11/23/2017', end='11/26/2017')\n",
    "    date8 = pd.date_range(start='12/25/2017',end='12/26/2017')\n",
    "    date9 = pd.date_range(start='12/29/2017',end='12/29/2017')\n",
    "    date10 = pd.date_range(start='1/1/2018', end='1/08/2018')\n",
    "    date11 = pd.date_range(start='3/26/2018', end='3/30/2018')\n",
    "    date12 = pd.date_range(start='11/22/2018', end='11/23/2018')\n",
    "    date13 = pd.date_range(start='12/24/2018',end='12/25/2018')\n",
    "    date14 = pd.date_range(start='12/31/2018',end='12/31/2018')\n",
    "    site4_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12).union(date13).union(date14)\n",
    "    sids = dataframe.site_id == 4\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site4_hol))), 'is_holiday'] = 1\n",
    "    # site 5\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/3/2016')\n",
    "    date2 = pd.date_range(start='3/19/2016', end='4/17/2016')\n",
    "    date3 = pd.date_range(start='6/11/2016', end='8/21/2016')\n",
    "    date4 = pd.date_range(start='1/1/2017',end='1/8/2017')\n",
    "    date5 = pd.date_range(start='3/23/2017', end='4/23/2017')\n",
    "    date6 = pd.date_range(start='6/17/2017', end='8/20/2017')\n",
    "    date7 = pd.date_range(start='1/1/2018',end='1/7/2018')\n",
    "    date8 = pd.date_range(start='3/17/2018', end='4/15/2018')\n",
    "    date9 = pd.date_range(start='6/16/2018', end='8/19/2018')\n",
    "    site5_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9)\n",
    "    sids = dataframe.site_id == 5\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site5_hol))), 'is_holiday'] = 1\n",
    "    # site 7&11\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/06/2016')\n",
    "    date2 = pd.date_range(start='2/29/2016', end='3/4/2016')\n",
    "    date3 = pd.date_range(start='11/24/2016', end='11/25/2016')\n",
    "    date4 = pd.date_range(start='12/23/2016',end='12/31/2016')\n",
    "    date5 = pd.date_range(start='1/1/2017', end='1/03/2017')\n",
    "    date6 = pd.date_range(start='2/27/2017', end='3/3/2017')\n",
    "    date7 = pd.date_range(start='11/23/2017', end='11/24/2017')\n",
    "    date8 = pd.date_range(start='12/23/2017',end='12/31/2017')\n",
    "    date9 = pd.date_range(start='1/1/2018', end='1/07/2018')\n",
    "    date10 = pd.date_range(start='3/5/2018', end='3/9/2018')\n",
    "    date11 = pd.date_range(start='11/22/2018', end='11/23/2018')\n",
    "    date12 = pd.date_range(start='12/24/2018',end='12/31/2018')\n",
    "    site_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12)\n",
    "    sids = dataframe.site_id == 7\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site_hol))), 'is_holiday'] = 1\n",
    "    sids = dataframe.site_id == 11\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site_hol))), 'is_holiday'] = 1\n",
    "    # site 9\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/06/2016')\n",
    "    date2 = pd.date_range(start='3/14/2016', end='3/19/2016')\n",
    "    date3 = pd.date_range(start='11/23/2016', end='11/26/2016')\n",
    "    date4 = pd.date_range(start='12/23/2016',end='12/31/2016')\n",
    "    date5 = pd.date_range(start='1/1/2017', end='1/04/2017')\n",
    "    date6 = pd.date_range(start='3/13/2017', end='3/18/2017')\n",
    "    date7 = pd.date_range(start='11/22/2017', end='11/25/2017')\n",
    "    date8 = pd.date_range(start='12/23/2017',end='12/31/2017')\n",
    "    date9 = pd.date_range(start='1/1/2018', end='1/03/2018')\n",
    "    date10 = pd.date_range(start='3/12/2018', end='3/17/2018')\n",
    "    date11 = pd.date_range(start='11/21/2018', end='11/24/2018')\n",
    "    date12 = pd.date_range(start='12/24/2018',end='12/31/2018')\n",
    "    site9_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12)\n",
    "    sids = dataframe.site_id == 9\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site9_hol))), 'is_holiday'] = 1\n",
    "    # site 10\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/10/2016')\n",
    "    date2 = pd.date_range(start='3/5/2016', end='3/13/2016')\n",
    "    date3 = pd.date_range(start='11/24/2016', end='11/25/2016')\n",
    "    date4 = pd.date_range(start='12/23/2016',end='12/31/2016')\n",
    "    date5 = pd.date_range(start='1/1/2017', end='1/08/2017')\n",
    "    date6 = pd.date_range(start='3/4/2017', end='3/12/2017')\n",
    "    date7 = pd.date_range(start='11/23/2017', end='11/24/2017')\n",
    "    date8 = pd.date_range(start='12/24/2017',end='12/31/2017')\n",
    "    date9 = pd.date_range(start='1/1/2018', end='1/07/2018')\n",
    "    date10 = pd.date_range(start='3/3/2018', end='3/11/2018')\n",
    "    date11 = pd.date_range(start='11/22/2018', end='11/23/2018')\n",
    "    date12 = pd.date_range(start='12/24/2018',end='12/31/2018')\n",
    "    site10_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12)\n",
    "    sids = dataframe.site_id == 10\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site10_hol))), 'is_holiday'] = 1\n",
    "    # site 13\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/12/2016')\n",
    "    date2 = pd.date_range(start='3/7/2016', end='3/11/2016')\n",
    "    date3 = pd.date_range(start='10/27/2016', end='10/28/2016')\n",
    "    date4 = pd.date_range(start='11/24/2016', end='11/25/2016')\n",
    "    date5 = pd.date_range(start='12/23/2016',end='12/30/2016')\n",
    "    date6 = pd.date_range(start='1/1/2017',end='1/10/2017')\n",
    "    date7 = pd.date_range(start='3/6/2017', end='3/10/2017')\n",
    "    date8 = pd.date_range(start='10/26/2017', end='10/27/2017')\n",
    "    date9 = pd.date_range(start='11/23/2017', end='11/24/2017')\n",
    "    date10 = pd.date_range(start='12/25/2017',end='12/29/2017')\n",
    "    date11 = pd.date_range(start='1/1/2018',end='1/9/2018')\n",
    "    date12 = pd.date_range(start='3/5/2018', end='3/9/2018')\n",
    "    date13 = pd.date_range(start='10/25/2018', end='10/26/2018')\n",
    "    date14 = pd.date_range(start='11/22/2018', end='11/23/2018')\n",
    "    date15 = pd.date_range(start='12/25/2018',end='12/26/2018')\n",
    "    date16 = pd.date_range(start='12/31/2018',end='12/31/2018')\n",
    "    site13_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12).union(date13).union(date14).union(date15).union(date16)\n",
    "    sids = dataframe.site_id == 13\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site13_hol))), 'is_holiday'] = 1\n",
    "    # site 14\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/03/2016')\n",
    "    date2 = pd.date_range(start='3/5/2016', end='3/13/2016')\n",
    "    date3 = pd.date_range(start='11/23/2016', end='11/27/2016')\n",
    "    date4 = pd.date_range(start='1/1/2017', end='1/02/2017')\n",
    "    date5 = pd.date_range(start='3/4/2017', end='3/12/2017')\n",
    "    date6 = pd.date_range(start='11/22/2017', end='11/26/2017')\n",
    "    date7 = pd.date_range(start='1/1/2018', end='1/02/2018')\n",
    "    date8 = pd.date_range(start='3/3/2018', end='3/11/2018')\n",
    "    date9 = pd.date_range(start='11/21/2018', end='11/25/2018')\n",
    "    site14_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9)\n",
    "    sids = dataframe.site_id == 14\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site14_hol))), 'is_holiday'] = 1\n",
    "    # site 15\n",
    "    date1 = pd.date_range(start='1/1/2016',end='1/03/2016')\n",
    "    date2 = pd.date_range(start='2/13/2016',end='2/17/2016')\n",
    "    date3 = pd.date_range(start='3/26/2016', end='4/3/2016')\n",
    "    date4 = pd.date_range(start='10/8/2016', end='10/11/2016')\n",
    "    date5 = pd.date_range(start='11/23/2016', end='11/27/2016')\n",
    "    date6 = pd.date_range(start='12/26/2016',end='12/31/2016')\n",
    "    date7 = pd.date_range(start='1/1/2017',end='1/02/2017')\n",
    "    date8 = pd.date_range(start='2/18/2017',end='2/21/2017')\n",
    "    date9 = pd.date_range(start='4/1/2017', end='4/9/2017')\n",
    "    date10 = pd.date_range(start='10/7/2017', end='10/10/2017')\n",
    "    date11 = pd.date_range(start='11/22/2017', end='11/26/2017')\n",
    "    date12 = pd.date_range(start='12/25/2017',end='12/31/2017')\n",
    "    date13 = pd.date_range(start='2/17/2018',end='2/20/2018')\n",
    "    date14 = pd.date_range(start='3/31/2018', end='4/8/2018')\n",
    "    date15 = pd.date_range(start='10/6/2018', end='10/9/2018')\n",
    "    date16 = pd.date_range(start='11/21/2018', end='11/25/2018')\n",
    "    date17 = pd.date_range(start='12/24/2018',end='12/31/2018')\n",
    "    site15_hol = date1.union(date2).union(date3).union(date4).union(date5).union(date6).union(date7).union(date8).union(date9).union(date10).union(date11).union(date12).union(date13).union(date14).union(date15).union(date16).union(date17)\n",
    "    sids = dataframe.site_id == 15\n",
    "    dataframe.loc[(sids) & (dataframe.timestamp.isin((site15_hol))), 'is_holiday'] = 1\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    start_mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != object:  # Exclude strings            \n",
    "            # Print current column type\n",
    "            print(\"******************************\")\n",
    "            print(\"Column: \",col)\n",
    "            print(\"dtype before: \",df[col].dtype)            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = df[col].max()\n",
    "            mn = df[col].min()\n",
    "            print(\"min for this col: \",mn)\n",
    "            print(\"max for this col: \",mx)\n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(df[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                df[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = df[col].fillna(0).astype(np.int64)\n",
    "            result = (df[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        df[col] = df[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        df[col] = df[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        df[col] = df[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)    \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            \n",
    "            # Print new column type\n",
    "            print(\"dtype after: \",df[col].dtype)\n",
    "            print(\"******************************\")\n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return df, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_df = pd.read_feather('leak.feather')\n",
    "leak_df.fillna(0, inplace=True)\n",
    "leak_df = leak_df[(leak_df.timestamp.dt.year > 2016) & (leak_df.timestamp.dt.year < 2019)]\n",
    "leak_df.loc[leak_df.meter_reading < 0, 'meter_reading'] = 0 # remove large negative values\n",
    "leak_df = leak_df[leak_df.building_id!=245]\n",
    "leak_df['timestamp'] = leak_df['timestamp'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "building_df = pd.read_csv(\"../building_metadata.csv\")\n",
    "train = pd.read_csv(\"../train.csv\")\n",
    "\n",
    "bad_rows = pd.read_csv('../rows_to_drop.csv')\n",
    "train = train.drop(index=bad_rows['0'].values)\n",
    "del bad_rows\n",
    "\n",
    "train = pd.concat([train, leak_df]).reset_index(drop = True)\n",
    "train = train.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\n",
    "train = train.merge(weather, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"])\n",
    "del weather, weather_train, weather_test, leak_df\n",
    "\n",
    "train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\n",
    "train[\"hour\"] = train[\"timestamp\"].dt.hour\n",
    "train[\"weekday\"] = train[\"timestamp\"].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "              \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "              \"2017-01-02\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "              \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "              \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "              \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "              \"2019-01-01\"]\n",
    "uk_holidays = [\"2016-01-01\", \"2016-01-02\", \"2016-03-25\", \"2016-03-28\", \"2016-05-02\", \n",
    "               \"2016-05-30\", \"2016-08-29\", \"2016-12-25\", \"2016-12-26\", \"2016-12-27\", \n",
    "               \"2017-01-01\", \"2017-01-02\", \"2017-04-14\", \"2017-04-17\", \"2017-05-01\", \n",
    "               \"2017-05-29\", \"2017-08-28\", \"2017-12-25\", \"2017-12-26\", \n",
    "               \"2018-01-01\", \"2018-01-02\", \"2018-03-30\", \"2018-04-02\", \"2018-05-07\", \n",
    "               \"2018-05-28\", \"2018-08-27\", \"2018-12-25\", \"2018-12-26\",\n",
    "               \"2019-01-01\"]\n",
    "ir_holidays = [\"2016-01-01\", \"2016-01-02\", \"2016-03-17\", \"2016-03-28\", \"2016-05-02\", \n",
    "               \"2016-06-02\", \"2016-08-01\", \"2016-10-31\", \"2016-12-26\", \"2016-12-27\", \n",
    "               \"2017-01-01\", \"2017-01-02\", \"2017-03-17\", \"2017-04-14\", \"2017-05-01\", \n",
    "               \"2017-06-05\", \"2017-08-07\", \"2017-10-30\", \"2017-12-25\", \"2017-12-26\", \n",
    "               \"2018-01-01\", \"2018-01-02\", \"2018-03-19\", \"2018-04-02\", \"2018-05-07\", \n",
    "               \"2018-06-04\", \"2018-08-06\", \"2018-10-29\", \"2018-12-25\", \"2018-12-26\",\n",
    "               \"2019-01-01\"]\n",
    "\n",
    "def set_holiday(train):\n",
    "    train[\"is_holiday\"] = 0\n",
    "    train.loc[train.weekday.isin([5,6]), 'is_holiday'] = 1\n",
    "    \n",
    "    us_zone = [0,2,3,4,6,7,8,9,10,11,13,14,15]\n",
    "    for sid in us_zone:\n",
    "        sids = train.site_id == sid\n",
    "        train.loc[(sids) & (train.timestamp.isin(us_holidays)), 'is_holiday'] = 1\n",
    "    uk_zone = [1,5]\n",
    "    for sid in uk_zone:\n",
    "        sids = train.site_id == sid\n",
    "        train.loc[(sids) & (train.timestamp.isin(uk_holidays)), 'is_holiday'] = 1\n",
    "    sids = train.site_id == 12\n",
    "    train.loc[(sids) & (train.timestamp.isin(ir_holidays)), 'is_holiday'] = 1\n",
    "    return train\n",
    "\n",
    "train = set_holiday(train)\n",
    "train = set_holiday2(train)\n",
    "\n",
    "train['month'] = train['timestamp'].dt.month\n",
    "train['month'].replace((1, 2, 3, 12), 1, inplace = True)\n",
    "train['month'].replace((6, 7, 8, 9), 2, inplace = True)\n",
    "train['month'].replace((10, 11, 4, 5), 3, inplace = True)\n",
    "del train[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### new features\n",
    "train[\"puse_hour\"] = train[\"primary_use\"].astype(str) + '_' + train[\"hour\"].astype(str)\n",
    "train[\"holiday_hour\"] = train[\"is_holiday\"].astype(str) + '_' + train[\"hour\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_categorical_encoders(df, features):\n",
    "    encoders = {}\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].fillna('missing')\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(df[feature].values)\n",
    "        le_dict = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "        encoders[feature] = le_dict\n",
    "    return encoders\n",
    "\n",
    "def encode_categorical_features(df, features, encoders):\n",
    "    for f in features:\n",
    "        df[f] = df[f].fillna('missing')\n",
    "        df[f] = df[f].map(encoders[f])\n",
    "\n",
    "encoded_features = [\"primary_use\",\"puse_hour\",\"holiday_hour\"]       \n",
    "encoders = generate_categorical_encoders(train, encoded_features)\n",
    "encode_categorical_features(train, encoded_features, encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.log1p(train[\"meter_reading\"])\n",
    "del train[\"meter_reading\"] \n",
    "\n",
    "drop_cols = [\"sea_level_pressure\", \"wind_speed\", \"wind_direction\"]\n",
    "train = train.drop(drop_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"site_id\",\"building_id\",\"primary_use\",\"hour\", \"weekday\", \"meter\",\"puse_hour\",\"holiday_hour\"]\n",
    "\n",
    "numericals = [i for i in train.columns if i not in categoricals]\n",
    "feat_cols = categoricals + numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 8180.440246582031  MB\n",
      "******************************\n",
      "Column:  building_id\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  1448\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  meter\n",
      "dtype before:  float64\n",
      "min for this col:  0.0\n",
      "max for this col:  3.0\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  site_id\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  15\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  primary_use\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  15\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  square_feet\n",
      "dtype before:  int64\n",
      "min for this col:  283\n",
      "max for this col:  875000\n",
      "dtype after:  uint32\n",
      "******************************\n",
      "******************************\n",
      "Column:  year_built\n",
      "dtype before:  float64\n",
      "min for this col:  1900.0\n",
      "max for this col:  2017.0\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  floor_count\n",
      "dtype before:  float64\n",
      "min for this col:  1.0\n",
      "max for this col:  26.0\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature\n",
      "dtype before:  float64\n",
      "min for this col:  -28.9\n",
      "max for this col:  48.3\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage\n",
      "dtype before:  float64\n",
      "min for this col:  0.0\n",
      "max for this col:  9.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature\n",
      "dtype before:  float64\n",
      "min for this col:  -35.0\n",
      "max for this col:  26.7\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr\n",
      "dtype before:  float64\n",
      "min for this col:  -1.0\n",
      "max for this col:  597.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  humidity\n",
      "dtype before:  float64\n",
      "min for this col:  2.12834523177749\n",
      "max for this col:  100.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  heat_index\n",
      "dtype before:  float64\n",
      "min for this col:  -39.2\n",
      "max for this col:  43.13\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  feels_like\n",
      "dtype before:  float64\n",
      "min for this col:  -47.35\n",
      "max for this col:  48.3\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  ave_temp\n",
      "dtype before:  float64\n",
      "min for this col:  -31.65\n",
      "max for this col:  31.349999999999998\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -28.3\n",
      "max for this col:  47.6\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -28.9\n",
      "max for this col:  46.7\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  13.73\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -34.34\n",
      "max for this col:  25.66\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -35.0\n",
      "max for this col:  25.6\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  13.45\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  9.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  9.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  4.93\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -1.0\n",
      "max for this col:  217.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -1.0\n",
      "max for this col:  217.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  241.6\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  ave_temp_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -30.89\n",
      "max for this col:  30.47\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  ave_temp_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -31.66\n",
      "max for this col:  30.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  ave_temp_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  13.23\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  humidity_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  2.78\n",
      "max for this col:  100.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  humidity_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  2.129\n",
      "max for this col:  100.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  humidity_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  35.56\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  heat_index_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -38.22\n",
      "max for this col:  42.34\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  heat_index_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -39.2\n",
      "max for this col:  41.4\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  heat_index_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  14.625\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -25.05882352941176\n",
      "max for this col:  31.360416666666662\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -23.306570512820507\n",
      "max for this col:  29.933333333333323\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -9.0\n",
      "max for this col:  9.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -579.0\n",
      "max for this col:  597.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  ave_temp_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -21.894951923076924\n",
      "max for this col:  30.646874999999994\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  humidity_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -64.73757530896768\n",
      "max for this col:  64.61152450657028\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  heat_index_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -26.68\n",
      "max for this col:  34.41\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  hour\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  23\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  weekday\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  is_holiday\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  1\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  month\n",
      "dtype before:  int64\n",
      "min for this col:  1\n",
      "max for this col:  3\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  puse_hour\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  383\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  holiday_hour\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  47\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  5105.311248779297  MB\n",
      "This is  62.40875912408759 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "train, NAlist = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    #Inputs\n",
    "    site_id = Input(shape=[1], name=\"site_id\")\n",
    "    building_id = Input(shape=[1], name=\"building_id\")\n",
    "    meter = Input(shape=[1], name=\"meter\")\n",
    "    primary_use = Input(shape=[1], name=\"primary_use\")\n",
    "    hour = Input(shape=[1], name=\"hour\")\n",
    "    weekday = Input(shape=[1], name=\"weekday\")\n",
    "    puse_hour = Input(shape=[1], name=\"puse_hour\")\n",
    "    holiday_hour = Input(shape=[1], name=\"holiday_hour\")\n",
    "    num_input = Input(shape=(len(numericals),), name=\"num_input\")\n",
    "    \n",
    "   \n",
    "    #Embeddings layers\n",
    "    emb_site_id = Embedding(16, 6)(site_id)\n",
    "    emb_building_id = Embedding(1449, 128)(building_id)\n",
    "    emb_meter = Embedding(4, 2)(meter)\n",
    "    emb_primary_use = Embedding(16, 6)(primary_use)\n",
    "    emb_hour = Embedding(24, 8)(hour)\n",
    "    emb_weekday = Embedding(7, 2)(weekday)\n",
    "    emb_puse_hour = Embedding(384, 16)(puse_hour)\n",
    "    emb_holiday_hour = Embedding(48, 12)(holiday_hour)\n",
    "    categ = concatenate([Flatten() (emb_site_id), Flatten() (emb_building_id), Flatten() (emb_meter), \n",
    "                         Flatten() (emb_primary_use), Flatten() (emb_hour), Flatten() (emb_weekday), \n",
    "                         Flatten() (emb_puse_hour), Flatten() (emb_holiday_hour)])\n",
    "    \n",
    "     ### Categorical\n",
    "    n_unit = 128\n",
    "    decay_rate = 0.5\n",
    "    for k in range(3):\n",
    "        categ = Dense(n_unit)(categ)\n",
    "        categ = PReLU()(categ)\n",
    "        categ = BatchNormalization()(categ)\n",
    "        categ = Dropout(0.1)(categ)     \n",
    "        n_unit = int(n_unit * decay_rate)\n",
    "    \n",
    "    ### Dense\n",
    "    numerical = Dense(64)(num_input)\n",
    "    numerical = PReLU()(numerical)\n",
    "    numerical = BatchNormalization()(numerical)\n",
    "    numerical = Dropout(0.1)(numerical)\n",
    "    numerical = Dense(64)(numerical)\n",
    "    numerical = PReLU()(numerical)\n",
    "    numerical = BatchNormalization()(numerical)\n",
    "    numerical = Dropout(0.1)(numerical)\n",
    "    numerical = Dense(32)(numerical)\n",
    "    numerical = PReLU()(numerical)\n",
    "    numerical = BatchNormalization()(numerical)\n",
    "    numerical = Dropout(0.1)(numerical)  \n",
    "    \n",
    "    #main layer\n",
    "    x = concatenate([categ, numerical])\n",
    "    n_unit = 128\n",
    "    decay_rate = 0.5\n",
    "    for k in range(4):\n",
    "        x = Dense(n_unit)(x)\n",
    "        x = PReLU()(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.1)(x)     \n",
    "        n_unit = int(n_unit * decay_rate)\n",
    "\n",
    "    #output\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model([site_id,building_id,meter,primary_use,hour,weekday,puse_hour,holiday_hour,\n",
    "                   num_input], output)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr=0.001, decay = 0.0001),\n",
    "                  loss= mse_loss,\n",
    "                  metrics=[root_mean_squared_error])\n",
    "    return model\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(df, num_cols, cat_cols):\n",
    "    X = {col: np.array(df[col]) for col in cat_cols}\n",
    "    X['num_input'] = df[num_cols].values\n",
    "    return X\n",
    "\n",
    "def train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold, patience=5):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint(directory+\"model_\" + str(fold) + \".hdf5\",\n",
    "                                       save_best_only=True, verbose=1, monitor='val_root_mean_squared_error', mode='min')\n",
    "\n",
    "    hist = keras_model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                            validation_data=(X_v, y_valid), verbose=1,\n",
    "                            callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    keras_model = load_model(directory+\"model_\" + str(fold) + \".hdf5\", custom_objects={'root_mean_squared_error': root_mean_squared_error})\n",
    "    \n",
    "    \n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Train on 25044710 samples, validate on 6261178 samples\n",
      "Epoch 1/100\n",
      "25044710/25044710 [==============================] - 320s 13us/step - loss: 0.9755 - root_mean_squared_error: 0.9323 - val_loss: 0.7097 - val_root_mean_squared_error: 0.8187\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 0.81874, saving model to nn_leak/model_0.hdf5\n",
      "Epoch 2/100\n",
      "25044710/25044710 [==============================] - 321s 13us/step - loss: 0.6659 - root_mean_squared_error: 0.8140 - val_loss: 0.7126 - val_root_mean_squared_error: 0.8199\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 0.81874\n",
      "Epoch 3/100\n",
      "25044710/25044710 [==============================] - 318s 13us/step - loss: 0.6453 - root_mean_squared_error: 0.8012 - val_loss: 0.7195 - val_root_mean_squared_error: 0.8251\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.81874\n",
      "Epoch 4/100\n",
      "25044710/25044710 [==============================] - 316s 13us/step - loss: 0.6365 - root_mean_squared_error: 0.7957 - val_loss: 0.7050 - val_root_mean_squared_error: 0.8152\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 0.81874 to 0.81517, saving model to nn_leak/model_0.hdf5\n",
      "Epoch 5/100\n",
      "25044710/25044710 [==============================] - 316s 13us/step - loss: 0.6310 - root_mean_squared_error: 0.7923 - val_loss: 0.7051 - val_root_mean_squared_error: 0.8152\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 0.81517 to 0.81516, saving model to nn_leak/model_0.hdf5\n",
      "Epoch 6/100\n",
      "25044710/25044710 [==============================] - 317s 13us/step - loss: 0.6276 - root_mean_squared_error: 0.7902 - val_loss: 0.6997 - val_root_mean_squared_error: 0.8118\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error improved from 0.81516 to 0.81177, saving model to nn_leak/model_0.hdf5\n",
      "Epoch 7/100\n",
      "25044710/25044710 [==============================] - 316s 13us/step - loss: 0.6252 - root_mean_squared_error: 0.7886 - val_loss: 0.7063 - val_root_mean_squared_error: 0.8161\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.81177\n",
      "Epoch 8/100\n",
      "25044710/25044710 [==============================] - 316s 13us/step - loss: 0.6232 - root_mean_squared_error: 0.7873 - val_loss: 0.7013 - val_root_mean_squared_error: 0.8126\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.81177\n",
      "Epoch 9/100\n",
      "25044710/25044710 [==============================] - 321s 13us/step - loss: 0.6217 - root_mean_squared_error: 0.7864 - val_loss: 0.7112 - val_root_mean_squared_error: 0.8192\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error did not improve from 0.81177\n",
      "Epoch 00009: early stopping\n",
      "**************************************************\n",
      "Fold: 1\n",
      "Train on 25044710 samples, validate on 6261178 samples\n",
      "Epoch 1/100\n",
      "25044710/25044710 [==============================] - 335s 13us/step - loss: 0.9728 - root_mean_squared_error: 0.9305 - val_loss: 0.7226 - val_root_mean_squared_error: 0.8239\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 0.82391, saving model to nn_leak/model_1.hdf5\n",
      "Epoch 2/100\n",
      "25044710/25044710 [==============================] - 334s 13us/step - loss: 0.6615 - root_mean_squared_error: 0.8113 - val_loss: 0.7045 - val_root_mean_squared_error: 0.8130\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 0.82391 to 0.81296, saving model to nn_leak/model_1.hdf5\n",
      "Epoch 3/100\n",
      "25044710/25044710 [==============================] - 328s 13us/step - loss: 0.6407 - root_mean_squared_error: 0.7984 - val_loss: 0.6991 - val_root_mean_squared_error: 0.8092\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 0.81296 to 0.80921, saving model to nn_leak/model_1.hdf5\n",
      "Epoch 4/100\n",
      "25044710/25044710 [==============================] - 330s 13us/step - loss: 0.6313 - root_mean_squared_error: 0.7925 - val_loss: 0.7102 - val_root_mean_squared_error: 0.8159\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.80921\n",
      "Epoch 5/100\n",
      "25044710/25044710 [==============================] - 333s 13us/step - loss: 0.6255 - root_mean_squared_error: 0.7888 - val_loss: 0.7130 - val_root_mean_squared_error: 0.8177\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.80921\n",
      "Epoch 6/100\n",
      "25044710/25044710 [==============================] - 332s 13us/step - loss: 0.6218 - root_mean_squared_error: 0.7865 - val_loss: 0.6993 - val_root_mean_squared_error: 0.8089\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error improved from 0.80921 to 0.80889, saving model to nn_leak/model_1.hdf5\n",
      "Epoch 00006: early stopping\n",
      "**************************************************\n",
      "Fold: 2\n",
      "Train on 25044710 samples, validate on 6261178 samples\n",
      "Epoch 1/100\n",
      "25044710/25044710 [==============================] - 346s 14us/step - loss: 0.9568 - root_mean_squared_error: 0.9216 - val_loss: 0.8196 - val_root_mean_squared_error: 0.8796\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 0.87963, saving model to nn_leak/model_2.hdf5\n",
      "Epoch 2/100\n",
      "25044710/25044710 [==============================] - 344s 14us/step - loss: 0.6504 - root_mean_squared_error: 0.8044 - val_loss: 0.7188 - val_root_mean_squared_error: 0.8208\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 0.87963 to 0.82082, saving model to nn_leak/model_2.hdf5\n",
      "Epoch 3/100\n",
      "25044710/25044710 [==============================] - 344s 14us/step - loss: 0.6297 - root_mean_squared_error: 0.7914 - val_loss: 0.7333 - val_root_mean_squared_error: 0.8297\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.82082\n",
      "Epoch 4/100\n",
      "25044710/25044710 [==============================] - 344s 14us/step - loss: 0.6209 - root_mean_squared_error: 0.7859 - val_loss: 0.7505 - val_root_mean_squared_error: 0.8385\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.82082\n",
      "Epoch 5/100\n",
      "25044710/25044710 [==============================] - 344s 14us/step - loss: 0.6158 - root_mean_squared_error: 0.7826 - val_loss: 0.7103 - val_root_mean_squared_error: 0.8162\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 0.82082 to 0.81616, saving model to nn_leak/model_2.hdf5\n",
      "Epoch 6/100\n",
      "25044710/25044710 [==============================] - 337s 13us/step - loss: 0.6126 - root_mean_squared_error: 0.7806 - val_loss: 0.7047 - val_root_mean_squared_error: 0.8124\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error improved from 0.81616 to 0.81244, saving model to nn_leak/model_2.hdf5\n",
      "Epoch 7/100\n",
      "25044710/25044710 [==============================] - 337s 13us/step - loss: 0.6099 - root_mean_squared_error: 0.7788 - val_loss: 0.7048 - val_root_mean_squared_error: 0.8125\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.81244\n",
      "Epoch 8/100\n",
      "25044710/25044710 [==============================] - 337s 13us/step - loss: 0.6081 - root_mean_squared_error: 0.7777 - val_loss: 0.7051 - val_root_mean_squared_error: 0.8124\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error improved from 0.81244 to 0.81237, saving model to nn_leak/model_2.hdf5\n",
      "Epoch 9/100\n",
      "25044710/25044710 [==============================] - 337s 13us/step - loss: 0.6065 - root_mean_squared_error: 0.7766 - val_loss: 0.7056 - val_root_mean_squared_error: 0.8128\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error did not improve from 0.81237\n",
      "Epoch 00009: early stopping\n",
      "**************************************************\n",
      "Fold: 3\n",
      "Train on 25044711 samples, validate on 6261177 samples\n",
      "Epoch 1/100\n",
      "25044711/25044711 [==============================] - 348s 14us/step - loss: 0.9233 - root_mean_squared_error: 0.9018 - val_loss: 1.1714 - val_root_mean_squared_error: 1.0283\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 1.02829, saving model to nn_leak/model_3.hdf5\n",
      "Epoch 2/100\n",
      "25044711/25044711 [==============================] - 347s 14us/step - loss: 0.5930 - root_mean_squared_error: 0.7681 - val_loss: 1.1723 - val_root_mean_squared_error: 1.0293\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 1.02829\n",
      "Epoch 3/100\n",
      "25044711/25044711 [==============================] - 347s 14us/step - loss: 0.5725 - root_mean_squared_error: 0.7546 - val_loss: 1.1353 - val_root_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 1.02829 to 1.01213, saving model to nn_leak/model_3.hdf5\n",
      "Epoch 4/100\n",
      "25044711/25044711 [==============================] - 352s 14us/step - loss: 0.5634 - root_mean_squared_error: 0.7486 - val_loss: 1.1470 - val_root_mean_squared_error: 1.0170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 1.01213\n",
      "Epoch 5/100\n",
      "25044711/25044711 [==============================] - 349s 14us/step - loss: 0.5581 - root_mean_squared_error: 0.7450 - val_loss: 1.1368 - val_root_mean_squared_error: 1.0122\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 1.01213\n",
      "Epoch 6/100\n",
      "25044711/25044711 [==============================] - 349s 14us/step - loss: 0.5540 - root_mean_squared_error: 0.7423 - val_loss: 1.1401 - val_root_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 1.01213\n",
      "Epoch 00006: early stopping\n",
      "**************************************************\n",
      "Fold: 4\n",
      "Train on 25044711 samples, validate on 6261177 samples\n",
      "Epoch 1/100\n",
      "25044711/25044711 [==============================] - 361s 14us/step - loss: 1.0064 - root_mean_squared_error: 0.9476 - val_loss: 0.6402 - val_root_mean_squared_error: 0.7281\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 0.72810, saving model to nn_leak/model_4.hdf5\n",
      "Epoch 2/100\n",
      "25044711/25044711 [==============================] - 358s 14us/step - loss: 0.6784 - root_mean_squared_error: 0.8216 - val_loss: 0.6417 - val_root_mean_squared_error: 0.7279\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 0.72810 to 0.72787, saving model to nn_leak/model_4.hdf5\n",
      "Epoch 3/100\n",
      "25044711/25044711 [==============================] - 367s 15us/step - loss: 0.6565 - root_mean_squared_error: 0.8081 - val_loss: 0.6429 - val_root_mean_squared_error: 0.7284\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.72787\n",
      "Epoch 4/100\n",
      "25044711/25044711 [==============================] - 365s 15us/step - loss: 0.6476 - root_mean_squared_error: 0.8026 - val_loss: 0.6409 - val_root_mean_squared_error: 0.7250\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 0.72787 to 0.72500, saving model to nn_leak/model_4.hdf5\n",
      "Epoch 00004: early stopping\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 100\n",
    "models = []\n",
    "\n",
    "folds = 5\n",
    "\n",
    "kf = KFold(n_splits=folds)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(kf.split(train, train['month'])):\n",
    "    print('Fold:', fold_n)\n",
    "    X_train, X_valid = train.iloc[train_index], train.iloc[valid_index]\n",
    "    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "    X_t = get_keras_data(X_train, numericals, categoricals)\n",
    "    X_v = get_keras_data(X_valid, numericals, categoricals)\n",
    "    del X_train, X_valid\n",
    "    \n",
    "    keras_model = get_model()\n",
    "    mod = train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold_n, patience=3)\n",
    "    models.append(mod)\n",
    "    \n",
    "    print('*'* 50)\n",
    "    del mod, X_t, X_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, target, y_train, y_valid, kf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"../test.csv\")\n",
    "test = test.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\n",
    "del building_df\n",
    "\n",
    "weather_test = pd.read_csv(\"../weather_test.csv\")\n",
    "weather_test = fill_weather_dataset(weather_test)\n",
    "weather_test = create_lag_features(weather_test, 6)\n",
    "\n",
    "test = test.merge(weather_test, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"], how = \"left\")\n",
    "del weather_test\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 10895.864868164062  MB\n",
      "******************************\n",
      "Column:  site_id\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  15\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  building_id\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  1448\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  primary_use\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  15\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  hour\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  23\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  weekday\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  6\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  meter\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  3\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  puse_hour\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  383\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  holiday_hour\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  47\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  square_feet\n",
      "dtype before:  int64\n",
      "min for this col:  283\n",
      "max for this col:  875000\n",
      "dtype after:  uint32\n",
      "******************************\n",
      "******************************\n",
      "Column:  year_built\n",
      "dtype before:  float64\n",
      "min for this col:  1900.0\n",
      "max for this col:  2017.0\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  floor_count\n",
      "dtype before:  float64\n",
      "min for this col:  1.0\n",
      "max for this col:  26.0\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature\n",
      "dtype before:  float64\n",
      "min for this col:  -28.1\n",
      "max for this col:  48.3\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage\n",
      "dtype before:  float64\n",
      "min for this col:  0.0\n",
      "max for this col:  9.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature\n",
      "dtype before:  float64\n",
      "min for this col:  -31.6\n",
      "max for this col:  26.7\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr\n",
      "dtype before:  float64\n",
      "min for this col:  -1.0\n",
      "max for this col:  597.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  humidity\n",
      "dtype before:  float64\n",
      "min for this col:  2.12834523177749\n",
      "max for this col:  100.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  heat_index\n",
      "dtype before:  float64\n",
      "min for this col:  -37.83\n",
      "max for this col:  43.13\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  feels_like\n",
      "dtype before:  float64\n",
      "min for this col:  -44.94\n",
      "max for this col:  48.3\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  ave_temp\n",
      "dtype before:  float64\n",
      "min for this col:  -29.85\n",
      "max for this col:  30.25\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -26.42\n",
      "max for this col:  47.6\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -28.1\n",
      "max for this col:  46.7\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  14.83\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -30.42\n",
      "max for this col:  26.5\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -31.6\n",
      "max for this col:  26.1\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  15.336\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  9.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  9.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  4.93\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -1.0\n",
      "max for this col:  137.2\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -1.0\n",
      "max for this col:  105.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  241.6\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  ave_temp_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -28.34\n",
      "max for this col:  29.72\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  ave_temp_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -29.84\n",
      "max for this col:  29.4\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  ave_temp_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  14.21\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  humidity_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  2.78\n",
      "max for this col:  100.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  humidity_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  2.129\n",
      "max for this col:  100.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  humidity_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  35.56\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  heat_index_mean_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -36.06\n",
      "max for this col:  42.34\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  heat_index_min_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  -37.84\n",
      "max for this col:  41.4\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  heat_index_std_lag6\n",
      "dtype before:  float16\n",
      "min for this col:  0.0\n",
      "max for this col:  15.11\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -27.904057971014492\n",
      "max for this col:  26.908510638297866\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -29.156891766882516\n",
      "max for this col:  30.446808510638306\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -9.0\n",
      "max for this col:  9.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -579.0\n",
      "max for this col:  597.0\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  ave_temp_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -26.954671600370027\n",
      "max for this col:  27.65957446808512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  humidity_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -64.73757530896768\n",
      "max for this col:  65.50396803652728\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  heat_index_diff1\n",
      "dtype before:  float64\n",
      "min for this col:  -28.740000000000002\n",
      "max for this col:  30.09\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  is_holiday\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  1\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  month\n",
      "dtype before:  int64\n",
      "min for this col:  1\n",
      "max for this col:  3\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  6799.974060058594  MB\n",
      "This is  62.40875912408759 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\n",
    "test[\"hour\"] = test[\"timestamp\"].dt.hour\n",
    "test[\"weekday\"] = test[\"timestamp\"].dt.weekday\n",
    "\n",
    "test['month'] = test['timestamp'].dt.month\n",
    "test['month'].replace((1, 2, 3, 12), 1, inplace = True)\n",
    "test['month'].replace((6, 7, 8, 9), 2, inplace = True)\n",
    "test['month'].replace((10, 11, 4, 5), 3, inplace = True)\n",
    "\n",
    "test = set_holiday(test)\n",
    "test = set_holiday2(test)\n",
    "\n",
    "test[\"puse_hour\"] = test[\"primary_use\"].astype(str) + '_' + test[\"hour\"].astype(str)\n",
    "test[\"holiday_hour\"] = test[\"is_holiday\"].astype(str) + '_' + test[\"hour\"].astype(str)\n",
    "encode_categorical_features(test, encoded_features, encoders)\n",
    "    \n",
    "test = test[feat_cols]\n",
    "test, NAlist = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 834/834 [14:20<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "i=0\n",
    "res=[]\n",
    "step_size = 50000\n",
    "for j in tqdm(range(int(np.ceil(test.shape[0]/50000)))):\n",
    "    for_prediction = get_keras_data(test.iloc[i:i+step_size], numericals, categoricals)\n",
    "    res.append(np.expm1(sum([model.predict(for_prediction, batch_size=1024) for model in models])/folds))\n",
    "    i+=step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>191.884705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85.250664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8.288164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>154.256226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1100.767090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697595</td>\n",
       "      <td>41697595</td>\n",
       "      <td>5.580867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697596</td>\n",
       "      <td>41697596</td>\n",
       "      <td>4.823460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697597</td>\n",
       "      <td>41697597</td>\n",
       "      <td>8.191454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697598</td>\n",
       "      <td>41697598</td>\n",
       "      <td>165.286911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697599</td>\n",
       "      <td>41697599</td>\n",
       "      <td>3.951975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0     191.884705\n",
       "1                1      85.250664\n",
       "2                2       8.288164\n",
       "3                3     154.256226\n",
       "4                4    1100.767090\n",
       "...            ...            ...\n",
       "41697595  41697595       5.580867\n",
       "41697596  41697596       4.823460\n",
       "41697597  41697597       8.191454\n",
       "41697598  41697598     165.286911\n",
       "41697599  41697599       3.951975\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.concatenate(res)\n",
    "submission = pd.read_csv('../sample_submission.csv')\n",
    "submission['meter_reading'] = res\n",
    "submission.loc[submission['meter_reading']<0, 'meter_reading'] = 0\n",
    "submission.to_csv(f'{filename}.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model():\n",
    "    \n",
    "#     model_input = Input(shape=(6,))\n",
    "#     num_input = Lambda(lambda x: x[:, :5], output_shape=(5,))(model_input)\n",
    "#     cat_input = Lambda(lambda x: x[:, 5:], output_shape=(1,))(model_input)\n",
    "    \n",
    "#     x = num_input\n",
    "#     x = Dense(256)(num_input)\n",
    "#     x = PReLU()(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(rate=0.5)(x)\n",
    "\n",
    "#     emb_dim = 128\n",
    "#     y = Embedding(200, emb_dim, input_length=1)(cat_input)\n",
    "#     y = Flatten()(y)\n",
    "    \n",
    "#     z = Concatenate()([x, y])\n",
    "#     z = Dense(256)(z)\n",
    "#     z = PReLU()(z)\n",
    "#     z = BatchNormalization()(z)\n",
    "#     z = Dropout(rate=0.5)(z)\n",
    "#     z = Dense(256)(z)\n",
    "#     z = PReLU()(z)\n",
    "#     z = BatchNormalization()(z)\n",
    "#     z = Dropout(rate=0.5)(z)\n",
    "#     z = Dense(256)(z)\n",
    "#     z = PReLU()(z)\n",
    "#     z = BatchNormalization()(z)\n",
    "#     output = Dense(1, activation=\"sigmoid\")(z)\n",
    "#     model = Model(inputs=model_input, outputs=output)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model_3():\n",
    "#     inp = keras.layers.Input((num_features*num_preds,))\n",
    "#     x = keras.layers.Reshape((num_features*num_preds,1))(inp)\n",
    "#     x = keras.layers.Conv1D(32,num_preds,strides=num_preds, activation='elu')(x)\n",
    "#     x = keras.layers.BatchNormalization()(x)\n",
    "#     x = keras.layers.Conv1D(24,1, activation='elu')(x)\n",
    "#     x = keras.layers.BatchNormalization()(x)\n",
    "#     x = keras.layers.Conv1D(16,1, activation='elu')(x)\n",
    "#     x = keras.layers.BatchNormalization()(x)\n",
    "#     x = keras.layers.Conv1D(4,1, activation='elu')(x)\n",
    "#     x = keras.layers.Flatten()(x)\n",
    "#     x = keras.layers.Reshape((num_features*4,1))(x)\n",
    "#     x = keras.layers.AveragePooling1D(2)(x)\n",
    "#     x = keras.layers.Flatten()(x)\n",
    "#     x = keras.layers.BatchNormalization()(x)\n",
    "#     out = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "#     return keras.Model(inputs=inp, outputs=out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
