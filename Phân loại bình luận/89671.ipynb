{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix, vstack\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.linear_model import *\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "#import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost.sklearn  import XGBClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = wordcloud.WordCloud(width=1440, height=1080).generate(\" \".join(df.comment.astype(str)))\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def extract_emojis(str):\n",
    "    return [c for c in str if c in emoji.UNICODE_EMOJI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_df = train_df[train_df['label'] == 0]\n",
    "good_comment = good_df['comment'].values\n",
    "good_emoji = []\n",
    "for c in good_comment:\n",
    "    good_emoji += extract_emojis(c)\n",
    "\n",
    "good_emoji = np.unique(np.asarray(good_emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_df = train_df[train_df['label'] == 1]\n",
    "bad_comment = bad_df['comment'].values\n",
    "\n",
    "bad_emoji = []\n",
    "for c in bad_comment:\n",
    "    bad_emoji += extract_emojis(c)\n",
    "\n",
    "bad_emoji = np.unique(np.asarray(bad_emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_emoji_fix = [\n",
    "    '↖', '↗', '☀', '☺', '♀', '♥', '✌', '✨', '❣', '❤', '⭐', '🆗', '^^',\n",
    "       '🌝', '🌟', '🌧', '🌷', '🌸', '🌺', '🌼', '🍓', '🎈', '🎉', '🐅', '🐾', '👉',\n",
    "       '👌', '👍', '👏', '💋', '💌', '💐', '💓', '💕', '💖', '💗', '💙', '💚', '💛',\n",
    "       '💜', '💞', '💟', '💥', '💪', '💮', '💯', '💰', '📑', '🖤', '😀', '😁', '😂',\n",
    "       '😃', '😄', '😅', '😆', '😇', '😉', '😊', '😋', '😌', '😍', '😎', '😑', '😓', '😔', \n",
    "    '😖', '😗', '😘', '😙', '😚', '😛', '😜', '😝', '😞', '😟', '😯', '😰', '😱', '😲', '😳', '😻', '🙂', '🙃', '🙄', '🙆', '🙌', '🤑', '🤔', '🤗',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just remove \"good\" emoji :D\n",
    "bad_emoji_fix = [\n",
    "    '☹', '✋', '❌', '❓', '👎', '👶', '💀',\n",
    "       '😐', '😑', '😒', '😓', '😔', ':(', 't^t', 't ^ t',\n",
    "       '😞', '😟', '😠', '😡', '😢', '😣', '😤', '😥', '😧', '😩', '😪', '😫', '😬',\n",
    "       '😭', '😳', '😵', '😶', '🙁', '🙄', '🤔',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_good_bad_emoji(row):\n",
    "    comment = row['comment']\n",
    "    n_good_emoji = 0\n",
    "    n_bad_emoji = 0\n",
    "    for c in comment:\n",
    "        if c in good_emoji_fix:\n",
    "            n_good_emoji += 1\n",
    "        if c in bad_emoji_fix:\n",
    "            n_bad_emoji += 1\n",
    "    \n",
    "    row['n_good_emoji'] = n_good_emoji\n",
    "    row['n_bad_emoji'] = n_bad_emoji\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].astype(str).fillna(' ')\n",
    "# df = df.apply(count_good_bad_emoji, axis=1)\n",
    "\n",
    "# df['count_sent']=df[\"comment\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "# #Word count in each comment:\n",
    "# df['count_word']=df[\"comment\"].apply(lambda x: len(str(x).split()))\n",
    "# #Unique word count\n",
    "# df['count_unique_word']=df[\"comment\"].apply(lambda x: len(set(str(x).split())))\n",
    "# #Letter count\n",
    "# df['count_letters']=df[\"comment\"].apply(lambda x: len(str(x)))\n",
    "# #punctuation count\n",
    "# df[\"count_punctuations\"] =df[\"comment\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "# #upper case words count\n",
    "# df[\"count_words_upper\"] = df[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "# df['words_vs_unique'] = df['count_unique_word'] / df['count_word'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_mapping = {\n",
    "    \"ship\": \"vận chuyển\",\n",
    "    \"shop\": \"cửa hàng\",\n",
    "    \"mik\": \" mình \",\n",
    "    \"ko\": \"không\",\n",
    "    \"tl\": \"trả lời\",\n",
    "    \"fb\": \"mạng xã hội\", \n",
    "    \"face\": \"mạng xã hội\",\n",
    "    \"thanks\": \"cảm ơn\",\n",
    "    \"thank\": \"cảm ơn\",\n",
    "    \"tks\": \"cảm ơn\", \n",
    "    \"tk\": \"cảm ơn\",\n",
    "    \"sp\": \"sản phẩm\",\n",
    "    \"dc\": \"được\",\n",
    "    \"đc\": \"được\",\n",
    "    \"đx\": \"được\",\n",
    "    \"nhjet\": \"nhiệt\",\n",
    "    \"inb\":\"inbox\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Dung dc sp tot cam on \\r\\nshop Đóng gói sản ph...\n",
       "1         Chất lượng sản phẩm tuyệt vời . Son mịn nhưng...\n",
       "2         Chất lượng sản phẩm tuyệt vời nhưng k có hộp ...\n",
       "3        :(( Mình hơi thất vọng 1 chút vì mình đã kỳ vọ...\n",
       "4        Lần trước mình mua áo gió màu hồng rất ok mà đ...\n",
       "5         Chất lượng sản phẩm tuyệt vời có điều không c...\n",
       "6        Đã nhận đc hàng rất nhanh mới đặt buổi tối mà ...\n",
       "7        Các siêu phẩm thấy cấu hình toàn tựa tựa nhau....\n",
       "8        Hàng ship nhanh  chất lượng tốt  tư vấn nhiệt ...\n",
       "9        Đồng hồ đẹp nhưng 1 cái đứt dây  1 cái k chạy ...\n",
       "10        Chất lượng sản phẩm tuyệt vời.y hình chụp.đán...\n",
       "11       Hjhj shop giao hàng nhanh quá. Đẹp lắm ạ bé nh...\n",
       "12                                   \"nhìn đẹp phết nhỉ..\"\n",
       "13       Đóng gói rất đẹp. Chất lượng sản phẩm rất tốt ...\n",
       "14                            Săn đc với giá 11k. Toẹt vời\n",
       "15                                         OK rất hài lòng\n",
       "16                 Giao thiếu mình cái này rồi shop ơi T^T\n",
       "17             Chất lượng sản phẩm tuyệt vời tôi rất thích\n",
       "18       Giày đẹp lắm có điều dây hơi ngắn tí ạ  Chất l...\n",
       "19                            Yếm vải đẹp nhưng ít mẫu đẹp\n",
       "20        Chất lượng sản phẩm tuyệt vời Đóng gói sản ph...\n",
       "21       không hài lòng sản phẩm cho lắm. giặt lan đầu ...\n",
       "22                 Giao hàng nhanh, mặc đẹp\\r\\nCám ơn shop\n",
       "23        Chất lượng sản phẩm tuyệt vời bao bì cute phô...\n",
       "24       Đồng hồ thì đẹp thật. Nhưng tại sao kim lúc ch...\n",
       "25       Giao hàng siêu nhanh.\\r\\nĐóng gói cẩn thận và ...\n",
       "26       \"Cũng hơi bất tiện xu thế này e rằng đa phằn n...\n",
       "27                   Toàn hàng trungkhi mua quên ko coi kĩ\n",
       "28        Đóng gói sản phẩm rất đẹp và chắc chắn. Được ...\n",
       "29       Hôm nay chiên thử cá hồi, cá chiên ăn ngọt hơn...\n",
       "                               ...                        \n",
       "27038     Thời gian giao hàng rất nhanh. Lần đầu mua hà...\n",
       "27039    Gà chưa vàng.gia vị chưa thấm.đặt loại cay mà ...\n",
       "27040     Chất lượng sản phẩm tuyệt vời Đóng gói sản ph...\n",
       "27041    Sữa tắm không thơm lắm giao hàng nhanh đóng gó...\n",
       "27042    Mới giao mà nút chai dầu gội đã bị gãy  Chất l...\n",
       "27043    Giao hàng nhanh. Shop nhjet tình. Máy thì siêu...\n",
       "27044                             Shop phục vụ nhiệt tình \n",
       "27045    Sản phẩm bị móp khi vận chuyển nhắn tin shop k...\n",
       "27046     Chất lượng sản phẩm tuyệt vời Đóng gói sản ph...\n",
       "27047                                 Cốm ngon tuyệt......\n",
       "27048                                         Gói hàng kém\n",
       "27049                                         Chuẩn mẫu.  \n",
       "27050    shop lam an k có tâm.đặt đơn 99k thi giao hang...\n",
       "27051    Nhận hàng xong là dùng thử luôn cảm giác ban đ...\n",
       "27052       Chất lượng sản phẩm tuyệt vời đúng như hình \n",
       "27053             Ko thơm bằng loại màu hồng mua ở bibomar\n",
       "27054    Nước giặt không có tem chính hãng nhãn dán lỏn...\n",
       "27055    Hôm nay mình xin không hài lòng vs nty shop vì...\n",
       "27056                             Shop gói hàng siêu kĩ ^^\n",
       "27057    Giao hàng lâu. Sai màu . Nhanh trôi. Hóng mãi ...\n",
       "27058                          Sản phẩm đẹp đúng như hình.\n",
       "27059     Chất lượng sản phẩm rất kém. Kh biết tại t xu...\n",
       "27060     Chất lượng sản phẩm tuyệt vời Đóng gói sản ph...\n",
       "27061                               Shop phục vụ rất tốt. \n",
       "27062         Bé mặc ko vừa mình muốn đổi size thanks shop\n",
       "27063     Thời gian giao hàng rất nhanh.ngon.mà cay quá...\n",
       "27064                                      Sản phẩm hơi cũ\n",
       "27065      Sản phẩm chắc chắn nhưng k bóng bằng trong hình\n",
       "27066     Chất lượng sản phẩm tuyệt vời có mùi thơm rất...\n",
       "27067                           như quảng cáo. sim rất tốt\n",
       "Name: comment, Length: 27068, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['comment']\n",
    "tokenizer=TweetTokenizer()\n",
    "lem = WordNetLemmatizer()\n",
    "def clean(comment):\n",
    "    \"\"\"\n",
    "    This function receives comments and returns clean word-list\n",
    "    \"\"\"\n",
    "    #Convert to lower case , so that Hi and hi are the same\n",
    "    comment=comment.lower()\n",
    "    #remove \\n\n",
    "    comment=re.sub(\"\\\\n\",\"\",comment)\n",
    "    # remove leaky elements like ip,user\n",
    "    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n",
    "    #removing usernames\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    \n",
    "    #Split the sentences into words\n",
    "    words=tokenizer.tokenize(comment)\n",
    "    \n",
    "    # (')aphostophe  replacement (ie)   you're --> you are  \n",
    "    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n",
    "#     words=[correct_mapping[word] if word in correct_mapping else word for word in words]\n",
    "#     words=[lem.lemmatize(word, \"v\") for word in words]\n",
    "#     words=[re.sub(r'(^r$)|(^r`$)', r'rồi', word) for word in words]\n",
    "#     words=[re.sub(r'(^k$)|(^kh$)', r'không', word) for word in words]\n",
    "#     words=[re.sub(r'(^m$)', r'mình', word) for word in words]\n",
    "\n",
    "    clean_sent = \" \".join(words)\n",
    "    # remove any non alphanum,digit character\n",
    "    #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n",
    "    #clean_sent=re.sub(\"  \",\" \",clean_sent)\n",
    "    return(clean_sent)\n",
    "\n",
    "clean_corpus=corpus.apply(lambda x :clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    min_df = 5,\n",
    "    sublinear_tf=True,\n",
    "    analyzer='char',\n",
    "    token_pattern=r'\\S',\n",
    "    ngram_range=(1, 6),\n",
    "    max_features=10000)\n",
    "char_vectorizer.fit(clean_corpus)\n",
    "features_char = np.array(char_vectorizer.get_feature_names())\n",
    "train_char_features = char_vectorizer.transform(clean_corpus.iloc[:train_df.shape[0]])\n",
    "test_char_features = char_vectorizer.transform(clean_corpus.iloc[train_df.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.words.txt', 'r', encoding = 'utf_8') as my_file:\n",
    "    all_words = {word.strip() for word in my_file.read().split('\\n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bởi_tại',\n",
       " 'kể_cả',\n",
       " '',\n",
       " 'dễ_ăn',\n",
       " 'vì_sao',\n",
       " 'sau',\n",
       " 'để_giống',\n",
       " 'nhận_thấy',\n",
       " 'vừa_qua',\n",
       " 'thế_mà',\n",
       " 'bằng_ấy',\n",
       " 'có_ngày',\n",
       " 'đủ',\n",
       " 'ăn_chắc',\n",
       " 'cho_đến_nỗi',\n",
       " 'với_lại',\n",
       " 'thuộc',\n",
       " 'nhận_làm',\n",
       " 'chú_khách',\n",
       " 'bỏ_không',\n",
       " 'hay_nói',\n",
       " 'cùng',\n",
       " 'cật_lực',\n",
       " 'dạ_dạ',\n",
       " 'phải_rồi',\n",
       " 'phải_cái',\n",
       " 'nói_thật',\n",
       " 'vung_tàn_tán',\n",
       " 'xem',\n",
       " 'từ_điều',\n",
       " 'ít_nữa',\n",
       " 'tìm',\n",
       " 'ngọn_nguồn',\n",
       " 'ắt_hẳn',\n",
       " 'tênh_tênh',\n",
       " 'đủ_dùng',\n",
       " 'chính_điểm',\n",
       " 'khác_nào',\n",
       " 'chịu_tốt',\n",
       " 'dùng_cho',\n",
       " 'phải_lời',\n",
       " 'nhóm',\n",
       " 'xoẹt',\n",
       " 'vốn_dĩ',\n",
       " 'giữ_ý',\n",
       " 'không_phải_không',\n",
       " 'luôn_tay',\n",
       " 'cả_nhà',\n",
       " 'amen',\n",
       " 'chỉ',\n",
       " 'ông_tạo',\n",
       " 'tắp_tắp',\n",
       " 'thế_nên',\n",
       " 'quan_tâm',\n",
       " 'đang_tay',\n",
       " 'bởi_nhưng',\n",
       " 'đặt_để',\n",
       " 'đặt_làm',\n",
       " 'ào_ào',\n",
       " 'do',\n",
       " 'ai',\n",
       " 'thường_đến',\n",
       " 'trước_kia',\n",
       " 'con_dạ',\n",
       " 'khi_không',\n",
       " 'nhỡ_ra',\n",
       " 'bằng_cứ',\n",
       " 'phải_lại',\n",
       " 'ra_chơi',\n",
       " 'cả',\n",
       " 'khoảng',\n",
       " 'nhất_thì',\n",
       " 'để_phần',\n",
       " 'nhân_dịp',\n",
       " 'ở_lại',\n",
       " 'tất_tật',\n",
       " 'nói_lại',\n",
       " 'về_không',\n",
       " 'vèo',\n",
       " 'nói_trước',\n",
       " 'ở_như',\n",
       " 'mở_nước',\n",
       " 'cho_thấy',\n",
       " 'làm_vì',\n",
       " 'đủ_điều',\n",
       " 'chứ_không',\n",
       " 'như_không',\n",
       " 'chính_là',\n",
       " 'ra_ngôi',\n",
       " 'đây_rồi',\n",
       " 'từng_thời_gian',\n",
       " 'cho_tin',\n",
       " 'mà_không',\n",
       " 'ví_thử',\n",
       " 'chọn_bên',\n",
       " 'lên_cao',\n",
       " 'nghe_tin',\n",
       " 'dù_cho',\n",
       " 'nhận_nhau',\n",
       " 'nhung_nhăng',\n",
       " 'số_cụ_thể',\n",
       " 'tránh_tình_trạng',\n",
       " 'thời_điểm',\n",
       " 'nước_ăn',\n",
       " 'làm_lại',\n",
       " 'ngày_rày',\n",
       " 'từ_nay',\n",
       " 'đến_nỗi',\n",
       " 'luôn',\n",
       " 'tiếp_đó',\n",
       " 'nhớ_lấy',\n",
       " 'không_được',\n",
       " 'câu_hỏi',\n",
       " 'lần_sau',\n",
       " 'nhận_biết',\n",
       " 'khách',\n",
       " 'không',\n",
       " 'để_cho',\n",
       " 'tôi_con',\n",
       " 'sau_đây',\n",
       " 'tha_hồ',\n",
       " 'vạn_nhất',\n",
       " 'biết_mấy',\n",
       " 'tăm_tắp',\n",
       " 'như_thế_nào',\n",
       " 'chứ_còn',\n",
       " 'nhớ',\n",
       " 'bất_kỳ',\n",
       " 'lấy_ra',\n",
       " 'cách_không',\n",
       " 'cây',\n",
       " 'khác_xa',\n",
       " 'vẫn',\n",
       " 'sau_cùng',\n",
       " 'chợt_nhìn',\n",
       " 'khó_khăn',\n",
       " 'có_người',\n",
       " 'rốt_cuộc',\n",
       " 'bắt_đầu_từ',\n",
       " 'bằng_không',\n",
       " 'chỉn',\n",
       " 'tự_ý',\n",
       " 'thuộc_từ',\n",
       " 'đúng_ngày',\n",
       " 'mới',\n",
       " 'lâu_nay',\n",
       " 'phải',\n",
       " 'cá_nhân',\n",
       " 'dầu_sao',\n",
       " 'cuốn',\n",
       " 'thấp',\n",
       " 'ngày_đến',\n",
       " 'vèo_vèo',\n",
       " 'à_ơi',\n",
       " 'nghĩ_xa',\n",
       " 'theo_bước',\n",
       " 'từ_thế',\n",
       " 'chứ_lại',\n",
       " 'bỏ_cha',\n",
       " 'thì_phải',\n",
       " 'quả_thật',\n",
       " 'xoành_xoạch',\n",
       " 'vung_tán_tàn',\n",
       " 'chắc_người',\n",
       " 'từ_đó',\n",
       " 'quả_thế',\n",
       " 'a_ha',\n",
       " 'đây_đó',\n",
       " 'đặc_biệt',\n",
       " 'thường_số',\n",
       " 'nhằm_lúc',\n",
       " 'thực_tế',\n",
       " 'những',\n",
       " 'theo_như',\n",
       " 'ít_biết',\n",
       " 'phải_cách',\n",
       " 'vào_lúc',\n",
       " 'các_cậu',\n",
       " 'rốt_cục',\n",
       " 'bỏ_ra',\n",
       " 'phè_phè',\n",
       " 'giữ',\n",
       " 'toé_khói',\n",
       " 'thỏm',\n",
       " 'thảo_hèn',\n",
       " 'nghĩ_lại',\n",
       " 'xuống',\n",
       " 'bấy_chầy',\n",
       " 'nữa_khi',\n",
       " 'tăng_cấp',\n",
       " 'ở_đây',\n",
       " 'chung_ái',\n",
       " 'đâu_cũng',\n",
       " 'khác_nhau',\n",
       " 'bỏ_riêng',\n",
       " 'tìm_cách',\n",
       " 'chẳng_nữa',\n",
       " 'mọi_người',\n",
       " 'biết_bao',\n",
       " 'khiến',\n",
       " 'việc',\n",
       " 'ngay_cả',\n",
       " 'buổi_sớm',\n",
       " 'nói_nhỏ',\n",
       " 'trong_đó',\n",
       " 'bỗng_thấy',\n",
       " 'chịu_lời',\n",
       " 'gần_ngày',\n",
       " 'phía_bạn',\n",
       " 'hiểu',\n",
       " 'trước_khi',\n",
       " 'mọi_khi',\n",
       " 'đâu_nào',\n",
       " 'cổ_lai',\n",
       " 'bỗng',\n",
       " 'lần_lần',\n",
       " 'bỏ_xa',\n",
       " 'mỗi_lúc',\n",
       " 'bộ_điều',\n",
       " 'có_chứ',\n",
       " 'là_cùng',\n",
       " 'càng_hay',\n",
       " 'họ_gần',\n",
       " 'chớ_như',\n",
       " 'bất_nhược',\n",
       " 'bà',\n",
       " 'bông',\n",
       " 'sớm',\n",
       " 'từ_tính',\n",
       " 'ào_vào',\n",
       " 'nhờ_chuyển',\n",
       " 'nóc',\n",
       " 'nhìn_theo',\n",
       " 'riêng',\n",
       " 'nói_qua',\n",
       " 'tù_tì',\n",
       " 'ờ_ờ',\n",
       " 'bởi_ai',\n",
       " 'chúng_mình',\n",
       " 'sất',\n",
       " 'làm_tắp_lự',\n",
       " 'lên_xuống',\n",
       " 'hay_biết',\n",
       " 'phải_giờ',\n",
       " 'răng',\n",
       " 'bỏ',\n",
       " 'nhớ_bập_bõm',\n",
       " 'xoét',\n",
       " 'ngay_lúc_này',\n",
       " 'xem_lại',\n",
       " 'nay',\n",
       " 'tránh_khỏi',\n",
       " 'dù_dì',\n",
       " 'gặp_phải',\n",
       " 'trả_trước',\n",
       " 'làm_theo',\n",
       " 'tấm_các',\n",
       " 'nói_riêng',\n",
       " 'đến_giờ',\n",
       " 'dạ_con',\n",
       " 'mỗi',\n",
       " 'ứ_ừ',\n",
       " 'người_khác',\n",
       " 'thêm_chuyện',\n",
       " 'hay_tin',\n",
       " 'phụt',\n",
       " 'bển',\n",
       " 'mức',\n",
       " 'đến_hay',\n",
       " 'nhận_họ',\n",
       " 'chiếc',\n",
       " 'lấy_giống',\n",
       " 'lên',\n",
       " 'vài_tên',\n",
       " 'đáng_kể',\n",
       " 'nói_thêm',\n",
       " 'ngay_khi_đến',\n",
       " 'cách',\n",
       " 'làm_ngay',\n",
       " 'bất_quá',\n",
       " 'như_thế',\n",
       " 'vậy',\n",
       " 'thoắt',\n",
       " 'là_phải',\n",
       " 'gây',\n",
       " 'xảy_ra',\n",
       " 'cơ_hồ',\n",
       " 'đại_phàm',\n",
       " 'dần_dần',\n",
       " 'mới_đây',\n",
       " 'ở_nhờ',\n",
       " 'chơi_họ',\n",
       " 'hết_ý',\n",
       " 'cái_đã',\n",
       " 'đều',\n",
       " 'tới_gần',\n",
       " 'rén_bước',\n",
       " 'do_vì',\n",
       " 'ngõ_hầu',\n",
       " 'âu_là',\n",
       " 'lấy_để',\n",
       " 'nước',\n",
       " 'quá_tay',\n",
       " 'cuộc',\n",
       " 'đang',\n",
       " 'đặt_ra',\n",
       " 'đi_khỏi',\n",
       " 'thửa',\n",
       " 'thoạt_nhiên',\n",
       " 'phót',\n",
       " 'qua_tay',\n",
       " 'lại_bộ',\n",
       " 'đưa_xuống',\n",
       " 'bỏ_cuộc',\n",
       " 'căn_cắt',\n",
       " 'cứ',\n",
       " 'thuần',\n",
       " 'cho_nên',\n",
       " 'bằng_người',\n",
       " 'ạ',\n",
       " 'nhằm_để',\n",
       " 'nức_nở',\n",
       " 'ăn_tay',\n",
       " 'qua_lại',\n",
       " 'tại_đây',\n",
       " 'nghe_được',\n",
       " 'phải_chăng',\n",
       " 'quay_bước',\n",
       " 'chắc_hẳn',\n",
       " 'gây_cho',\n",
       " 'ái_chà',\n",
       " 'biết_đâu_đấy',\n",
       " 'hết_ráo',\n",
       " 'chứ_không_phải',\n",
       " 'cả_người',\n",
       " 'đến',\n",
       " 'chăng_chắc',\n",
       " 'nếu_có',\n",
       " 'ào',\n",
       " 'vừa_rồi',\n",
       " 'những_như',\n",
       " 'cả_nghe',\n",
       " 'nhưng',\n",
       " 'cứ_điểm',\n",
       " 'đưa_ra',\n",
       " 'nghiễm_nhiên',\n",
       " 'mở',\n",
       " 'đáng_lí',\n",
       " 'tựu_trung',\n",
       " 'ắt_thật',\n",
       " 'thế_nào',\n",
       " 'ba_họ',\n",
       " 'ngoài_xa',\n",
       " 'đâu_có',\n",
       " 'lời_nói',\n",
       " 'cần_gì',\n",
       " 'cả_ngày',\n",
       " 'ừ',\n",
       " 'chính_thị',\n",
       " 'lấy_vào',\n",
       " 'đã_thế',\n",
       " 'tháng_năm',\n",
       " 'ăn_người',\n",
       " 'trả_ngay',\n",
       " 'đưa_em',\n",
       " 'ngay_lúc',\n",
       " 'ngày_này',\n",
       " 'vừa_lúc',\n",
       " 'dì',\n",
       " 'nếu_cần',\n",
       " 'lúc_này',\n",
       " 'sao_bằng',\n",
       " 'cho',\n",
       " 'ăn_riêng',\n",
       " 'chành_chạnh',\n",
       " 'ít_hơn',\n",
       " 'nếu_mà',\n",
       " 'vả_chăng',\n",
       " 'gần',\n",
       " 'đáo_để',\n",
       " 'khác',\n",
       " 'ở_đó',\n",
       " 'số_phần',\n",
       " 'ngay_lập_tức',\n",
       " 'mà_vẫn',\n",
       " 'đến_gần',\n",
       " 'dành',\n",
       " 'nên_chăng',\n",
       " 'bây_nhiêu',\n",
       " 'cứ_như',\n",
       " 'tuy_là',\n",
       " 'tăng',\n",
       " 'lúc_sáng',\n",
       " 'xin_gặp',\n",
       " 'cũng_vậy',\n",
       " 'nghe_hiểu',\n",
       " 'chẳng_những',\n",
       " 'lại_còn',\n",
       " 'còn',\n",
       " 'gần_đến',\n",
       " 'ráo_cả',\n",
       " 'thì_thôi',\n",
       " 'bao_lâu',\n",
       " 'nguồn',\n",
       " 'có_ý',\n",
       " 'đi_đều',\n",
       " 'giữ_lấy',\n",
       " 'nước_cùng',\n",
       " 'hơn_nữa',\n",
       " 'nói_đủ',\n",
       " 'đâu_đâu',\n",
       " 'trong',\n",
       " 'rồi_sao',\n",
       " 'có_thể',\n",
       " 'thốt',\n",
       " 'quan_trọng',\n",
       " 'thường',\n",
       " 'chuyển',\n",
       " 'đúng',\n",
       " 'lượng_từ',\n",
       " 'lần_sang',\n",
       " 'nhằm_vào',\n",
       " 'năm',\n",
       " 'chú_mày',\n",
       " 'béng',\n",
       " 'sẽ',\n",
       " 'mà_thôi',\n",
       " 'khoảng_cách',\n",
       " 'thật_là',\n",
       " 'ráo_nước',\n",
       " 'nhưng_mà',\n",
       " 'đại_loại',\n",
       " 'hơn',\n",
       " 'chắc_lòng',\n",
       " 'ô_hay',\n",
       " 'suýt_nữa',\n",
       " 'mỗi_một',\n",
       " 'quay_số',\n",
       " 'lúc_khác',\n",
       " 'đưa_đến',\n",
       " 'nói_rõ',\n",
       " 'ô_hô',\n",
       " 'sau_nữa',\n",
       " 'vâng_chịu',\n",
       " 'qua_đi',\n",
       " 'như_sau',\n",
       " 'ở_vào',\n",
       " 'tuy_có',\n",
       " 'ông_từ',\n",
       " 'sự',\n",
       " 'xiết_bao',\n",
       " 'ăn_làm',\n",
       " '_sạch',\n",
       " 'lòng_không',\n",
       " 'đưa_về',\n",
       " 'thấy_tháng',\n",
       " 'nhằm',\n",
       " 'áng_như',\n",
       " 'trả',\n",
       " 'biết_chừng_nào',\n",
       " 'từ_tại',\n",
       " 'điểm',\n",
       " 'thuộc_lại',\n",
       " 'vô_hình_trung',\n",
       " 'phải_không',\n",
       " 'đơn_vị',\n",
       " 'bên_có',\n",
       " 'của_ngọt',\n",
       " 'chính_bản',\n",
       " 'vâng_dạ',\n",
       " 'nhận',\n",
       " 'ôi_thôi',\n",
       " 'nghe_rõ',\n",
       " 'trong_vùng',\n",
       " 'người',\n",
       " 'nhìn_thấy',\n",
       " 'nhìn_xuống',\n",
       " 'chị_bộ',\n",
       " 'thiếu_điểm',\n",
       " 'chùn_chũn',\n",
       " 'nhận_việc',\n",
       " 'anh',\n",
       " 'bất_cứ',\n",
       " 'cả_tin',\n",
       " 'cha_chả',\n",
       " 'rút_cục',\n",
       " 'nghe_ra',\n",
       " 'như_ý',\n",
       " 'rích',\n",
       " 'tỏ_vẻ',\n",
       " 'xệp',\n",
       " 'mà_lại',\n",
       " 'như_thể',\n",
       " 'không_dùng',\n",
       " 'chứ_lị',\n",
       " 'như_quả',\n",
       " 'quá_trình',\n",
       " 'không_khỏi',\n",
       " 'mang_lại',\n",
       " 'chưa_dễ',\n",
       " 'khi_trước',\n",
       " 'bà_ấy',\n",
       " 'vượt_quá',\n",
       " 'vô_vàn',\n",
       " 'đến_điều',\n",
       " 'vì_rằng',\n",
       " 'ra_tay',\n",
       " 'rõ_thật',\n",
       " 'bị',\n",
       " 'chúng_ta',\n",
       " 'không_phải',\n",
       " 'tuổi_cả',\n",
       " 'phắt',\n",
       " 'một_cách',\n",
       " 'từ_ái',\n",
       " 'cũng_được',\n",
       " 'cao_răng',\n",
       " 'ông',\n",
       " 'duy',\n",
       " 'tới_nơi',\n",
       " 'ba_cùng',\n",
       " 'lấy_lý_do',\n",
       " 'bấy_nay',\n",
       " 'cho_hay',\n",
       " 'phần_nhiều',\n",
       " 'bản',\n",
       " 'tính_phỏng',\n",
       " 'nữa_rồi',\n",
       " 'ư',\n",
       " 'hết_chuyện',\n",
       " 'mang_mang',\n",
       " 'rồi_ra',\n",
       " 'nghe_đâu_như',\n",
       " 'bản_bộ',\n",
       " 'đến_lời',\n",
       " 'mất_còn',\n",
       " 'phải_biết',\n",
       " 'làm_lấy',\n",
       " 'bởi_thế',\n",
       " 'tự_khi',\n",
       " 'thuộc_cách',\n",
       " 'tính_từ',\n",
       " 'ngày',\n",
       " 'đưa_cho',\n",
       " 'bấy_chừ',\n",
       " 'bởi_sao',\n",
       " 'quá_tin',\n",
       " 'mối',\n",
       " 'chị_ấy',\n",
       " 'vượt',\n",
       " 'tiếp_theo',\n",
       " 'còn_về',\n",
       " 'lại_thôi',\n",
       " 'đại_để',\n",
       " 'đầy',\n",
       " 'vậy_mà',\n",
       " 'chui_cha',\n",
       " 'phía_trên',\n",
       " 'lấy_thế',\n",
       " 'dễ_gì',\n",
       " 'quá_đáng',\n",
       " 'chết_nỗi',\n",
       " 'nếu_được',\n",
       " 'tin',\n",
       " 'tốt_bạn',\n",
       " 'hỏi',\n",
       " 'thanh_ba',\n",
       " 'bỏ_quá',\n",
       " 'làm_nên',\n",
       " 'nhà_khó',\n",
       " 'dưới',\n",
       " 'giờ_đây',\n",
       " 'không_còn',\n",
       " 'lên_cơn',\n",
       " 'đặt_mức',\n",
       " 'có_khi',\n",
       " 'đầu_tiên',\n",
       " 'sao_bản',\n",
       " 'thường_thôi',\n",
       " 'thì_ra',\n",
       " 'thế_sự',\n",
       " 'là_là',\n",
       " 'vừa_khi',\n",
       " 'ứ_hự',\n",
       " 'số',\n",
       " 'không_ai',\n",
       " 'oái',\n",
       " 'ăn_sáng',\n",
       " 'vâng_ý',\n",
       " 'đầy_tuổi',\n",
       " 'bất_chợt',\n",
       " 'ngồi_bệt',\n",
       " 'tránh',\n",
       " 'chia_sẻ',\n",
       " 'xa_nhà',\n",
       " 'tin_vào',\n",
       " 'nước_đến',\n",
       " 'ơi',\n",
       " 'tự',\n",
       " 'ra_ý',\n",
       " 'dù_gì',\n",
       " 'tránh_xa',\n",
       " 'họ_xa',\n",
       " 'thực_vậy',\n",
       " 'dùng',\n",
       " 'khó_tránh',\n",
       " 'dài_ra',\n",
       " 'ra_vào',\n",
       " 'nếu_thế',\n",
       " 'cô_tăng',\n",
       " 'ơi_là',\n",
       " 'dù_sao',\n",
       " 'chung_qui',\n",
       " 'tại_sao',\n",
       " 'lại_ăn',\n",
       " 'hay_sao',\n",
       " 'hết_rồi',\n",
       " 'phỏng_theo',\n",
       " 'có_nhiều',\n",
       " 'trả_của',\n",
       " 'ngày_nào',\n",
       " 'bỗng_đâu',\n",
       " 'quay_đi',\n",
       " 'dào',\n",
       " 'pho',\n",
       " 'người_người',\n",
       " 'bao_giờ',\n",
       " 'giờ',\n",
       " 'thứ_bản',\n",
       " 'nghen',\n",
       " 'tanh',\n",
       " 'dù_rằng',\n",
       " 'biết',\n",
       " 'ý_da',\n",
       " 'chưa_có',\n",
       " 'giá_trị_thực_tế',\n",
       " 'dùng_hết',\n",
       " 'cuối_cùng',\n",
       " 'giá_trị',\n",
       " 'vô_kể',\n",
       " 'giữa',\n",
       " 'phải_khi',\n",
       " 'cha',\n",
       " 'ren_rén',\n",
       " 'làm_riêng',\n",
       " 'quá',\n",
       " 'nào_hay',\n",
       " 'nên_chi',\n",
       " 'giờ_đi',\n",
       " 'lại_nói',\n",
       " 'bỏ_mẹ',\n",
       " 'khó_biết',\n",
       " 'kể_tới',\n",
       " 'bỗng_nhiên',\n",
       " 'phỏng_tính',\n",
       " 'nói_phải',\n",
       " 'cuối_điểm',\n",
       " 'tấn_tới',\n",
       " 'úi_chà',\n",
       " 'tự_tạo',\n",
       " 'thoạt',\n",
       " 'chớ_kể',\n",
       " 'sáng_ý',\n",
       " 'phía',\n",
       " 'làm_sao',\n",
       " 'tột_cùng',\n",
       " 'từng_phần',\n",
       " 'đó',\n",
       " 'trở_thành',\n",
       " 'thi_thoảng',\n",
       " 'dẫu_mà',\n",
       " 'mọi_lúc',\n",
       " 'nhiệt_liệt',\n",
       " 'nhìn_chung',\n",
       " 'dành_dành',\n",
       " 'bởi_vậy',\n",
       " 'gì_gì',\n",
       " 'khoảng_không',\n",
       " 'vì_chưng',\n",
       " 'rất',\n",
       " 'tại_lòng',\n",
       " 'sau_sau',\n",
       " 'trước_sau',\n",
       " 'lúc_ấy',\n",
       " 'trước_ngày',\n",
       " 'từ_giờ',\n",
       " 'lấy_làm',\n",
       " 'mọi_thứ',\n",
       " 'dữ_cách',\n",
       " 'bạn',\n",
       " 'trừ_phi',\n",
       " 'nói_chung',\n",
       " 'công_nhiên',\n",
       " 'cao_sang',\n",
       " 'ít_nhiều',\n",
       " 'tìm_bạn',\n",
       " 'lòng',\n",
       " 'cơn',\n",
       " 'mợ',\n",
       " 'làm_tôi',\n",
       " 'nên_làm',\n",
       " 'cơ',\n",
       " 'cảm_ơn',\n",
       " 'phốc',\n",
       " 'gây_thêm',\n",
       " 'vừa',\n",
       " 'về',\n",
       " 'cho_tới',\n",
       " 'nghĩ_ra',\n",
       " 'phía_sau',\n",
       " 'tăng_thế',\n",
       " 'mới_hay',\n",
       " 'đi',\n",
       " 'trệu_trạo',\n",
       " 'sử_dụng',\n",
       " 'tên',\n",
       " 'khác_thường',\n",
       " 'xềnh_xệch',\n",
       " 'vì_thế',\n",
       " 'nhất',\n",
       " 'điều',\n",
       " 'được_cái',\n",
       " 'đâu_đây',\n",
       " 'thế_đó',\n",
       " 'tháng_ngày',\n",
       " 'như_tuồng',\n",
       " 'nước_lên',\n",
       " 'tiện_thể',\n",
       " 'đã_vậy',\n",
       " 'không_đầy',\n",
       " 'gây_giống',\n",
       " 'trong_ngoài',\n",
       " 'bên_bị',\n",
       " 'vụt',\n",
       " 'ngoài_này',\n",
       " 'gây_ra',\n",
       " 'thậm',\n",
       " 'ái_dà',\n",
       " 'tất_thảy',\n",
       " 'chắc_vào',\n",
       " 'tránh_ra',\n",
       " 'trước_nhất',\n",
       " 'dạ_bán',\n",
       " 'cấp_số',\n",
       " 'tên_cái',\n",
       " 'chọn_ra',\n",
       " 'là',\n",
       " 'thiếu_gì',\n",
       " 'thuần_ái',\n",
       " 'tăng_chúng',\n",
       " 'cật_sức',\n",
       " 'về_sau',\n",
       " 'nhiều',\n",
       " 'hết_nói',\n",
       " 'thế',\n",
       " 'đến_thì',\n",
       " 'luôn_luôn',\n",
       " 'tột',\n",
       " 'ăn_hết',\n",
       " 'chao_ôi',\n",
       " 'hỏi_lại',\n",
       " 'cho_ăn',\n",
       " 'khó_chơi',\n",
       " 'khẳng_định',\n",
       " 'đúng_tuổi',\n",
       " 'ngăn_ngắt',\n",
       " 'duy_chỉ',\n",
       " 'lần_nào',\n",
       " 'cũng_vậy_thôi',\n",
       " 'bất_đồ',\n",
       " 'quá_bán',\n",
       " 'thường_khi',\n",
       " 'một',\n",
       " 'tập_trung',\n",
       " 'thấy',\n",
       " 'tìm_việc',\n",
       " 'lấy_lại',\n",
       " 'cao_xa',\n",
       " 'tốc_tả',\n",
       " 'tấn',\n",
       " 'nghe_nhìn',\n",
       " 'vả_lại',\n",
       " 'lâu_ngày',\n",
       " 'toẹt',\n",
       " 'nói_tốt',\n",
       " 'quả_là',\n",
       " 'hay_đâu',\n",
       " 'cùng_chung',\n",
       " 'không_gì',\n",
       " 'để_mà',\n",
       " 'xuất_kì_bất_ý',\n",
       " 'lúc_đến',\n",
       " 'đã_là',\n",
       " 'hết_cả',\n",
       " 'sẽ_hay',\n",
       " 'mở_mang',\n",
       " 'thương_ôi',\n",
       " 'tò_te',\n",
       " 'không_những',\n",
       " 'quá_nhiều',\n",
       " 'sự_việc',\n",
       " 'cả_năm',\n",
       " 'những_lúc',\n",
       " 'tối_ư',\n",
       " 'rất_lâu',\n",
       " 'tạo',\n",
       " 'thường_thường',\n",
       " 'nhờ',\n",
       " 'nhà_ngươi',\n",
       " 'giờ_đến',\n",
       " 'gặp_khó_khăn',\n",
       " 'cho_chắc',\n",
       " 'hầu_hết',\n",
       " 'những_muốn',\n",
       " 'thì_là',\n",
       " 'veo',\n",
       " 'tít_mù',\n",
       " 'bất_giác',\n",
       " 'tự_lượng',\n",
       " 'gặp',\n",
       " 'trỏng',\n",
       " 'á_à',\n",
       " 'ví_bằng',\n",
       " 'nghe_nói',\n",
       " 'nói_khó',\n",
       " 'ra_bài',\n",
       " 'ráo',\n",
       " 'thà_rằng',\n",
       " 'trong_lúc',\n",
       " 'chăng_nữa',\n",
       " 'đến_cùng',\n",
       " 'lần_theo',\n",
       " 'bây_giờ',\n",
       " 'bèn',\n",
       " 'với',\n",
       " 'chẳng_lẽ',\n",
       " 'ra_người',\n",
       " 'cơ_mà',\n",
       " 'không_có_gì',\n",
       " 'xa',\n",
       " 'nhìn_nhận',\n",
       " 'bởi_thế_cho_nên',\n",
       " 'ngọt',\n",
       " 'rồi_thì',\n",
       " 'lại_làm',\n",
       " 'ngay',\n",
       " 'gần_hết',\n",
       " 'điều_kiện',\n",
       " 'ra_lời',\n",
       " 'thếch',\n",
       " 'dần_dà',\n",
       " 'bất_kì',\n",
       " 'rõ_là',\n",
       " 'buổi',\n",
       " 'ra_sao',\n",
       " 'còn_như',\n",
       " 'ba_ba',\n",
       " 'ngày_càng',\n",
       " 'ở_trên',\n",
       " 'tạo_ý',\n",
       " 'chắc_ăn',\n",
       " 'biết_được',\n",
       " 'phần_sau',\n",
       " 'ít_thôi',\n",
       " 'hay',\n",
       " 'bị_chú',\n",
       " 'tốt',\n",
       " 'hỏi_xem',\n",
       " 'sau_chót',\n",
       " 'nhỏ',\n",
       " 'veo_veo',\n",
       " 'ngôi_nhà',\n",
       " 'nhau',\n",
       " 'thật_chắc',\n",
       " 'biết_đâu_chừng',\n",
       " 'phía_bên',\n",
       " 'sang_tay',\n",
       " 'trước_hết',\n",
       " 'trước_tuổi',\n",
       " 'vài',\n",
       " 'nhiên_hậu',\n",
       " 'vừa_mới',\n",
       " 'rõ',\n",
       " 'vô_luận',\n",
       " 'thật_ra',\n",
       " 'có_được',\n",
       " 'lên_ngôi',\n",
       " 'bất_tử',\n",
       " 'phần',\n",
       " 'dạ_dài',\n",
       " 'thực_ra',\n",
       " 'mạnh',\n",
       " 'thế_lại',\n",
       " 'quá_ư',\n",
       " 'thím',\n",
       " 'lấy_thêm',\n",
       " 'từng_cái',\n",
       " 'tha_hồ_chơi',\n",
       " 'biết_trước',\n",
       " 'thành_thử',\n",
       " 'hơn_hết',\n",
       " 'chầm_chập',\n",
       " 'lượng_cả',\n",
       " 'bỗng_dưng',\n",
       " 'thế_ra',\n",
       " 'chớ_không',\n",
       " 'cao_thấp',\n",
       " 'làm',\n",
       " 'nó',\n",
       " 'ngồi_không',\n",
       " 'từ_từ',\n",
       " 'ăn_cuộc',\n",
       " 'tạo_cơ_hội',\n",
       " 'nhé',\n",
       " 'trực_tiếp_làm',\n",
       " 'lại_nữa',\n",
       " 'nhận_ra',\n",
       " 'cho_về',\n",
       " 'vùng_lên',\n",
       " 'đáng_số',\n",
       " 'rằng',\n",
       " 'kể',\n",
       " 'đối_với',\n",
       " 'nhón_nhén',\n",
       " 'tha_hồ_ăn',\n",
       " 'hoặc_là',\n",
       " 'từng_nhà',\n",
       " 'làm_bằng',\n",
       " 'ít_khi',\n",
       " 'thà',\n",
       " 'chứ_như',\n",
       " 'bản_thân',\n",
       " 'thời_gian_tính',\n",
       " 'đạt',\n",
       " 'ông_nhỏ',\n",
       " 'bỏ_việc',\n",
       " 'số_loại',\n",
       " 'so_với',\n",
       " 'ngôi',\n",
       " 'thích_thuộc',\n",
       " 'đi_trước',\n",
       " 'bản_ý',\n",
       " 'nhớ_lại',\n",
       " 'có_tháng',\n",
       " 'phải_người',\n",
       " 'rồi',\n",
       " 'hết_của',\n",
       " 'số_cho_biết',\n",
       " 'sang_sáng',\n",
       " 'biết_đâu',\n",
       " 'cao_ráo',\n",
       " 'hỗ_trợ',\n",
       " 'chưa_tính',\n",
       " 'cậu',\n",
       " 'lấy_ráo',\n",
       " 'chuyển_đạt',\n",
       " 'trực_tiếp',\n",
       " 'như_trước',\n",
       " 'họ',\n",
       " 'chết_tiệt',\n",
       " 'đến_thế',\n",
       " 'thình_lình',\n",
       " 'ăn_hỏi',\n",
       " 'khó_nghĩ',\n",
       " 'nói_xa',\n",
       " 'giảm_thấp',\n",
       " 'tuy_thế',\n",
       " 'bỏ_mình',\n",
       " 'sau_hết',\n",
       " 'đầy_năm',\n",
       " 'buổi_ngày',\n",
       " 'bởi_chưng',\n",
       " 'ấy_là',\n",
       " 'tắp',\n",
       " 'dài_lời',\n",
       " 'nhà_việc',\n",
       " 'ra_đây',\n",
       " 'bằng_nào',\n",
       " 'này_nọ',\n",
       " 'quá_thì',\n",
       " 'phù_hợp',\n",
       " 'ngay_khi',\n",
       " 'cho_đang',\n",
       " 'xin',\n",
       " 'mọi_nơi',\n",
       " 'có_ăn',\n",
       " 'như_thường',\n",
       " 'phải_chi',\n",
       " 'bằng',\n",
       " 'từ_ấy',\n",
       " 'tốt_mối',\n",
       " 'không_biết',\n",
       " 'phải_tay',\n",
       " 'vào_gặp',\n",
       " 'muốn',\n",
       " 'nhờ_nhờ',\n",
       " 'phăn_phắt',\n",
       " 'chú_dẫn',\n",
       " 'thường_xuất_hiện',\n",
       " 'nhất_loạt',\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    min_df = 5,\n",
    "    max_df = 0.8,\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(1, 4),\n",
    "#    stop_words = all_words,\n",
    "    max_features=30000)\n",
    "word_vectorizer.fit(clean_corpus)\n",
    "features = np.array(word_vectorizer.get_feature_names())\n",
    "train_word_features = word_vectorizer.transform(clean_corpus.iloc[:train_df.shape[0]])\n",
    "test_word_features = word_vectorizer.transform(clean_corpus.iloc[train_df.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25725,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skew in numerical features: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count_words_upper</th>\n",
       "      <td>34.086162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_bad_emoji</th>\n",
       "      <td>32.608676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_good_emoji</th>\n",
       "      <td>20.515688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_punctuations</th>\n",
       "      <td>19.358625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_sent</th>\n",
       "      <td>8.956432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_letters</th>\n",
       "      <td>6.018465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_word</th>\n",
       "      <td>5.932763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_unique_word</th>\n",
       "      <td>3.806975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>0.311152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words_vs_unique</th>\n",
       "      <td>-2.544749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Skew\n",
       "count_words_upper   34.086162\n",
       "n_bad_emoji         32.608676\n",
       "n_good_emoji        20.515688\n",
       "count_punctuations  19.358625\n",
       "count_sent           8.956432\n",
       "count_letters        6.018465\n",
       "count_word           5.932763\n",
       "count_unique_word    3.806975\n",
       "label                0.311152\n",
       "words_vs_unique     -2.544749"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 skewed numerical features to Box Cox transform\n"
     ]
    }
   ],
   "source": [
    "skewness = skewness.loc[abs(skewness.Skew) > 0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    if 'label' not in feat:\n",
    "        df[feat] = boxcox1p(df[feat], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:train_df.shape[0]]\n",
    "test_df = df.iloc[train_df.shape[0]:]\n",
    "\n",
    "y_train = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUED_COLS = ['id', 'comment', 'label']\n",
    "static_cols = [c for c in train_df.columns if not c in EXCLUED_COLS]\n",
    "X_train_static = train_df[static_cols].values\n",
    "X_test_static = test_df[static_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack([train_word_features, csr_matrix(X_train_static)]).tocsr()\n",
    "X_test = hstack([test_word_features, csr_matrix(X_test_static)]).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "\n",
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location):\n",
    "    \"\"\"Returns trained word2vec\n",
    "\n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found w2vmodel\n"
     ]
    }
   ],
   "source": [
    "w2vec = get_word2vec(\n",
    "    MySentences(\n",
    "        train_df['comment'].values, \n",
    "    ),\n",
    "    'w2vmodel'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\n",
    "trainDataVecs = mean_embedding_vectorizer.fit_transform(train_df['comment'])\n",
    "testDataVecs = mean_embedding_vectorizer.fit_transform(test_df['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest to training data....\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "    \n",
    "print(\"Fitting random forest to training data....\")    \n",
    "forest = forest.fit(trainDataVecs, train_df[\"label\"])\n",
    "\n",
    "result = forest.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack([trainDataVecs, csr_matrix(X_train_static)]).toarray()\n",
    "X_test = hstack([testDataVecs, csr_matrix(X_test_static)]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StackNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class GNB(self,):\n",
    "\n",
    "def pr(y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "x = X_train\n",
    "test_x = X_test\n",
    "\n",
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=1.3, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r\n",
    "\n",
    "preds = np.zeros((len(test_df), 1))\n",
    "\n",
    "label_cols = ['label']\n",
    "m,r = get_mdl(train_df['label'])\n",
    "preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb1 = lgb.LGBMClassifier(max_depth=16, n_estimators=400,learning_rate=0.05,colsample_bytree=0.3,num_leaves=250,\n",
    "                          metric='auc',objective='binary', n_jobs=-1)\n",
    "lgb2 = lgb.LGBMClassifier(max_depth=17, n_estimators=400,learning_rate=0.05,colsample_bytree=0.3,num_leaves=120,\n",
    "                          metric='f1',objective='binary', n_jobs=-1)\n",
    "xgb1 = XGBClassifier(learning_rate =0.05,n_estimators=400,max_depth = 16,min_child_weight=3,gamma=0.5,subsample=0.8,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=13,\n",
    "                     metric='f1')\n",
    "xgb2 = XGBClassifier(learning_rate =0.04,n_estimators=400,max_depth = 15,min_child_weight=3,gamma=0.7,subsample=1,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=13,\n",
    "                     metric='f1')\n",
    "lr1 = LogisticRegression(C = 1.2, random_state=1)\n",
    "lr2 = LogisticRegression(C = 1.3, random_state=2019)\n",
    "rf1 = RandomForestClassifier (n_estimators=400, criterion=\"entropy\", max_depth=12, max_features=0.5, random_state=3)\n",
    "rf1_1 = RandomForestClassifier (n_estimators=500, criterion=\"entropy\", max_depth=11, max_features=0.5, random_state=2019)\n",
    "rf1_2 = RandomForestClassifier (n_estimators=500, criterion=\"entropy\", max_depth=11, max_features=0.5, random_state=2)\n",
    "rf2 = RandomForestClassifier(n_estimators=400, criterion=\"entropy\", max_depth=8, max_features=0.5, random_state=1)\n",
    "rf3 = RandomForestClassifier(n_estimators=500, criterion=\"entropy\", max_depth=7, max_features=0.5, random_state=2)\n",
    "et1 = ExtraTreesClassifier(n_estimators=400, criterion=\"entropy\", max_depth=10, max_features=0.5, random_state=2, n_jobs=-1)\n",
    "et2 = ExtraTreesClassifier(n_estimators=400, criterion=\"entropy\", max_depth=7, max_features=0.5, random_state=2019, n_jobs=-1)\n",
    "gbc1 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=7, max_features=0.5, random_state=1)\n",
    "gbc2 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=8, max_features=0.5, random_state=2019)\n",
    "dt1 = DecisionTreeClassifier(criterion='gini', max_depth=17, min_samples_split=2, max_features=0.5, random_state=1)\n",
    "dt2 = DecisionTreeClassifier(criterion='gini', max_depth=13, min_samples_split=2, max_features=0.5, random_state=5)\n",
    "cat1 = CatBoostClassifier(learning_rate=0.1, iterations=400, random_seed=1, logging_level='Silent')\n",
    "knn1 = KNeighborsClassifier(n_neighbors=5, leaf_size=30, p=2, n_jobs=-1) \n",
    "knn2 = KNeighborsClassifier(n_neighbors=4, leaf_size=30, p=2, n_jobs=-1) \n",
    "svm1 = svm.SVC(C = 20.0, kernel='rbf', random_state = 1)\n",
    "ada1 = AdaBoostClassifier(base_estimator=None, n_estimators=400, learning_rate=0.05, algorithm='SAMME.R', random_state=1)\n",
    "ada2 = AdaBoostClassifier(base_estimator=None, n_estimators=400, learning_rate=0.05, algorithm='SAMME.R', random_state=3)\n",
    "\n",
    "#models=[[gbc1],[rf2]]\n",
    "models=[[lgb1, xgb1, lr1, rf1, dt1, gbc1, rf1_1, ada1],[lgb2, xgb2, rf2, lr2, cat1, et2, gbc2, ada2, knn1, knn2],[rf3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Start of Level 0 ======================\n",
      "Input Dimensionality 25734 at Level 0 \n",
      "8 models included in Level 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 1/5 , model 0 , f1===0.878412 \n",
      "Level 0, fold 1/5 , model 1 , f1===0.872845 \n",
      "Level 0, fold 1/5 , model 2 , f1===0.881392 \n",
      "Level 0, fold 1/5 , model 3 , f1===0.832224 \n",
      "Level 0, fold 1/5 , model 4 , f1===0.824924 \n",
      "Level 0, fold 1/5 , model 5 , f1===0.875178 \n",
      "Level 0, fold 1/5 , model 6 , f1===0.831505 \n",
      "Level 0, fold 1/5 , model 7 , f1===0.843049 \n",
      "=========== end of fold 1 in level 0 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 2/5 , model 0 , f1===0.883209 \n",
      "Level 0, fold 2/5 , model 1 , f1===0.877043 \n",
      "Level 0, fold 2/5 , model 2 , f1===0.887712 \n",
      "Level 0, fold 2/5 , model 3 , f1===0.842353 \n",
      "Level 0, fold 2/5 , model 4 , f1===0.831090 \n",
      "Level 0, fold 2/5 , model 5 , f1===0.869971 \n",
      "Level 0, fold 2/5 , model 6 , f1===0.839906 \n",
      "Level 0, fold 2/5 , model 7 , f1===0.851375 \n",
      "=========== end of fold 2 in level 0 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 3/5 , model 0 , f1===0.890756 \n",
      "Level 0, fold 3/5 , model 1 , f1===0.885630 \n",
      "Level 0, fold 3/5 , model 2 , f1===0.892622 \n",
      "Level 0, fold 3/5 , model 3 , f1===0.837958 \n",
      "Level 0, fold 3/5 , model 4 , f1===0.819020 \n",
      "Level 0, fold 3/5 , model 5 , f1===0.878877 \n",
      "Level 0, fold 3/5 , model 6 , f1===0.832196 \n",
      "Level 0, fold 3/5 , model 7 , f1===0.854218 \n",
      "=========== end of fold 3 in level 0 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 4/5 , model 0 , f1===0.892450 \n",
      "Level 0, fold 4/5 , model 1 , f1===0.885877 \n",
      "Level 0, fold 4/5 , model 2 , f1===0.889365 \n",
      "Level 0, fold 4/5 , model 3 , f1===0.841022 \n",
      "Level 0, fold 4/5 , model 4 , f1===0.840819 \n",
      "Level 0, fold 4/5 , model 5 , f1===0.890792 \n",
      "Level 0, fold 4/5 , model 6 , f1===0.838431 \n",
      "Level 0, fold 4/5 , model 7 , f1===0.859208 \n",
      "=========== end of fold 4 in level 0 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 5/5 , model 0 , f1===0.874106 \n",
      "Level 0, fold 5/5 , model 1 , f1===0.874596 \n",
      "Level 0, fold 5/5 , model 2 , f1===0.885643 \n",
      "Level 0, fold 5/5 , model 3 , f1===0.830811 \n",
      "Level 0, fold 5/5 , model 4 , f1===0.820758 \n",
      "Level 0, fold 5/5 , model 5 , f1===0.877419 \n",
      "Level 0, fold 5/5 , model 6 , f1===0.828859 \n",
      "Level 0, fold 5/5 , model 7 , f1===0.841664 \n",
      "=========== end of fold 5 in level 0 ===========\n",
      "Level 0, model 0 , f1===0.883787 \n",
      "Level 0, model 1 , f1===0.879198 \n",
      "Level 0, model 2 , f1===0.887347 \n",
      "Level 0, model 3 , f1===0.836874 \n",
      "Level 0, model 4 , f1===0.827322 \n",
      "Level 0, model 5 , f1===0.878448 \n",
      "Level 0, model 6 , f1===0.834179 \n",
      "Level 0, model 7 , f1===0.849903 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimensionality of level 0 is 8 \n",
      "====================== End of Level 0 ======================\n",
      " level 0 lasted 5454.859944 seconds \n",
      "====================== Start of Level 1 ======================\n",
      "Input Dimensionality 8 at Level 1 \n",
      "10 models included in Level 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1, fold 1/5 , model 0 , f1===0.867002 \n",
      "Level 1, fold 1/5 , model 1 , f1===0.884724 \n",
      "Level 1, fold 1/5 , model 2 , f1===0.890372 \n",
      "Level 1, fold 1/5 , model 3 , f1===0.881952 \n",
      "Level 1, fold 1/5 , model 4 , f1===0.890060 \n",
      "Level 1, fold 1/5 , model 5 , f1===0.889045 \n",
      "Level 1, fold 1/5 , model 6 , f1===0.885281 \n",
      "Level 1, fold 1/5 , model 7 , f1===0.889667 \n",
      "Level 1, fold 1/5 , model 8 , f1===0.872401 \n",
      "Level 1, fold 1/5 , model 9 , f1===0.875433 \n",
      "=========== end of fold 1 in level 1 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1, fold 2/5 , model 0 , f1===0.867573 \n",
      "Level 1, fold 2/5 , model 1 , f1===0.886605 \n",
      "Level 1, fold 2/5 , model 2 , f1===0.889516 \n",
      "Level 1, fold 2/5 , model 3 , f1===0.888889 \n",
      "Level 1, fold 2/5 , model 4 , f1===0.888653 \n",
      "Level 1, fold 2/5 , model 5 , f1===0.890454 \n",
      "Level 1, fold 2/5 , model 6 , f1===0.886766 \n",
      "Level 1, fold 2/5 , model 7 , f1===0.890915 \n",
      "Level 1, fold 2/5 , model 8 , f1===0.880799 \n",
      "Level 1, fold 2/5 , model 9 , f1===0.883495 \n",
      "=========== end of fold 2 in level 1 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1, fold 3/5 , model 0 , f1===0.881878 \n",
      "Level 1, fold 3/5 , model 1 , f1===0.890830 \n",
      "Level 1, fold 3/5 , model 2 , f1===0.896727 \n",
      "Level 1, fold 3/5 , model 3 , f1===0.893254 \n",
      "Level 1, fold 3/5 , model 4 , f1===0.894718 \n",
      "Level 1, fold 3/5 , model 5 , f1===0.897781 \n",
      "Level 1, fold 3/5 , model 6 , f1===0.888564 \n",
      "Level 1, fold 3/5 , model 7 , f1===0.897075 \n",
      "Level 1, fold 3/5 , model 8 , f1===0.881965 \n",
      "Level 1, fold 3/5 , model 9 , f1===0.881766 \n",
      "=========== end of fold 3 in level 1 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1, fold 4/5 , model 0 , f1===0.885000 \n",
      "Level 1, fold 4/5 , model 1 , f1===0.897236 \n",
      "Level 1, fold 4/5 , model 2 , f1===0.899258 \n",
      "Level 1, fold 4/5 , model 3 , f1===0.893754 \n",
      "Level 1, fold 4/5 , model 4 , f1===0.896527 \n",
      "Level 1, fold 4/5 , model 5 , f1===0.900882 \n",
      "Level 1, fold 4/5 , model 6 , f1===0.894906 \n",
      "Level 1, fold 4/5 , model 7 , f1===0.899192 \n",
      "Level 1, fold 4/5 , model 8 , f1===0.887777 \n",
      "Level 1, fold 4/5 , model 9 , f1===0.882496 \n",
      "=========== end of fold 4 in level 1 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1, fold 5/5 , model 0 , f1===0.879828 \n",
      "Level 1, fold 5/5 , model 1 , f1===0.887234 \n",
      "Level 1, fold 5/5 , model 2 , f1===0.891173 \n",
      "Level 1, fold 5/5 , model 3 , f1===0.888809 \n",
      "Level 1, fold 5/5 , model 4 , f1===0.890071 \n",
      "Level 1, fold 5/5 , model 5 , f1===0.890619 \n",
      "Level 1, fold 5/5 , model 6 , f1===0.883970 \n",
      "Level 1, fold 5/5 , model 7 , f1===0.889203 \n",
      "Level 1, fold 5/5 , model 8 , f1===0.864535 \n",
      "Level 1, fold 5/5 , model 9 , f1===0.868835 \n",
      "=========== end of fold 5 in level 1 ===========\n",
      "Level 1, model 0 , f1===0.876256 \n",
      "Level 1, model 1 , f1===0.889326 \n",
      "Level 1, model 2 , f1===0.893409 \n",
      "Level 1, model 3 , f1===0.889332 \n",
      "Level 1, model 4 , f1===0.892006 \n",
      "Level 1, model 5 , f1===0.893756 \n",
      "Level 1, model 6 , f1===0.887897 \n",
      "Level 1, model 7 , f1===0.893210 \n",
      "Level 1, model 8 , f1===0.877495 \n",
      "Level 1, model 9 , f1===0.878405 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimensionality of level 1 is 10 \n",
      "====================== End of Level 1 ======================\n",
      " level 1 lasted 372.217851 seconds \n",
      "====================== Start of Level 2 ======================\n",
      "Input Dimensionality 10 at Level 2 \n",
      "1 models included in Level 2 \n",
      "Level 2, fold 1/5 , model 0 , f1===0.887091 \n",
      "=========== end of fold 1 in level 2 ===========\n",
      "Level 2, fold 2/5 , model 0 , f1===0.889201 \n",
      "=========== end of fold 2 in level 2 ===========\n",
      "Level 2, fold 3/5 , model 0 , f1===0.898034 \n",
      "=========== end of fold 3 in level 2 ===========\n",
      "Level 2, fold 4/5 , model 0 , f1===0.896283 \n",
      "=========== end of fold 4 in level 2 ===========\n",
      "Level 2, fold 5/5 , model 0 , f1===0.891720 \n",
      "=========== end of fold 5 in level 2 ===========\n",
      "Level 2, model 0 , f1===0.892466 \n",
      "Output dimensionality of level 2 is 1 \n",
      "====================== End of Level 2 ======================\n",
      " level 2 lasted 123.807996 seconds \n",
      "====================== End of fit ======================\n",
      " fit() lasted 5950.945961 seconds \n",
      "====================== Start of Level 0 ======================\n",
      "1 estimators included in Level 0 \n",
      "====================== Start of Level 1 ======================\n",
      "1 estimators included in Level 1 \n",
      "====================== Start of Level 2 ======================\n",
      "1 estimators included in Level 2 \n"
     ]
    }
   ],
   "source": [
    "from pystacknet.pystacknet import StackNetClassifier\n",
    "\n",
    "model = StackNetClassifier(\n",
    "    models, metric=\"f1\", \n",
    "    folds=5,\n",
    "    restacking=False, \n",
    "    use_retraining=True, \n",
    "    use_proba=True, \n",
    "    random_state=12345, n_jobs=1, verbose=1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds=model.predict_proba(X_test) ## 893951 893741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cls = np.argmax(preds, axis=1)\n",
    "pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission['label'] = pred_cls\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb1 = lgb.LGBMClassifier(max_depth=16, n_estimators=400,learning_rate=0.05,colsample_bytree=0.3,num_leaves=120,\n",
    "                          metric='f1',objective='binary', n_jobs=-1)\n",
    "lgb2 = lgb.LGBMClassifier(max_depth=17, n_estimators=400,learning_rate=0.04,colsample_bytree=0.3,num_leaves=200,\n",
    "                          metric='f1',objective='binary', n_jobs=-1)\n",
    "xgb1 = XGBClassifier(learning_rate =0.05,n_estimators=400,max_depth = 10,min_child_weight=3,gamma=0,subsample=0.8,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=-1,scale_pos_weight=1,seed=13,\n",
    "                     metric='f1')\n",
    "lr1 = LogisticRegression(C = 1.2, random_state=1)\n",
    "lr2 = LogisticRegression(C = 1.3, random_state=2019)\n",
    "rf1 = RandomForestClassifier (n_estimators=400, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=3)\n",
    "rf2 = RandomForestClassifier(n_estimators=400, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1)\n",
    "et1 = ExtraTreesClassifier(n_estimators=400, criterion=\"entropy\", max_depth=9, max_features=0.5, random_state=2, n_jobs=-1)\n",
    "gbc1 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=13, max_features=0.5, random_state=1)\n",
    "gbc2 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=12, max_features=0.5, random_state=2019)\n",
    "dt1 = DecisionTreeClassifier(criterion='gini', max_depth=13, min_samples_split=2, max_features=0.5, random_state=1)\n",
    "cat1 = CatBoostClassifier(learning_rate=0.1, iterations=400, random_seed=1, logging_level='Silent', random_state=1)\n",
    "knn1 = KNeighborsClassifier(n_neighbors=5, leaf_size=100, p=2, n_jobs=-1) \n",
    "svm1 = svm.SVC(C = 20.0, kernel='rbf', random_state = 1, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_models = [dt1, gbc1, rf1, lgb1, xgb1, lr1, et1, lgb2, lr2, gbc2]\n",
    "layer1_names = ['dt1', 'gbc1', 'rf1', 'lgb1', 'xgb1', 'lr1', 'et1', 'lgb2', 'lr2', 'gbc2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_models = [et1]\n",
    "layer1_names = ['et1']\n",
    "\n",
    "# X_train = hstack([trainDataVecs, train_word_features, csr_matrix(X_train_static)]).tocsr()\n",
    "# X_test = hstack([testDataVecs, test_word_features, csr_matrix(X_test_static)]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_train = np.zeros(shape=(len(train_df),len(layer1_models)))\n",
    "oof_test = np.zeros(shape=(len(test_df),len(layer1_models)))\n",
    "\n",
    "# Recording results\n",
    "layer1_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training et1\n",
      "Fold no 1\n",
      "Fold no 2\n",
      "Fold no 3\n",
      "Fold no 4\n",
      "Fold no 5\n",
      "Training CV score: 0.77526\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(layer1_models)):\n",
    "    print('\\n')\n",
    "    name = layer1_names[i]\n",
    "    model = layer1_models[i]\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "    print('Training %s' %name)\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "        print('Fold no %i'%(fold_+1))\n",
    "        trn_data = X_train[trn_idx]\n",
    "        trn_label = y_train[trn_idx]\n",
    "        val_data = X_train[val_idx]\n",
    "        val_label = y_train[val_idx]\n",
    "        if ('lgb' in name) or ('xgb' in name):\n",
    "            model.fit(X=trn_data, y=trn_label,\n",
    "                     eval_set=[(trn_data, trn_label), (val_data, val_label)],\n",
    "                     verbose=200)\n",
    "        else:\n",
    "            model.fit(X=trn_data, y=trn_label)\n",
    "        \n",
    "        oof_train[val_idx,i] = model.predict(val_data)\n",
    "        oof_test[:,i] += model.predict(X_test)/5\n",
    "        \n",
    "    score = f1_score(oof_train[:,i], y_train)\n",
    "    layer1_score.append(score)\n",
    "    print('Training CV score: %.5f' %score) #7603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2_models = [gbc1]\n",
    "layer2_names = ['gbc1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to record result\n",
    "train_pred = np.zeros(shape=(len(train_df),2))\n",
    "test_pred = np.zeros(shape=(len(test_df),2))\n",
    "\n",
    "layer2_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training gbc1\n",
      "Training score: 0.89246\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(layer2_models)):\n",
    "    print('\\n')\n",
    "    name = layer2_names[i]\n",
    "    model = layer2_models[i]\n",
    "    print('Training %s' %name)\n",
    "    model.fit(oof_train, y_train)\n",
    "    score = f1_score(model.predict_proba(oof_train).argmax(axis=1), y_train)\n",
    "    train_pred += model.predict_proba(oof_train)/len(layer2_models)\n",
    "    test_pred += model.predict_proba(oof_test)/len(layer2_models)\n",
    "    layer2_score.append(score)\n",
    "    print('Training score: %.5f' % score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission['label'] = test_pred\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "models = [\n",
    "    RandomForestClassifier (n_estimators=300, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n",
    "    ExtraTreesClassifier (n_estimators=300, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n",
    "    GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=5, max_features=0.5, random_state=1),\n",
    "    LogisticRegression(random_state=1)\n",
    "]\n",
    "\n",
    "def cross_val_and_predict(clf, X, y, X_test, nfolds):\n",
    "    kf = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=42)\n",
    "    \n",
    "    oof_preds = np.zeros((X.shape[0], 2))\n",
    "    sub_preds = np.zeros((X_test.shape[0], 2))\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        oof_preds[valid_idx] = clf.predict_proba(X_valid)\n",
    "        sub_preds += clf.predict_proba(X_test) / kf.n_splits\n",
    "        \n",
    "    return oof_preds, sub_preds\n",
    "\n",
    "sub_preds = []\n",
    "\n",
    "for clf in models:\n",
    "    oof_pred, sub_pred = cross_val_and_predict(clf, X_train, y_train, X_test, nfolds=5)\n",
    "    oof_pred_cls = oof_pred.argmax(axis=1)\n",
    "    oof_f1 = f1_score(y_pred=oof_pred_cls, y_true=y_train)\n",
    "    \n",
    "    print(clf.__class__)\n",
    "    print(f\"F1 CV: {oof_f1}\")\n",
    "    \n",
    "    sub_preds.append(sub_pred)\n",
    "\n",
    "sub_preds = np.asarray(sub_preds)\n",
    "sub_preds = sub_preds.mean(axis=0)\n",
    "sub_pred_cls = sub_preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.where((np.round(preds.ravel()) + sub_pred_cls + pred_cls) >= 2, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.65, 'learning_rate': 0.05, 'max_depth': 10, 'metric': 'f1', 'n_estimators': 600, 'num_leaves': 80, 'objective': 'binary', 'random_state': 1, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# Create parameters to search\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'binary',\n",
    "          'nthread': 3, # Updated from nthread\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5,\n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'binary_error'}\n",
    "\n",
    "gridParams = {\n",
    "    'max_depth':[10, 15, 20, 25],\n",
    "    'learning_rate': [0.02, 0.04, 0.05, 0.06],\n",
    "    'n_estimators': [500, 600],\n",
    "    'num_leaves': [80],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'random_state' : [1], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65],\n",
    "    'subsample' : [0.75],\n",
    "    'reg_alpha' : [1],\n",
    "    'reg_lambda' : [1],\n",
    "    'metric':['f1']\n",
    "    }\n",
    "\n",
    "# Create classifier to use. Note that parameters have to be input manually\n",
    "# not as a dict!\n",
    "mdl = lgb.LGBMClassifier(boosting_type= 'gbdt',\n",
    "          objective = 'binary',\n",
    "          n_jobs = -1, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'],\n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'],\n",
    "          subsample_freq = params['subsample_freq'],\n",
    "          min_split_gain = params['min_split_gain'],\n",
    "          min_child_weight = params['min_child_weight'],\n",
    "          min_child_samples = params['min_child_samples'],\n",
    "          scale_pos_weight = params['scale_pos_weight'])\n",
    "\n",
    "# To view the default model params:\n",
    "mdl.get_params().keys()\n",
    "\n",
    "# Create the grid\n",
    "grid = GridSearchCV(mdl, gridParams,\n",
    "                    verbose=0,\n",
    "                    cv=4,\n",
    "                    n_jobs=-1)\n",
    "# Run the grid\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 38.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best hyperparameters:\n",
      "{'max_depth': 17}\n"
     ]
    }
   ],
   "source": [
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        'max_depth': [16, 18, 20, 22]\n",
    "        }\n",
    "\n",
    "xgb1 = XGBClassifier(learning_rate =0.05,n_estimators=300,max_depth = 16,min_child_weight=3,gamma=0.5,subsample=0.8,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=1,scale_pos_weight=1,seed=13,\n",
    "                     metric='f1')\n",
    "\n",
    "folds = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "grid = GridSearchCV(estimator=xgb1, param_grid=params, scoring='f1', n_jobs=5, cv=skf.split(X_train,y_train), verbose=1 )\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('\\n Best hyperparameters:')\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
