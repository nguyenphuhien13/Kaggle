{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix, vstack\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.linear_model import *\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "#import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost.sklearn  import XGBClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = wordcloud.WordCloud(width=1440, height=1080).generate(\" \".join(df.comment.astype(str)))\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def extract_emojis(str):\n",
    "    return [c for c in str if c in emoji.UNICODE_EMOJI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_df = train_df[train_df['label'] == 0]\n",
    "good_comment = good_df['comment'].values\n",
    "good_emoji = []\n",
    "for c in good_comment:\n",
    "    good_emoji += extract_emojis(c)\n",
    "\n",
    "good_emoji = np.unique(np.asarray(good_emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_df = train_df[train_df['label'] == 1]\n",
    "bad_comment = bad_df['comment'].values\n",
    "\n",
    "bad_emoji = []\n",
    "for c in bad_comment:\n",
    "    bad_emoji += extract_emojis(c)\n",
    "\n",
    "bad_emoji = np.unique(np.asarray(bad_emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_emoji_fix = [\n",
    "    '‚Üñ', '‚Üó', '‚òÄ', '‚ò∫', '‚ôÄ', '‚ô•', '‚úå', '‚ú®', '‚ù£', '‚ù§', '‚≠ê', 'üÜó', '^^',\n",
    "       'üåù', 'üåü', 'üåß', 'üå∑', 'üå∏', 'üå∫', 'üåº', 'üçì', 'üéà', 'üéâ', 'üêÖ', 'üêæ', 'üëâ',\n",
    "       'üëå', 'üëç', 'üëè', 'üíã', 'üíå', 'üíê', 'üíì', 'üíï', 'üíñ', 'üíó', 'üíô', 'üíö', 'üíõ',\n",
    "       'üíú', 'üíû', 'üíü', 'üí•', 'üí™', 'üíÆ', 'üíØ', 'üí∞', 'üìë', 'üñ§', 'üòÄ', 'üòÅ', 'üòÇ',\n",
    "       'üòÉ', 'üòÑ', 'üòÖ', 'üòÜ', 'üòá', 'üòâ', 'üòä', 'üòã', 'üòå', 'üòç', 'üòé', 'üòë', 'üòì', 'üòî', \n",
    "    'üòñ', 'üòó', 'üòò', 'üòô', 'üòö', 'üòõ', 'üòú', 'üòù', 'üòû', 'üòü', 'üòØ', 'üò∞', 'üò±', 'üò≤', 'üò≥', 'üòª', 'üôÇ', 'üôÉ', 'üôÑ', 'üôÜ', 'üôå', 'ü§ë', 'ü§î', 'ü§ó',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just remove \"good\" emoji :D\n",
    "bad_emoji_fix = [\n",
    "    '‚òπ', '‚úã', '‚ùå', '‚ùì', 'üëé', 'üë∂', 'üíÄ',\n",
    "       'üòê', 'üòë', 'üòí', 'üòì', 'üòî', ':(', 't^t', 't ^ t',\n",
    "       'üòû', 'üòü', 'üò†', 'üò°', 'üò¢', 'üò£', 'üò§', 'üò•', 'üòß', 'üò©', 'üò™', 'üò´', 'üò¨',\n",
    "       'üò≠', 'üò≥', 'üòµ', 'üò∂', 'üôÅ', 'üôÑ', 'ü§î',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_good_bad_emoji(row):\n",
    "    comment = row['comment']\n",
    "    n_good_emoji = 0\n",
    "    n_bad_emoji = 0\n",
    "    for c in comment:\n",
    "        if c in good_emoji_fix:\n",
    "            n_good_emoji += 1\n",
    "        if c in bad_emoji_fix:\n",
    "            n_bad_emoji += 1\n",
    "    \n",
    "    row['n_good_emoji'] = n_good_emoji\n",
    "    row['n_bad_emoji'] = n_bad_emoji\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].astype(str).fillna(' ')\n",
    "# df = df.apply(count_good_bad_emoji, axis=1)\n",
    "\n",
    "# df['count_sent']=df[\"comment\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "# #Word count in each comment:\n",
    "# df['count_word']=df[\"comment\"].apply(lambda x: len(str(x).split()))\n",
    "# #Unique word count\n",
    "# df['count_unique_word']=df[\"comment\"].apply(lambda x: len(set(str(x).split())))\n",
    "# #Letter count\n",
    "# df['count_letters']=df[\"comment\"].apply(lambda x: len(str(x)))\n",
    "# #punctuation count\n",
    "# df[\"count_punctuations\"] =df[\"comment\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "# #upper case words count\n",
    "# df[\"count_words_upper\"] = df[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "# df['words_vs_unique'] = df['count_unique_word'] / df['count_word'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_mapping = {\n",
    "    \"ship\": \"v·∫≠n chuy·ªÉn\",\n",
    "    \"shop\": \"c·ª≠a h√†ng\",\n",
    "    \"mik\": \" m√¨nh \",\n",
    "    \"ko\": \"kh√¥ng\",\n",
    "    \"tl\": \"tr·∫£ l·ªùi\",\n",
    "    \"fb\": \"m·∫°ng x√£ h·ªôi\", \n",
    "    \"face\": \"m·∫°ng x√£ h·ªôi\",\n",
    "    \"thanks\": \"c·∫£m ∆°n\",\n",
    "    \"thank\": \"c·∫£m ∆°n\",\n",
    "    \"tks\": \"c·∫£m ∆°n\", \n",
    "    \"tk\": \"c·∫£m ∆°n\",\n",
    "    \"sp\": \"s·∫£n ph·∫©m\",\n",
    "    \"dc\": \"ƒë∆∞·ª£c\",\n",
    "    \"ƒëc\": \"ƒë∆∞·ª£c\",\n",
    "    \"ƒëx\": \"ƒë∆∞·ª£c\",\n",
    "    \"nhjet\": \"nhi·ªát\",\n",
    "    \"inb\":\"inbox\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Dung dc sp tot cam on \\r\\nshop ƒê√≥ng g√≥i s·∫£n ph...\n",
       "1         Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi . Son m·ªãn nh∆∞ng...\n",
       "2         Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi nh∆∞ng k c√≥ h·ªôp ...\n",
       "3        :(( M√¨nh h∆°i th·∫•t v·ªçng 1 ch√∫t v√¨ m√¨nh ƒë√£ k·ª≥ v·ªç...\n",
       "4        L·∫ßn tr∆∞·ªõc m√¨nh mua √°o gi√≥ m√†u h·ªìng r·∫•t ok m√† ƒë...\n",
       "5         Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi c√≥ ƒëi·ªÅu kh√¥ng c...\n",
       "6        ƒê√£ nh·∫≠n ƒëc h√†ng r·∫•t nhanh m·ªõi ƒë·∫∑t bu·ªïi t·ªëi m√† ...\n",
       "7        C√°c si√™u ph·∫©m th·∫•y c·∫•u h√¨nh to√†n t·ª±a t·ª±a nhau....\n",
       "8        H√†ng ship nhanh  ch·∫•t l∆∞·ª£ng t·ªët  t∆∞ v·∫•n nhi·ªát ...\n",
       "9        ƒê·ªìng h·ªì ƒë·∫πp nh∆∞ng 1 c√°i ƒë·ª©t d√¢y  1 c√°i k ch·∫°y ...\n",
       "10        Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi.y h√¨nh ch·ª•p.ƒë√°n...\n",
       "11       Hjhj shop giao h√†ng nhanh qu√°. ƒê·∫πp l·∫Øm ·∫° b√© nh...\n",
       "12                                   \"nh√¨n ƒë·∫πp ph·∫øt nh·ªâ..\"\n",
       "13       ƒê√≥ng g√≥i r·∫•t ƒë·∫πp. Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t t·ªët ...\n",
       "14                            SƒÉn ƒëc v·ªõi gi√° 11k. To·∫πt v·ªùi\n",
       "15                                         OK r·∫•t h√†i l√≤ng\n",
       "16                 Giao thi·∫øu m√¨nh c√°i n√†y r·ªìi shop ∆°i T^T\n",
       "17             Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi t√¥i r·∫•t th√≠ch\n",
       "18       Gi√†y ƒë·∫πp l·∫Øm c√≥ ƒëi·ªÅu d√¢y h∆°i ng·∫Øn t√≠ ·∫°¬† Ch·∫•t l...\n",
       "19                            Y·∫øm v·∫£i ƒë·∫πp nh∆∞ng √≠t m·∫´u ƒë·∫πp\n",
       "20        Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n ph...\n",
       "21       kh√¥ng h√†i l√≤ng s·∫£n ph·∫©m cho l·∫Øm. gi·∫∑t lan ƒë·∫ßu ...\n",
       "22                 Giao h√†ng nhanh, m·∫∑c ƒë·∫πp\\r\\nC√°m ∆°n shop\n",
       "23        Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi bao b√¨ cute ph√¥...\n",
       "24       ƒê·ªìng h·ªì th√¨ ƒë·∫πp th·∫≠t. Nh∆∞ng t·∫°i sao kim l√∫c ch...\n",
       "25       Giao h√†ng si√™u nhanh.\\r\\nƒê√≥ng g√≥i c·∫©n th·∫≠n v√† ...\n",
       "26       \"C≈©ng h∆°i b·∫•t ti·ªán xu th·∫ø n√†y e r·∫±ng ƒëa ph·∫±n n...\n",
       "27                   To√†n h√†ng trungkhi mua qu√™n ko coi kƒ©\n",
       "28        ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn. ƒê∆∞·ª£c ...\n",
       "29       H√¥m nay chi√™n th·ª≠ c√° h·ªìi, c√° chi√™n ƒÉn ng·ªçt h∆°n...\n",
       "                               ...                        \n",
       "27038     Th·ªùi gian giao h√†ng r·∫•t nhanh. L·∫ßn ƒë·∫ßu mua h√†...\n",
       "27039    G√† ch∆∞a v√†ng.gia v·ªã ch∆∞a th·∫•m.ƒë·∫∑t lo·∫°i cay m√† ...\n",
       "27040     Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n ph...\n",
       "27041    S·ªØa t·∫Øm kh√¥ng th∆°m l·∫Øm giao h√†ng nhanh ƒë√≥ng g√≥...\n",
       "27042    M·ªõi giao m√† n√∫t chai d·∫ßu g·ªôi ƒë√£ b·ªã g√£y¬† Ch·∫•t l...\n",
       "27043    Giao h√†ng nhanh. Shop nhjet t√¨nh. M√°y th√¨ si√™u...\n",
       "27044                             Shop ph·ª•c v·ª• nhi·ªát t√¨nh \n",
       "27045    S·∫£n ph·∫©m b·ªã m√≥p khi v·∫≠n chuy·ªÉn nh·∫Øn tin shop k...\n",
       "27046     Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n ph...\n",
       "27047                                 C·ªëm ngon tuy·ªát......\n",
       "27048                                         G√≥i h√†ng k√©m\n",
       "27049                                         Chu·∫©n m·∫´u.  \n",
       "27050    shop lam an k c√≥ t√¢m.ƒë·∫∑t ƒë∆°n 99k thi giao hang...\n",
       "27051    Nh·∫≠n h√†ng xong l√† d√πng th·ª≠ lu√¥n c·∫£m gi√°c ban ƒë...\n",
       "27052       Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒëuÃÅng nh∆∞ hiÃÄnh \n",
       "27053             Ko th∆°m b·∫±ng lo·∫°i m√†u h·ªìng mua ·ªü bibomar\n",
       "27054    N∆∞·ªõc gi·∫∑t kh√¥ng c√≥ tem ch√≠nh h√£ng nh√£n d√°n l·ªèn...\n",
       "27055    H√¥m nay m√¨nh xin kh√¥ng h√†i l√≤ng vs nty shop v√¨...\n",
       "27056                             Shop g√≥i h√†ng si√™u kƒ© ^^\n",
       "27057    Giao h√†ng l√¢u. Sai m√†u . Nhanh tr√¥i. H√≥ng m√£i ...\n",
       "27058                          S·∫£n ph·∫©m ƒë·∫πp ƒë√∫ng nh∆∞ h√¨nh.\n",
       "27059     Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t k√©m. Kh bi·∫øt t·∫°i t xu...\n",
       "27060     Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n ph...\n",
       "27061                               Shop ph·ª•c v·ª• r·∫•t t·ªët. \n",
       "27062         B√© m·∫∑c ko v·ª´a m√¨nh mu·ªën ƒë·ªïi size thanks shop\n",
       "27063     Th·ªùi gian giao h√†ng r·∫•t nhanh.ngon.m√† cay qu√°...\n",
       "27064                                      S·∫£n ph·∫©m h∆°i c≈©\n",
       "27065      S·∫£n ph·∫©m ch·∫Øc ch·∫Øn nh∆∞ng k b√≥ng b·∫±ng trong h√¨nh\n",
       "27066     Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi c√≥ m√πi th∆°m r·∫•t...\n",
       "27067                           nh∆∞ qu·∫£ng c√°o. sim r·∫•t t·ªët\n",
       "Name: comment, Length: 27068, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['comment']\n",
    "tokenizer=TweetTokenizer()\n",
    "lem = WordNetLemmatizer()\n",
    "def clean(comment):\n",
    "    \"\"\"\n",
    "    This function receives comments and returns clean word-list\n",
    "    \"\"\"\n",
    "    #Convert to lower case , so that Hi and hi are the same\n",
    "    comment=comment.lower()\n",
    "    #remove \\n\n",
    "    comment=re.sub(\"\\\\n\",\"\",comment)\n",
    "    # remove leaky elements like ip,user\n",
    "    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n",
    "    #removing usernames\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    \n",
    "    #Split the sentences into words\n",
    "    words=tokenizer.tokenize(comment)\n",
    "    \n",
    "    # (')aphostophe  replacement (ie)   you're --> you are  \n",
    "    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n",
    "#     words=[correct_mapping[word] if word in correct_mapping else word for word in words]\n",
    "#     words=[lem.lemmatize(word, \"v\") for word in words]\n",
    "#     words=[re.sub(r'(^r$)|(^r`$)', r'r·ªìi', word) for word in words]\n",
    "#     words=[re.sub(r'(^k$)|(^kh$)', r'kh√¥ng', word) for word in words]\n",
    "#     words=[re.sub(r'(^m$)', r'm√¨nh', word) for word in words]\n",
    "\n",
    "    clean_sent = \" \".join(words)\n",
    "    # remove any non alphanum,digit character\n",
    "    #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n",
    "    #clean_sent=re.sub(\"  \",\" \",clean_sent)\n",
    "    return(clean_sent)\n",
    "\n",
    "clean_corpus=corpus.apply(lambda x :clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    min_df = 5,\n",
    "    sublinear_tf=True,\n",
    "    analyzer='char',\n",
    "    token_pattern=r'\\S',\n",
    "    ngram_range=(1, 6),\n",
    "    max_features=10000)\n",
    "char_vectorizer.fit(clean_corpus)\n",
    "features_char = np.array(char_vectorizer.get_feature_names())\n",
    "train_char_features = char_vectorizer.transform(clean_corpus.iloc[:train_df.shape[0]])\n",
    "test_char_features = char_vectorizer.transform(clean_corpus.iloc[train_df.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.words.txt', 'r', encoding = 'utf_8') as my_file:\n",
    "    all_words = {word.strip() for word in my_file.read().split('\\n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b·ªüi_t·∫°i',\n",
       " 'k·ªÉ_c·∫£',\n",
       " '',\n",
       " 'd·ªÖ_ƒÉn',\n",
       " 'v√¨_sao',\n",
       " 'sau',\n",
       " 'ƒë·ªÉ_gi·ªëng',\n",
       " 'nh·∫≠n_th·∫•y',\n",
       " 'v·ª´a_qua',\n",
       " 'th·∫ø_m√†',\n",
       " 'b·∫±ng_·∫•y',\n",
       " 'c√≥_ng√†y',\n",
       " 'ƒë·ªß',\n",
       " 'ƒÉn_ch·∫Øc',\n",
       " 'cho_ƒë·∫øn_n·ªói',\n",
       " 'v·ªõi_l·∫°i',\n",
       " 'thu·ªôc',\n",
       " 'nh·∫≠n_l√†m',\n",
       " 'ch√∫_kh√°ch',\n",
       " 'b·ªè_kh√¥ng',\n",
       " 'hay_n√≥i',\n",
       " 'c√πng',\n",
       " 'c·∫≠t_l·ª±c',\n",
       " 'd·∫°_d·∫°',\n",
       " 'ph·∫£i_r·ªìi',\n",
       " 'ph·∫£i_c√°i',\n",
       " 'n√≥i_th·∫≠t',\n",
       " 'vung_t√†n_t√°n',\n",
       " 'xem',\n",
       " 't·ª´_ƒëi·ªÅu',\n",
       " '√≠t_n·ªØa',\n",
       " 't√¨m',\n",
       " 'ng·ªçn_ngu·ªìn',\n",
       " '·∫Øt_h·∫≥n',\n",
       " 't√™nh_t√™nh',\n",
       " 'ƒë·ªß_d√πng',\n",
       " 'ch√≠nh_ƒëi·ªÉm',\n",
       " 'kh√°c_n√†o',\n",
       " 'ch·ªãu_t·ªët',\n",
       " 'd√πng_cho',\n",
       " 'ph·∫£i_l·ªùi',\n",
       " 'nh√≥m',\n",
       " 'xo·∫πt',\n",
       " 'v·ªën_dƒ©',\n",
       " 'gi·ªØ_√Ω',\n",
       " 'kh√¥ng_ph·∫£i_kh√¥ng',\n",
       " 'lu√¥n_tay',\n",
       " 'c·∫£_nh√†',\n",
       " 'amen',\n",
       " 'ch·ªâ',\n",
       " '√¥ng_t·∫°o',\n",
       " 't·∫Øp_t·∫Øp',\n",
       " 'th·∫ø_n√™n',\n",
       " 'quan_t√¢m',\n",
       " 'ƒëang_tay',\n",
       " 'b·ªüi_nh∆∞ng',\n",
       " 'ƒë·∫∑t_ƒë·ªÉ',\n",
       " 'ƒë·∫∑t_l√†m',\n",
       " '√†o_√†o',\n",
       " 'do',\n",
       " 'ai',\n",
       " 'th∆∞·ªùng_ƒë·∫øn',\n",
       " 'tr∆∞·ªõc_kia',\n",
       " 'con_d·∫°',\n",
       " 'khi_kh√¥ng',\n",
       " 'nh·ª°_ra',\n",
       " 'b·∫±ng_c·ª©',\n",
       " 'ph·∫£i_l·∫°i',\n",
       " 'ra_ch∆°i',\n",
       " 'c·∫£',\n",
       " 'kho·∫£ng',\n",
       " 'nh·∫•t_th√¨',\n",
       " 'ƒë·ªÉ_ph·∫ßn',\n",
       " 'nh√¢n_d·ªãp',\n",
       " '·ªü_l·∫°i',\n",
       " 't·∫•t_t·∫≠t',\n",
       " 'n√≥i_l·∫°i',\n",
       " 'v·ªÅ_kh√¥ng',\n",
       " 'v√®o',\n",
       " 'n√≥i_tr∆∞·ªõc',\n",
       " '·ªü_nh∆∞',\n",
       " 'm·ªü_n∆∞·ªõc',\n",
       " 'cho_th·∫•y',\n",
       " 'l√†m_v√¨',\n",
       " 'ƒë·ªß_ƒëi·ªÅu',\n",
       " 'ch·ª©_kh√¥ng',\n",
       " 'nh∆∞_kh√¥ng',\n",
       " 'ch√≠nh_l√†',\n",
       " 'ra_ng√¥i',\n",
       " 'ƒë√¢y_r·ªìi',\n",
       " 't·ª´ng_th·ªùi_gian',\n",
       " 'cho_tin',\n",
       " 'm√†_kh√¥ng',\n",
       " 'v√≠_th·ª≠',\n",
       " 'ch·ªçn_b√™n',\n",
       " 'l√™n_cao',\n",
       " 'nghe_tin',\n",
       " 'd√π_cho',\n",
       " 'nh·∫≠n_nhau',\n",
       " 'nhung_nhƒÉng',\n",
       " 's·ªë_c·ª•_th·ªÉ',\n",
       " 'tr√°nh_t√¨nh_tr·∫°ng',\n",
       " 'th·ªùi_ƒëi·ªÉm',\n",
       " 'n∆∞·ªõc_ƒÉn',\n",
       " 'l√†m_l·∫°i',\n",
       " 'ng√†y_r√†y',\n",
       " 't·ª´_nay',\n",
       " 'ƒë·∫øn_n·ªói',\n",
       " 'lu√¥n',\n",
       " 'ti·∫øp_ƒë√≥',\n",
       " 'nh·ªõ_l·∫•y',\n",
       " 'kh√¥ng_ƒë∆∞·ª£c',\n",
       " 'c√¢u_h·ªèi',\n",
       " 'l·∫ßn_sau',\n",
       " 'nh·∫≠n_bi·∫øt',\n",
       " 'kh√°ch',\n",
       " 'kh√¥ng',\n",
       " 'ƒë·ªÉ_cho',\n",
       " 't√¥i_con',\n",
       " 'sau_ƒë√¢y',\n",
       " 'tha_h·ªì',\n",
       " 'v·∫°n_nh·∫•t',\n",
       " 'bi·∫øt_m·∫•y',\n",
       " 'tƒÉm_t·∫Øp',\n",
       " 'nh∆∞_th·∫ø_n√†o',\n",
       " 'ch·ª©_c√≤n',\n",
       " 'nh·ªõ',\n",
       " 'b·∫•t_k·ª≥',\n",
       " 'l·∫•y_ra',\n",
       " 'c√°ch_kh√¥ng',\n",
       " 'c√¢y',\n",
       " 'kh√°c_xa',\n",
       " 'v·∫´n',\n",
       " 'sau_c√πng',\n",
       " 'ch·ª£t_nh√¨n',\n",
       " 'kh√≥_khƒÉn',\n",
       " 'c√≥_ng∆∞·ªùi',\n",
       " 'r·ªët_cu·ªôc',\n",
       " 'b·∫Øt_ƒë·∫ßu_t·ª´',\n",
       " 'b·∫±ng_kh√¥ng',\n",
       " 'ch·ªân',\n",
       " 't·ª±_√Ω',\n",
       " 'thu·ªôc_t·ª´',\n",
       " 'ƒë√∫ng_ng√†y',\n",
       " 'm·ªõi',\n",
       " 'l√¢u_nay',\n",
       " 'ph·∫£i',\n",
       " 'c√°_nh√¢n',\n",
       " 'd·∫ßu_sao',\n",
       " 'cu·ªën',\n",
       " 'th·∫•p',\n",
       " 'ng√†y_ƒë·∫øn',\n",
       " 'v√®o_v√®o',\n",
       " '√†_∆°i',\n",
       " 'nghƒ©_xa',\n",
       " 'theo_b∆∞·ªõc',\n",
       " 't·ª´_th·∫ø',\n",
       " 'ch·ª©_l·∫°i',\n",
       " 'b·ªè_cha',\n",
       " 'th√¨_ph·∫£i',\n",
       " 'qu·∫£_th·∫≠t',\n",
       " 'xo√†nh_xo·∫°ch',\n",
       " 'vung_t√°n_t√†n',\n",
       " 'ch·∫Øc_ng∆∞·ªùi',\n",
       " 't·ª´_ƒë√≥',\n",
       " 'qu·∫£_th·∫ø',\n",
       " 'a_ha',\n",
       " 'ƒë√¢y_ƒë√≥',\n",
       " 'ƒë·∫∑c_bi·ªát',\n",
       " 'th∆∞·ªùng_s·ªë',\n",
       " 'nh·∫±m_l√∫c',\n",
       " 'th·ª±c_t·∫ø',\n",
       " 'nh·ªØng',\n",
       " 'theo_nh∆∞',\n",
       " '√≠t_bi·∫øt',\n",
       " 'ph·∫£i_c√°ch',\n",
       " 'v√†o_l√∫c',\n",
       " 'c√°c_c·∫≠u',\n",
       " 'r·ªët_c·ª•c',\n",
       " 'b·ªè_ra',\n",
       " 'ph√®_ph√®',\n",
       " 'gi·ªØ',\n",
       " 'to√©_kh√≥i',\n",
       " 'th·ªèm',\n",
       " 'th·∫£o_h√®n',\n",
       " 'nghƒ©_l·∫°i',\n",
       " 'xu·ªëng',\n",
       " 'b·∫•y_ch·∫ßy',\n",
       " 'n·ªØa_khi',\n",
       " 'tƒÉng_c·∫•p',\n",
       " '·ªü_ƒë√¢y',\n",
       " 'chung_√°i',\n",
       " 'ƒë√¢u_c≈©ng',\n",
       " 'kh√°c_nhau',\n",
       " 'b·ªè_ri√™ng',\n",
       " 't√¨m_c√°ch',\n",
       " 'ch·∫≥ng_n·ªØa',\n",
       " 'm·ªçi_ng∆∞·ªùi',\n",
       " 'bi·∫øt_bao',\n",
       " 'khi·∫øn',\n",
       " 'vi·ªác',\n",
       " 'ngay_c·∫£',\n",
       " 'bu·ªïi_s·ªõm',\n",
       " 'n√≥i_nh·ªè',\n",
       " 'trong_ƒë√≥',\n",
       " 'b·ªóng_th·∫•y',\n",
       " 'ch·ªãu_l·ªùi',\n",
       " 'g·∫ßn_ng√†y',\n",
       " 'ph√≠a_b·∫°n',\n",
       " 'hi·ªÉu',\n",
       " 'tr∆∞·ªõc_khi',\n",
       " 'm·ªçi_khi',\n",
       " 'ƒë√¢u_n√†o',\n",
       " 'c·ªï_lai',\n",
       " 'b·ªóng',\n",
       " 'l·∫ßn_l·∫ßn',\n",
       " 'b·ªè_xa',\n",
       " 'm·ªói_l√∫c',\n",
       " 'b·ªô_ƒëi·ªÅu',\n",
       " 'c√≥_ch·ª©',\n",
       " 'l√†_c√πng',\n",
       " 'c√†ng_hay',\n",
       " 'h·ªç_g·∫ßn',\n",
       " 'ch·ªõ_nh∆∞',\n",
       " 'b·∫•t_nh∆∞·ª£c',\n",
       " 'b√†',\n",
       " 'b√¥ng',\n",
       " 's·ªõm',\n",
       " 't·ª´_t√≠nh',\n",
       " '√†o_v√†o',\n",
       " 'nh·ªù_chuy·ªÉn',\n",
       " 'n√≥c',\n",
       " 'nh√¨n_theo',\n",
       " 'ri√™ng',\n",
       " 'n√≥i_qua',\n",
       " 't√π_t√¨',\n",
       " '·ªù_·ªù',\n",
       " 'b·ªüi_ai',\n",
       " 'ch√∫ng_m√¨nh',\n",
       " 's·∫•t',\n",
       " 'l√†m_t·∫Øp_l·ª±',\n",
       " 'l√™n_xu·ªëng',\n",
       " 'hay_bi·∫øt',\n",
       " 'ph·∫£i_gi·ªù',\n",
       " 'rƒÉng',\n",
       " 'b·ªè',\n",
       " 'nh·ªõ_b·∫≠p_b√µm',\n",
       " 'xo√©t',\n",
       " 'ngay_l√∫c_n√†y',\n",
       " 'xem_l·∫°i',\n",
       " 'nay',\n",
       " 'tr√°nh_kh·ªèi',\n",
       " 'd√π_d√¨',\n",
       " 'g·∫∑p_ph·∫£i',\n",
       " 'tr·∫£_tr∆∞·ªõc',\n",
       " 'l√†m_theo',\n",
       " 't·∫•m_c√°c',\n",
       " 'n√≥i_ri√™ng',\n",
       " 'ƒë·∫øn_gi·ªù',\n",
       " 'd·∫°_con',\n",
       " 'm·ªói',\n",
       " '·ª©_·ª´',\n",
       " 'ng∆∞·ªùi_kh√°c',\n",
       " 'th√™m_chuy·ªán',\n",
       " 'hay_tin',\n",
       " 'ph·ª•t',\n",
       " 'b·ªÉn',\n",
       " 'm·ª©c',\n",
       " 'ƒë·∫øn_hay',\n",
       " 'nh·∫≠n_h·ªç',\n",
       " 'chi·∫øc',\n",
       " 'l·∫•y_gi·ªëng',\n",
       " 'l√™n',\n",
       " 'v√†i_t√™n',\n",
       " 'ƒë√°ng_k·ªÉ',\n",
       " 'n√≥i_th√™m',\n",
       " 'ngay_khi_ƒë·∫øn',\n",
       " 'c√°ch',\n",
       " 'l√†m_ngay',\n",
       " 'b·∫•t_qu√°',\n",
       " 'nh∆∞_th·∫ø',\n",
       " 'v·∫≠y',\n",
       " 'tho·∫Øt',\n",
       " 'l√†_ph·∫£i',\n",
       " 'g√¢y',\n",
       " 'x·∫£y_ra',\n",
       " 'c∆°_h·ªì',\n",
       " 'ƒë·∫°i_ph√†m',\n",
       " 'd·∫ßn_d·∫ßn',\n",
       " 'm·ªõi_ƒë√¢y',\n",
       " '·ªü_nh·ªù',\n",
       " 'ch∆°i_h·ªç',\n",
       " 'h·∫øt_√Ω',\n",
       " 'c√°i_ƒë√£',\n",
       " 'ƒë·ªÅu',\n",
       " 't·ªõi_g·∫ßn',\n",
       " 'r√©n_b∆∞·ªõc',\n",
       " 'do_v√¨',\n",
       " 'ng√µ_h·∫ßu',\n",
       " '√¢u_l√†',\n",
       " 'l·∫•y_ƒë·ªÉ',\n",
       " 'n∆∞·ªõc',\n",
       " 'qu√°_tay',\n",
       " 'cu·ªôc',\n",
       " 'ƒëang',\n",
       " 'ƒë·∫∑t_ra',\n",
       " 'ƒëi_kh·ªèi',\n",
       " 'th·ª≠a',\n",
       " 'tho·∫°t_nhi√™n',\n",
       " 'ph√≥t',\n",
       " 'qua_tay',\n",
       " 'l·∫°i_b·ªô',\n",
       " 'ƒë∆∞a_xu·ªëng',\n",
       " 'b·ªè_cu·ªôc',\n",
       " 'cƒÉn_c·∫Øt',\n",
       " 'c·ª©',\n",
       " 'thu·∫ßn',\n",
       " 'cho_n√™n',\n",
       " 'b·∫±ng_ng∆∞·ªùi',\n",
       " '·∫°',\n",
       " 'nh·∫±m_ƒë·ªÉ',\n",
       " 'n·ª©c_n·ªü',\n",
       " 'ƒÉn_tay',\n",
       " 'qua_l·∫°i',\n",
       " 't·∫°i_ƒë√¢y',\n",
       " 'nghe_ƒë∆∞·ª£c',\n",
       " 'ph·∫£i_chƒÉng',\n",
       " 'quay_b∆∞·ªõc',\n",
       " 'ch·∫Øc_h·∫≥n',\n",
       " 'g√¢y_cho',\n",
       " '√°i_ch√†',\n",
       " 'bi·∫øt_ƒë√¢u_ƒë·∫•y',\n",
       " 'h·∫øt_r√°o',\n",
       " 'ch·ª©_kh√¥ng_ph·∫£i',\n",
       " 'c·∫£_ng∆∞·ªùi',\n",
       " 'ƒë·∫øn',\n",
       " 'chƒÉng_ch·∫Øc',\n",
       " 'n·∫øu_c√≥',\n",
       " '√†o',\n",
       " 'v·ª´a_r·ªìi',\n",
       " 'nh·ªØng_nh∆∞',\n",
       " 'c·∫£_nghe',\n",
       " 'nh∆∞ng',\n",
       " 'c·ª©_ƒëi·ªÉm',\n",
       " 'ƒë∆∞a_ra',\n",
       " 'nghi·ªÖm_nhi√™n',\n",
       " 'm·ªü',\n",
       " 'ƒë√°ng_l√≠',\n",
       " 't·ª±u_trung',\n",
       " '·∫Øt_th·∫≠t',\n",
       " 'th·∫ø_n√†o',\n",
       " 'ba_h·ªç',\n",
       " 'ngo√†i_xa',\n",
       " 'ƒë√¢u_c√≥',\n",
       " 'l·ªùi_n√≥i',\n",
       " 'c·∫ßn_g√¨',\n",
       " 'c·∫£_ng√†y',\n",
       " '·ª´',\n",
       " 'ch√≠nh_th·ªã',\n",
       " 'l·∫•y_v√†o',\n",
       " 'ƒë√£_th·∫ø',\n",
       " 'th√°ng_nƒÉm',\n",
       " 'ƒÉn_ng∆∞·ªùi',\n",
       " 'tr·∫£_ngay',\n",
       " 'ƒë∆∞a_em',\n",
       " 'ngay_l√∫c',\n",
       " 'ng√†y_n√†y',\n",
       " 'v·ª´a_l√∫c',\n",
       " 'd√¨',\n",
       " 'n·∫øu_c·∫ßn',\n",
       " 'l√∫c_n√†y',\n",
       " 'sao_b·∫±ng',\n",
       " 'cho',\n",
       " 'ƒÉn_ri√™ng',\n",
       " 'ch√†nh_ch·∫°nh',\n",
       " '√≠t_h∆°n',\n",
       " 'n·∫øu_m√†',\n",
       " 'v·∫£_chƒÉng',\n",
       " 'g·∫ßn',\n",
       " 'ƒë√°o_ƒë·ªÉ',\n",
       " 'kh√°c',\n",
       " '·ªü_ƒë√≥',\n",
       " 's·ªë_ph·∫ßn',\n",
       " 'ngay_l·∫≠p_t·ª©c',\n",
       " 'm√†_v·∫´n',\n",
       " 'ƒë·∫øn_g·∫ßn',\n",
       " 'd√†nh',\n",
       " 'n√™n_chƒÉng',\n",
       " 'b√¢y_nhi√™u',\n",
       " 'c·ª©_nh∆∞',\n",
       " 'tuy_l√†',\n",
       " 'tƒÉng',\n",
       " 'l√∫c_s√°ng',\n",
       " 'xin_g·∫∑p',\n",
       " 'c≈©ng_v·∫≠y',\n",
       " 'nghe_hi·ªÉu',\n",
       " 'ch·∫≥ng_nh·ªØng',\n",
       " 'l·∫°i_c√≤n',\n",
       " 'c√≤n',\n",
       " 'g·∫ßn_ƒë·∫øn',\n",
       " 'r√°o_c·∫£',\n",
       " 'th√¨_th√¥i',\n",
       " 'bao_l√¢u',\n",
       " 'ngu·ªìn',\n",
       " 'c√≥_√Ω',\n",
       " 'ƒëi_ƒë·ªÅu',\n",
       " 'gi·ªØ_l·∫•y',\n",
       " 'n∆∞·ªõc_c√πng',\n",
       " 'h∆°n_n·ªØa',\n",
       " 'n√≥i_ƒë·ªß',\n",
       " 'ƒë√¢u_ƒë√¢u',\n",
       " 'trong',\n",
       " 'r·ªìi_sao',\n",
       " 'c√≥_th·ªÉ',\n",
       " 'th·ªët',\n",
       " 'quan_tr·ªçng',\n",
       " 'th∆∞·ªùng',\n",
       " 'chuy·ªÉn',\n",
       " 'ƒë√∫ng',\n",
       " 'l∆∞·ª£ng_t·ª´',\n",
       " 'l·∫ßn_sang',\n",
       " 'nh·∫±m_v√†o',\n",
       " 'nƒÉm',\n",
       " 'ch√∫_m√†y',\n",
       " 'b√©ng',\n",
       " 's·∫Ω',\n",
       " 'm√†_th√¥i',\n",
       " 'kho·∫£ng_c√°ch',\n",
       " 'th·∫≠t_l√†',\n",
       " 'r√°o_n∆∞·ªõc',\n",
       " 'nh∆∞ng_m√†',\n",
       " 'ƒë·∫°i_lo·∫°i',\n",
       " 'h∆°n',\n",
       " 'ch·∫Øc_l√≤ng',\n",
       " '√¥_hay',\n",
       " 'su√Ωt_n·ªØa',\n",
       " 'm·ªói_m·ªôt',\n",
       " 'quay_s·ªë',\n",
       " 'l√∫c_kh√°c',\n",
       " 'ƒë∆∞a_ƒë·∫øn',\n",
       " 'n√≥i_r√µ',\n",
       " '√¥_h√¥',\n",
       " 'sau_n·ªØa',\n",
       " 'v√¢ng_ch·ªãu',\n",
       " 'qua_ƒëi',\n",
       " 'nh∆∞_sau',\n",
       " '·ªü_v√†o',\n",
       " 'tuy_c√≥',\n",
       " '√¥ng_t·ª´',\n",
       " 's·ª±',\n",
       " 'xi·∫øt_bao',\n",
       " 'ƒÉn_l√†m',\n",
       " '_s·∫°ch',\n",
       " 'l√≤ng_kh√¥ng',\n",
       " 'ƒë∆∞a_v·ªÅ',\n",
       " 'th·∫•y_th√°ng',\n",
       " 'nh·∫±m',\n",
       " '√°ng_nh∆∞',\n",
       " 'tr·∫£',\n",
       " 'bi·∫øt_ch·ª´ng_n√†o',\n",
       " 't·ª´_t·∫°i',\n",
       " 'ƒëi·ªÉm',\n",
       " 'thu·ªôc_l·∫°i',\n",
       " 'v√¥_h√¨nh_trung',\n",
       " 'ph·∫£i_kh√¥ng',\n",
       " 'ƒë∆°n_v·ªã',\n",
       " 'b√™n_c√≥',\n",
       " 'c·ªßa_ng·ªçt',\n",
       " 'ch√≠nh_b·∫£n',\n",
       " 'v√¢ng_d·∫°',\n",
       " 'nh·∫≠n',\n",
       " '√¥i_th√¥i',\n",
       " 'nghe_r√µ',\n",
       " 'trong_v√πng',\n",
       " 'ng∆∞·ªùi',\n",
       " 'nh√¨n_th·∫•y',\n",
       " 'nh√¨n_xu·ªëng',\n",
       " 'ch·ªã_b·ªô',\n",
       " 'thi·∫øu_ƒëi·ªÉm',\n",
       " 'ch√πn_ch≈©n',\n",
       " 'nh·∫≠n_vi·ªác',\n",
       " 'anh',\n",
       " 'b·∫•t_c·ª©',\n",
       " 'c·∫£_tin',\n",
       " 'cha_ch·∫£',\n",
       " 'r√∫t_c·ª•c',\n",
       " 'nghe_ra',\n",
       " 'nh∆∞_√Ω',\n",
       " 'r√≠ch',\n",
       " 't·ªè_v·∫ª',\n",
       " 'x·ªáp',\n",
       " 'm√†_l·∫°i',\n",
       " 'nh∆∞_th·ªÉ',\n",
       " 'kh√¥ng_d√πng',\n",
       " 'ch·ª©_l·ªã',\n",
       " 'nh∆∞_qu·∫£',\n",
       " 'qu√°_tr√¨nh',\n",
       " 'kh√¥ng_kh·ªèi',\n",
       " 'mang_l·∫°i',\n",
       " 'ch∆∞a_d·ªÖ',\n",
       " 'khi_tr∆∞·ªõc',\n",
       " 'b√†_·∫•y',\n",
       " 'v∆∞·ª£t_qu√°',\n",
       " 'v√¥_v√†n',\n",
       " 'ƒë·∫øn_ƒëi·ªÅu',\n",
       " 'v√¨_r·∫±ng',\n",
       " 'ra_tay',\n",
       " 'r√µ_th·∫≠t',\n",
       " 'b·ªã',\n",
       " 'ch√∫ng_ta',\n",
       " 'kh√¥ng_ph·∫£i',\n",
       " 'tu·ªïi_c·∫£',\n",
       " 'ph·∫Øt',\n",
       " 'm·ªôt_c√°ch',\n",
       " 't·ª´_√°i',\n",
       " 'c≈©ng_ƒë∆∞·ª£c',\n",
       " 'cao_rƒÉng',\n",
       " '√¥ng',\n",
       " 'duy',\n",
       " 't·ªõi_n∆°i',\n",
       " 'ba_c√πng',\n",
       " 'l·∫•y_l√Ω_do',\n",
       " 'b·∫•y_nay',\n",
       " 'cho_hay',\n",
       " 'ph·∫ßn_nhi·ªÅu',\n",
       " 'b·∫£n',\n",
       " 't√≠nh_ph·ªèng',\n",
       " 'n·ªØa_r·ªìi',\n",
       " '∆∞',\n",
       " 'h·∫øt_chuy·ªán',\n",
       " 'mang_mang',\n",
       " 'r·ªìi_ra',\n",
       " 'nghe_ƒë√¢u_nh∆∞',\n",
       " 'b·∫£n_b·ªô',\n",
       " 'ƒë·∫øn_l·ªùi',\n",
       " 'm·∫•t_c√≤n',\n",
       " 'ph·∫£i_bi·∫øt',\n",
       " 'l√†m_l·∫•y',\n",
       " 'b·ªüi_th·∫ø',\n",
       " 't·ª±_khi',\n",
       " 'thu·ªôc_c√°ch',\n",
       " 't√≠nh_t·ª´',\n",
       " 'ng√†y',\n",
       " 'ƒë∆∞a_cho',\n",
       " 'b·∫•y_ch·ª´',\n",
       " 'b·ªüi_sao',\n",
       " 'qu√°_tin',\n",
       " 'm·ªëi',\n",
       " 'ch·ªã_·∫•y',\n",
       " 'v∆∞·ª£t',\n",
       " 'ti·∫øp_theo',\n",
       " 'c√≤n_v·ªÅ',\n",
       " 'l·∫°i_th√¥i',\n",
       " 'ƒë·∫°i_ƒë·ªÉ',\n",
       " 'ƒë·∫ßy',\n",
       " 'v·∫≠y_m√†',\n",
       " 'chui_cha',\n",
       " 'ph√≠a_tr√™n',\n",
       " 'l·∫•y_th·∫ø',\n",
       " 'd·ªÖ_g√¨',\n",
       " 'qu√°_ƒë√°ng',\n",
       " 'ch·∫øt_n·ªói',\n",
       " 'n·∫øu_ƒë∆∞·ª£c',\n",
       " 'tin',\n",
       " 't·ªët_b·∫°n',\n",
       " 'h·ªèi',\n",
       " 'thanh_ba',\n",
       " 'b·ªè_qu√°',\n",
       " 'l√†m_n√™n',\n",
       " 'nh√†_kh√≥',\n",
       " 'd∆∞·ªõi',\n",
       " 'gi·ªù_ƒë√¢y',\n",
       " 'kh√¥ng_c√≤n',\n",
       " 'l√™n_c∆°n',\n",
       " 'ƒë·∫∑t_m·ª©c',\n",
       " 'c√≥_khi',\n",
       " 'ƒë·∫ßu_ti√™n',\n",
       " 'sao_b·∫£n',\n",
       " 'th∆∞·ªùng_th√¥i',\n",
       " 'th√¨_ra',\n",
       " 'th·∫ø_s·ª±',\n",
       " 'l√†_l√†',\n",
       " 'v·ª´a_khi',\n",
       " '·ª©_h·ª±',\n",
       " 's·ªë',\n",
       " 'kh√¥ng_ai',\n",
       " 'o√°i',\n",
       " 'ƒÉn_s√°ng',\n",
       " 'v√¢ng_√Ω',\n",
       " 'ƒë·∫ßy_tu·ªïi',\n",
       " 'b·∫•t_ch·ª£t',\n",
       " 'ng·ªìi_b·ªát',\n",
       " 'tr√°nh',\n",
       " 'chia_s·∫ª',\n",
       " 'xa_nh√†',\n",
       " 'tin_v√†o',\n",
       " 'n∆∞·ªõc_ƒë·∫øn',\n",
       " '∆°i',\n",
       " 't·ª±',\n",
       " 'ra_√Ω',\n",
       " 'd√π_g√¨',\n",
       " 'tr√°nh_xa',\n",
       " 'h·ªç_xa',\n",
       " 'th·ª±c_v·∫≠y',\n",
       " 'd√πng',\n",
       " 'kh√≥_tr√°nh',\n",
       " 'd√†i_ra',\n",
       " 'ra_v√†o',\n",
       " 'n·∫øu_th·∫ø',\n",
       " 'c√¥_tƒÉng',\n",
       " '∆°i_l√†',\n",
       " 'd√π_sao',\n",
       " 'chung_qui',\n",
       " 't·∫°i_sao',\n",
       " 'l·∫°i_ƒÉn',\n",
       " 'hay_sao',\n",
       " 'h·∫øt_r·ªìi',\n",
       " 'ph·ªèng_theo',\n",
       " 'c√≥_nhi·ªÅu',\n",
       " 'tr·∫£_c·ªßa',\n",
       " 'ng√†y_n√†o',\n",
       " 'b·ªóng_ƒë√¢u',\n",
       " 'quay_ƒëi',\n",
       " 'd√†o',\n",
       " 'pho',\n",
       " 'ng∆∞·ªùi_ng∆∞·ªùi',\n",
       " 'bao_gi·ªù',\n",
       " 'gi·ªù',\n",
       " 'th·ª©_b·∫£n',\n",
       " 'nghen',\n",
       " 'tanh',\n",
       " 'd√π_r·∫±ng',\n",
       " 'bi·∫øt',\n",
       " '√Ω_da',\n",
       " 'ch∆∞a_c√≥',\n",
       " 'gi√°_tr·ªã_th·ª±c_t·∫ø',\n",
       " 'd√πng_h·∫øt',\n",
       " 'cu·ªëi_c√πng',\n",
       " 'gi√°_tr·ªã',\n",
       " 'v√¥_k·ªÉ',\n",
       " 'gi·ªØa',\n",
       " 'ph·∫£i_khi',\n",
       " 'cha',\n",
       " 'ren_r√©n',\n",
       " 'l√†m_ri√™ng',\n",
       " 'qu√°',\n",
       " 'n√†o_hay',\n",
       " 'n√™n_chi',\n",
       " 'gi·ªù_ƒëi',\n",
       " 'l·∫°i_n√≥i',\n",
       " 'b·ªè_m·∫π',\n",
       " 'kh√≥_bi·∫øt',\n",
       " 'k·ªÉ_t·ªõi',\n",
       " 'b·ªóng_nhi√™n',\n",
       " 'ph·ªèng_t√≠nh',\n",
       " 'n√≥i_ph·∫£i',\n",
       " 'cu·ªëi_ƒëi·ªÉm',\n",
       " 't·∫•n_t·ªõi',\n",
       " '√∫i_ch√†',\n",
       " 't·ª±_t·∫°o',\n",
       " 'tho·∫°t',\n",
       " 'ch·ªõ_k·ªÉ',\n",
       " 's√°ng_√Ω',\n",
       " 'ph√≠a',\n",
       " 'l√†m_sao',\n",
       " 't·ªôt_c√πng',\n",
       " 't·ª´ng_ph·∫ßn',\n",
       " 'ƒë√≥',\n",
       " 'tr·ªü_th√†nh',\n",
       " 'thi_tho·∫£ng',\n",
       " 'd·∫´u_m√†',\n",
       " 'm·ªçi_l√∫c',\n",
       " 'nhi·ªát_li·ªát',\n",
       " 'nh√¨n_chung',\n",
       " 'd√†nh_d√†nh',\n",
       " 'b·ªüi_v·∫≠y',\n",
       " 'g√¨_g√¨',\n",
       " 'kho·∫£ng_kh√¥ng',\n",
       " 'v√¨_ch∆∞ng',\n",
       " 'r·∫•t',\n",
       " 't·∫°i_l√≤ng',\n",
       " 'sau_sau',\n",
       " 'tr∆∞·ªõc_sau',\n",
       " 'l√∫c_·∫•y',\n",
       " 'tr∆∞·ªõc_ng√†y',\n",
       " 't·ª´_gi·ªù',\n",
       " 'l·∫•y_l√†m',\n",
       " 'm·ªçi_th·ª©',\n",
       " 'd·ªØ_c√°ch',\n",
       " 'b·∫°n',\n",
       " 'tr·ª´_phi',\n",
       " 'n√≥i_chung',\n",
       " 'c√¥ng_nhi√™n',\n",
       " 'cao_sang',\n",
       " '√≠t_nhi·ªÅu',\n",
       " 't√¨m_b·∫°n',\n",
       " 'l√≤ng',\n",
       " 'c∆°n',\n",
       " 'm·ª£',\n",
       " 'l√†m_t√¥i',\n",
       " 'n√™n_l√†m',\n",
       " 'c∆°',\n",
       " 'c·∫£m_∆°n',\n",
       " 'ph·ªëc',\n",
       " 'g√¢y_th√™m',\n",
       " 'v·ª´a',\n",
       " 'v·ªÅ',\n",
       " 'cho_t·ªõi',\n",
       " 'nghƒ©_ra',\n",
       " 'ph√≠a_sau',\n",
       " 'tƒÉng_th·∫ø',\n",
       " 'm·ªõi_hay',\n",
       " 'ƒëi',\n",
       " 'tr·ªáu_tr·∫°o',\n",
       " 's·ª≠_d·ª•ng',\n",
       " 't√™n',\n",
       " 'kh√°c_th∆∞·ªùng',\n",
       " 'x·ªÅnh_x·ªách',\n",
       " 'v√¨_th·∫ø',\n",
       " 'nh·∫•t',\n",
       " 'ƒëi·ªÅu',\n",
       " 'ƒë∆∞·ª£c_c√°i',\n",
       " 'ƒë√¢u_ƒë√¢y',\n",
       " 'th·∫ø_ƒë√≥',\n",
       " 'th√°ng_ng√†y',\n",
       " 'nh∆∞_tu·ªìng',\n",
       " 'n∆∞·ªõc_l√™n',\n",
       " 'ti·ªán_th·ªÉ',\n",
       " 'ƒë√£_v·∫≠y',\n",
       " 'kh√¥ng_ƒë·∫ßy',\n",
       " 'g√¢y_gi·ªëng',\n",
       " 'trong_ngo√†i',\n",
       " 'b√™n_b·ªã',\n",
       " 'v·ª•t',\n",
       " 'ngo√†i_n√†y',\n",
       " 'g√¢y_ra',\n",
       " 'th·∫≠m',\n",
       " '√°i_d√†',\n",
       " 't·∫•t_th·∫£y',\n",
       " 'ch·∫Øc_v√†o',\n",
       " 'tr√°nh_ra',\n",
       " 'tr∆∞·ªõc_nh·∫•t',\n",
       " 'd·∫°_b√°n',\n",
       " 'c·∫•p_s·ªë',\n",
       " 't√™n_c√°i',\n",
       " 'ch·ªçn_ra',\n",
       " 'l√†',\n",
       " 'thi·∫øu_g√¨',\n",
       " 'thu·∫ßn_√°i',\n",
       " 'tƒÉng_ch√∫ng',\n",
       " 'c·∫≠t_s·ª©c',\n",
       " 'v·ªÅ_sau',\n",
       " 'nhi·ªÅu',\n",
       " 'h·∫øt_n√≥i',\n",
       " 'th·∫ø',\n",
       " 'ƒë·∫øn_th√¨',\n",
       " 'lu√¥n_lu√¥n',\n",
       " 't·ªôt',\n",
       " 'ƒÉn_h·∫øt',\n",
       " 'chao_√¥i',\n",
       " 'h·ªèi_l·∫°i',\n",
       " 'cho_ƒÉn',\n",
       " 'kh√≥_ch∆°i',\n",
       " 'kh·∫≥ng_ƒë·ªãnh',\n",
       " 'ƒë√∫ng_tu·ªïi',\n",
       " 'ngƒÉn_ng·∫Øt',\n",
       " 'duy_ch·ªâ',\n",
       " 'l·∫ßn_n√†o',\n",
       " 'c≈©ng_v·∫≠y_th√¥i',\n",
       " 'b·∫•t_ƒë·ªì',\n",
       " 'qu√°_b√°n',\n",
       " 'th∆∞·ªùng_khi',\n",
       " 'm·ªôt',\n",
       " 't·∫≠p_trung',\n",
       " 'th·∫•y',\n",
       " 't√¨m_vi·ªác',\n",
       " 'l·∫•y_l·∫°i',\n",
       " 'cao_xa',\n",
       " 't·ªëc_t·∫£',\n",
       " 't·∫•n',\n",
       " 'nghe_nh√¨n',\n",
       " 'v·∫£_l·∫°i',\n",
       " 'l√¢u_ng√†y',\n",
       " 'to·∫πt',\n",
       " 'n√≥i_t·ªët',\n",
       " 'qu·∫£_l√†',\n",
       " 'hay_ƒë√¢u',\n",
       " 'c√πng_chung',\n",
       " 'kh√¥ng_g√¨',\n",
       " 'ƒë·ªÉ_m√†',\n",
       " 'xu·∫•t_k√¨_b·∫•t_√Ω',\n",
       " 'l√∫c_ƒë·∫øn',\n",
       " 'ƒë√£_l√†',\n",
       " 'h·∫øt_c·∫£',\n",
       " 's·∫Ω_hay',\n",
       " 'm·ªü_mang',\n",
       " 'th∆∞∆°ng_√¥i',\n",
       " 't√≤_te',\n",
       " 'kh√¥ng_nh·ªØng',\n",
       " 'qu√°_nhi·ªÅu',\n",
       " 's·ª±_vi·ªác',\n",
       " 'c·∫£_nƒÉm',\n",
       " 'nh·ªØng_l√∫c',\n",
       " 't·ªëi_∆∞',\n",
       " 'r·∫•t_l√¢u',\n",
       " 't·∫°o',\n",
       " 'th∆∞·ªùng_th∆∞·ªùng',\n",
       " 'nh·ªù',\n",
       " 'nh√†_ng∆∞∆°i',\n",
       " 'gi·ªù_ƒë·∫øn',\n",
       " 'g·∫∑p_kh√≥_khƒÉn',\n",
       " 'cho_ch·∫Øc',\n",
       " 'h·∫ßu_h·∫øt',\n",
       " 'nh·ªØng_mu·ªën',\n",
       " 'th√¨_l√†',\n",
       " 'veo',\n",
       " 't√≠t_m√π',\n",
       " 'b·∫•t_gi√°c',\n",
       " 't·ª±_l∆∞·ª£ng',\n",
       " 'g·∫∑p',\n",
       " 'tr·ªèng',\n",
       " '√°_√†',\n",
       " 'v√≠_b·∫±ng',\n",
       " 'nghe_n√≥i',\n",
       " 'n√≥i_kh√≥',\n",
       " 'ra_b√†i',\n",
       " 'r√°o',\n",
       " 'th√†_r·∫±ng',\n",
       " 'trong_l√∫c',\n",
       " 'chƒÉng_n·ªØa',\n",
       " 'ƒë·∫øn_c√πng',\n",
       " 'l·∫ßn_theo',\n",
       " 'b√¢y_gi·ªù',\n",
       " 'b√®n',\n",
       " 'v·ªõi',\n",
       " 'ch·∫≥ng_l·∫Ω',\n",
       " 'ra_ng∆∞·ªùi',\n",
       " 'c∆°_m√†',\n",
       " 'kh√¥ng_c√≥_g√¨',\n",
       " 'xa',\n",
       " 'nh√¨n_nh·∫≠n',\n",
       " 'b·ªüi_th·∫ø_cho_n√™n',\n",
       " 'ng·ªçt',\n",
       " 'r·ªìi_th√¨',\n",
       " 'l·∫°i_l√†m',\n",
       " 'ngay',\n",
       " 'g·∫ßn_h·∫øt',\n",
       " 'ƒëi·ªÅu_ki·ªán',\n",
       " 'ra_l·ªùi',\n",
       " 'th·∫øch',\n",
       " 'd·∫ßn_d√†',\n",
       " 'b·∫•t_k√¨',\n",
       " 'r√µ_l√†',\n",
       " 'bu·ªïi',\n",
       " 'ra_sao',\n",
       " 'c√≤n_nh∆∞',\n",
       " 'ba_ba',\n",
       " 'ng√†y_c√†ng',\n",
       " '·ªü_tr√™n',\n",
       " 't·∫°o_√Ω',\n",
       " 'ch·∫Øc_ƒÉn',\n",
       " 'bi·∫øt_ƒë∆∞·ª£c',\n",
       " 'ph·∫ßn_sau',\n",
       " '√≠t_th√¥i',\n",
       " 'hay',\n",
       " 'b·ªã_ch√∫',\n",
       " 't·ªët',\n",
       " 'h·ªèi_xem',\n",
       " 'sau_ch√≥t',\n",
       " 'nh·ªè',\n",
       " 'veo_veo',\n",
       " 'ng√¥i_nh√†',\n",
       " 'nhau',\n",
       " 'th·∫≠t_ch·∫Øc',\n",
       " 'bi·∫øt_ƒë√¢u_ch·ª´ng',\n",
       " 'ph√≠a_b√™n',\n",
       " 'sang_tay',\n",
       " 'tr∆∞·ªõc_h·∫øt',\n",
       " 'tr∆∞·ªõc_tu·ªïi',\n",
       " 'v√†i',\n",
       " 'nhi√™n_h·∫≠u',\n",
       " 'v·ª´a_m·ªõi',\n",
       " 'r√µ',\n",
       " 'v√¥_lu·∫≠n',\n",
       " 'th·∫≠t_ra',\n",
       " 'c√≥_ƒë∆∞·ª£c',\n",
       " 'l√™n_ng√¥i',\n",
       " 'b·∫•t_t·ª≠',\n",
       " 'ph·∫ßn',\n",
       " 'd·∫°_d√†i',\n",
       " 'th·ª±c_ra',\n",
       " 'm·∫°nh',\n",
       " 'th·∫ø_l·∫°i',\n",
       " 'qu√°_∆∞',\n",
       " 'th√≠m',\n",
       " 'l·∫•y_th√™m',\n",
       " 't·ª´ng_c√°i',\n",
       " 'tha_h·ªì_ch∆°i',\n",
       " 'bi·∫øt_tr∆∞·ªõc',\n",
       " 'th√†nh_th·ª≠',\n",
       " 'h∆°n_h·∫øt',\n",
       " 'ch·∫ßm_ch·∫≠p',\n",
       " 'l∆∞·ª£ng_c·∫£',\n",
       " 'b·ªóng_d∆∞ng',\n",
       " 'th·∫ø_ra',\n",
       " 'ch·ªõ_kh√¥ng',\n",
       " 'cao_th·∫•p',\n",
       " 'l√†m',\n",
       " 'n√≥',\n",
       " 'ng·ªìi_kh√¥ng',\n",
       " 't·ª´_t·ª´',\n",
       " 'ƒÉn_cu·ªôc',\n",
       " 't·∫°o_c∆°_h·ªôi',\n",
       " 'nh√©',\n",
       " 'tr·ª±c_ti·∫øp_l√†m',\n",
       " 'l·∫°i_n·ªØa',\n",
       " 'nh·∫≠n_ra',\n",
       " 'cho_v·ªÅ',\n",
       " 'v√πng_l√™n',\n",
       " 'ƒë√°ng_s·ªë',\n",
       " 'r·∫±ng',\n",
       " 'k·ªÉ',\n",
       " 'ƒë·ªëi_v·ªõi',\n",
       " 'nh√≥n_nh√©n',\n",
       " 'tha_h·ªì_ƒÉn',\n",
       " 'ho·∫∑c_l√†',\n",
       " 't·ª´ng_nh√†',\n",
       " 'l√†m_b·∫±ng',\n",
       " '√≠t_khi',\n",
       " 'th√†',\n",
       " 'ch·ª©_nh∆∞',\n",
       " 'b·∫£n_th√¢n',\n",
       " 'th·ªùi_gian_t√≠nh',\n",
       " 'ƒë·∫°t',\n",
       " '√¥ng_nh·ªè',\n",
       " 'b·ªè_vi·ªác',\n",
       " 's·ªë_lo·∫°i',\n",
       " 'so_v·ªõi',\n",
       " 'ng√¥i',\n",
       " 'th√≠ch_thu·ªôc',\n",
       " 'ƒëi_tr∆∞·ªõc',\n",
       " 'b·∫£n_√Ω',\n",
       " 'nh·ªõ_l·∫°i',\n",
       " 'c√≥_th√°ng',\n",
       " 'ph·∫£i_ng∆∞·ªùi',\n",
       " 'r·ªìi',\n",
       " 'h·∫øt_c·ªßa',\n",
       " 's·ªë_cho_bi·∫øt',\n",
       " 'sang_s√°ng',\n",
       " 'bi·∫øt_ƒë√¢u',\n",
       " 'cao_r√°o',\n",
       " 'h·ªó_tr·ª£',\n",
       " 'ch∆∞a_t√≠nh',\n",
       " 'c·∫≠u',\n",
       " 'l·∫•y_r√°o',\n",
       " 'chuy·ªÉn_ƒë·∫°t',\n",
       " 'tr·ª±c_ti·∫øp',\n",
       " 'nh∆∞_tr∆∞·ªõc',\n",
       " 'h·ªç',\n",
       " 'ch·∫øt_ti·ªát',\n",
       " 'ƒë·∫øn_th·∫ø',\n",
       " 'th√¨nh_l√¨nh',\n",
       " 'ƒÉn_h·ªèi',\n",
       " 'kh√≥_nghƒ©',\n",
       " 'n√≥i_xa',\n",
       " 'gi·∫£m_th·∫•p',\n",
       " 'tuy_th·∫ø',\n",
       " 'b·ªè_m√¨nh',\n",
       " 'sau_h·∫øt',\n",
       " 'ƒë·∫ßy_nƒÉm',\n",
       " 'bu·ªïi_ng√†y',\n",
       " 'b·ªüi_ch∆∞ng',\n",
       " '·∫•y_l√†',\n",
       " 't·∫Øp',\n",
       " 'd√†i_l·ªùi',\n",
       " 'nh√†_vi·ªác',\n",
       " 'ra_ƒë√¢y',\n",
       " 'b·∫±ng_n√†o',\n",
       " 'n√†y_n·ªç',\n",
       " 'qu√°_th√¨',\n",
       " 'ph√π_h·ª£p',\n",
       " 'ngay_khi',\n",
       " 'cho_ƒëang',\n",
       " 'xin',\n",
       " 'm·ªçi_n∆°i',\n",
       " 'c√≥_ƒÉn',\n",
       " 'nh∆∞_th∆∞·ªùng',\n",
       " 'ph·∫£i_chi',\n",
       " 'b·∫±ng',\n",
       " 't·ª´_·∫•y',\n",
       " 't·ªët_m·ªëi',\n",
       " 'kh√¥ng_bi·∫øt',\n",
       " 'ph·∫£i_tay',\n",
       " 'v√†o_g·∫∑p',\n",
       " 'mu·ªën',\n",
       " 'nh·ªù_nh·ªù',\n",
       " 'phƒÉn_ph·∫Øt',\n",
       " 'ch√∫_d·∫´n',\n",
       " 'th∆∞·ªùng_xu·∫•t_hi·ªán',\n",
       " 'nh·∫•t_lo·∫°t',\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    min_df = 5,\n",
    "    max_df = 0.8,\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(1, 4),\n",
    "#    stop_words = all_words,\n",
    "    max_features=30000)\n",
    "word_vectorizer.fit(clean_corpus)\n",
    "features = np.array(word_vectorizer.get_feature_names())\n",
    "train_word_features = word_vectorizer.transform(clean_corpus.iloc[:train_df.shape[0]])\n",
    "test_word_features = word_vectorizer.transform(clean_corpus.iloc[train_df.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25725,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skew in numerical features: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count_words_upper</th>\n",
       "      <td>34.086162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_bad_emoji</th>\n",
       "      <td>32.608676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_good_emoji</th>\n",
       "      <td>20.515688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_punctuations</th>\n",
       "      <td>19.358625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_sent</th>\n",
       "      <td>8.956432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_letters</th>\n",
       "      <td>6.018465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_word</th>\n",
       "      <td>5.932763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_unique_word</th>\n",
       "      <td>3.806975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>0.311152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words_vs_unique</th>\n",
       "      <td>-2.544749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Skew\n",
       "count_words_upper   34.086162\n",
       "n_bad_emoji         32.608676\n",
       "n_good_emoji        20.515688\n",
       "count_punctuations  19.358625\n",
       "count_sent           8.956432\n",
       "count_letters        6.018465\n",
       "count_word           5.932763\n",
       "count_unique_word    3.806975\n",
       "label                0.311152\n",
       "words_vs_unique     -2.544749"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 skewed numerical features to Box Cox transform\n"
     ]
    }
   ],
   "source": [
    "skewness = skewness.loc[abs(skewness.Skew) > 0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    if 'label' not in feat:\n",
    "        df[feat] = boxcox1p(df[feat], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:train_df.shape[0]]\n",
    "test_df = df.iloc[train_df.shape[0]:]\n",
    "\n",
    "y_train = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUED_COLS = ['id', 'comment', 'label']\n",
    "static_cols = [c for c in train_df.columns if not c in EXCLUED_COLS]\n",
    "X_train_static = train_df[static_cols].values\n",
    "X_test_static = test_df[static_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack([train_word_features, csr_matrix(X_train_static)]).tocsr()\n",
    "X_test = hstack([test_word_features, csr_matrix(X_test_static)]).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "\n",
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location):\n",
    "    \"\"\"Returns trained word2vec\n",
    "\n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found w2vmodel\n"
     ]
    }
   ],
   "source": [
    "w2vec = get_word2vec(\n",
    "    MySentences(\n",
    "        train_df['comment'].values, \n",
    "    ),\n",
    "    'w2vmodel'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\n",
    "trainDataVecs = mean_embedding_vectorizer.fit_transform(train_df['comment'])\n",
    "testDataVecs = mean_embedding_vectorizer.fit_transform(test_df['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest to training data....\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "    \n",
    "print(\"Fitting random forest to training data....\")    \n",
    "forest = forest.fit(trainDataVecs, train_df[\"label\"])\n",
    "\n",
    "result = forest.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack([trainDataVecs, csr_matrix(X_train_static)]).toarray()\n",
    "X_test = hstack([testDataVecs, csr_matrix(X_test_static)]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StackNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class GNB(self,):\n",
    "\n",
    "def pr(y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "x = X_train\n",
    "test_x = X_test\n",
    "\n",
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=1.3, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r\n",
    "\n",
    "preds = np.zeros((len(test_df), 1))\n",
    "\n",
    "label_cols = ['label']\n",
    "m,r = get_mdl(train_df['label'])\n",
    "preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb1 = lgb.LGBMClassifier(max_depth=16, n_estimators=400,learning_rate=0.05,colsample_bytree=0.3,num_leaves=250,\n",
    "                          metric='auc',objective='binary', n_jobs=-1)\n",
    "lgb2 = lgb.LGBMClassifier(max_depth=17, n_estimators=400,learning_rate=0.05,colsample_bytree=0.3,num_leaves=120,\n",
    "                          metric='f1',objective='binary', n_jobs=-1)\n",
    "xgb1 = XGBClassifier(learning_rate =0.05,n_estimators=400,max_depth = 16,min_child_weight=3,gamma=0.5,subsample=0.8,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=13,\n",
    "                     metric='f1')\n",
    "xgb2 = XGBClassifier(learning_rate =0.04,n_estimators=400,max_depth = 15,min_child_weight=3,gamma=0.7,subsample=1,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=13,\n",
    "                     metric='f1')\n",
    "lr1 = LogisticRegression(C = 1.2, random_state=1)\n",
    "lr2 = LogisticRegression(C = 1.3, random_state=2019)\n",
    "rf1 = RandomForestClassifier (n_estimators=400, criterion=\"entropy\", max_depth=12, max_features=0.5, random_state=3)\n",
    "rf1_1 = RandomForestClassifier (n_estimators=500, criterion=\"entropy\", max_depth=11, max_features=0.5, random_state=2019)\n",
    "rf1_2 = RandomForestClassifier (n_estimators=500, criterion=\"entropy\", max_depth=11, max_features=0.5, random_state=2)\n",
    "rf2 = RandomForestClassifier(n_estimators=400, criterion=\"entropy\", max_depth=8, max_features=0.5, random_state=1)\n",
    "rf3 = RandomForestClassifier(n_estimators=500, criterion=\"entropy\", max_depth=7, max_features=0.5, random_state=2)\n",
    "et1 = ExtraTreesClassifier(n_estimators=400, criterion=\"entropy\", max_depth=10, max_features=0.5, random_state=2, n_jobs=-1)\n",
    "et2 = ExtraTreesClassifier(n_estimators=400, criterion=\"entropy\", max_depth=7, max_features=0.5, random_state=2019, n_jobs=-1)\n",
    "gbc1 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=7, max_features=0.5, random_state=1)\n",
    "gbc2 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=8, max_features=0.5, random_state=2019)\n",
    "dt1 = DecisionTreeClassifier(criterion='gini', max_depth=17, min_samples_split=2, max_features=0.5, random_state=1)\n",
    "dt2 = DecisionTreeClassifier(criterion='gini', max_depth=13, min_samples_split=2, max_features=0.5, random_state=5)\n",
    "cat1 = CatBoostClassifier(learning_rate=0.1, iterations=400, random_seed=1, logging_level='Silent')\n",
    "knn1 = KNeighborsClassifier(n_neighbors=5, leaf_size=30, p=2, n_jobs=-1) \n",
    "knn2 = KNeighborsClassifier(n_neighbors=4, leaf_size=30, p=2, n_jobs=-1) \n",
    "svm1 = svm.SVC(C = 20.0, kernel='rbf', random_state = 1)\n",
    "ada1 = AdaBoostClassifier(base_estimator=None, n_estimators=400, learning_rate=0.05, algorithm='SAMME.R', random_state=1)\n",
    "ada2 = AdaBoostClassifier(base_estimator=None, n_estimators=400, learning_rate=0.05, algorithm='SAMME.R', random_state=3)\n",
    "\n",
    "#models=[[gbc1],[rf2]]\n",
    "models=[[lgb1, xgb1, lr1, rf1, dt1, gbc1, rf1_1, ada1],[lgb2, xgb2, rf2, lr2, cat1, et2, gbc2, ada2, knn1, knn2],[rf3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Start of Level 0 ======================\n",
      "Input Dimensionality 25734 at Level 0 \n",
      "8 models included in Level 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 1/5 , model 0 , f1===0.878412 \n",
      "Level 0, fold 1/5 , model 1 , f1===0.872845 \n",
      "Level 0, fold 1/5 , model 2 , f1===0.881392 \n",
      "Level 0, fold 1/5 , model 3 , f1===0.832224 \n",
      "Level 0, fold 1/5 , model 4 , f1===0.824924 \n",
      "Level 0, fold 1/5 , model 5 , f1===0.875178 \n",
      "Level 0, fold 1/5 , model 6 , f1===0.831505 \n",
      "Level 0, fold 1/5 , model 7 , f1===0.843049 \n",
      "=========== end of fold 1 in level 0 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 2/5 , model 0 , f1===0.883209 \n",
      "Level 0, fold 2/5 , model 1 , f1===0.877043 \n",
      "Level 0, fold 2/5 , model 2 , f1===0.887712 \n",
      "Level 0, fold 2/5 , model 3 , f1===0.842353 \n",
      "Level 0, fold 2/5 , model 4 , f1===0.831090 \n",
      "Level 0, fold 2/5 , model 5 , f1===0.869971 \n",
      "Level 0, fold 2/5 , model 6 , f1===0.839906 \n",
      "Level 0, fold 2/5 , model 7 , f1===0.851375 \n",
      "=========== end of fold 2 in level 0 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 3/5 , model 0 , f1===0.890756 \n",
      "Level 0, fold 3/5 , model 1 , f1===0.885630 \n",
      "Level 0, fold 3/5 , model 2 , f1===0.892622 \n",
      "Level 0, fold 3/5 , model 3 , f1===0.837958 \n",
      "Level 0, fold 3/5 , model 4 , f1===0.819020 \n",
      "Level 0, fold 3/5 , model 5 , f1===0.878877 \n",
      "Level 0, fold 3/5 , model 6 , f1===0.832196 \n",
      "Level 0, fold 3/5 , model 7 , f1===0.854218 \n",
      "=========== end of fold 3 in level 0 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 4/5 , model 0 , f1===0.892450 \n",
      "Level 0, fold 4/5 , model 1 , f1===0.885877 \n",
      "Level 0, fold 4/5 , model 2 , f1===0.889365 \n",
      "Level 0, fold 4/5 , model 3 , f1===0.841022 \n",
      "Level 0, fold 4/5 , model 4 , f1===0.840819 \n",
      "Level 0, fold 4/5 , model 5 , f1===0.890792 \n",
      "Level 0, fold 4/5 , model 6 , f1===0.838431 \n",
      "Level 0, fold 4/5 , model 7 , f1===0.859208 \n",
      "=========== end of fold 4 in level 0 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 5/5 , model 0 , f1===0.874106 \n",
      "Level 0, fold 5/5 , model 1 , f1===0.874596 \n",
      "Level 0, fold 5/5 , model 2 , f1===0.885643 \n",
      "Level 0, fold 5/5 , model 3 , f1===0.830811 \n",
      "Level 0, fold 5/5 , model 4 , f1===0.820758 \n",
      "Level 0, fold 5/5 , model 5 , f1===0.877419 \n",
      "Level 0, fold 5/5 , model 6 , f1===0.828859 \n",
      "Level 0, fold 5/5 , model 7 , f1===0.841664 \n",
      "=========== end of fold 5 in level 0 ===========\n",
      "Level 0, model 0 , f1===0.883787 \n",
      "Level 0, model 1 , f1===0.879198 \n",
      "Level 0, model 2 , f1===0.887347 \n",
      "Level 0, model 3 , f1===0.836874 \n",
      "Level 0, model 4 , f1===0.827322 \n",
      "Level 0, model 5 , f1===0.878448 \n",
      "Level 0, model 6 , f1===0.834179 \n",
      "Level 0, model 7 , f1===0.849903 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimensionality of level 0 is 8 \n",
      "====================== End of Level 0 ======================\n",
      " level 0 lasted 5454.859944 seconds \n",
      "====================== Start of Level 1 ======================\n",
      "Input Dimensionality 8 at Level 1 \n",
      "10 models included in Level 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1, fold 1/5 , model 0 , f1===0.867002 \n",
      "Level 1, fold 1/5 , model 1 , f1===0.884724 \n",
      "Level 1, fold 1/5 , model 2 , f1===0.890372 \n",
      "Level 1, fold 1/5 , model 3 , f1===0.881952 \n",
      "Level 1, fold 1/5 , model 4 , f1===0.890060 \n",
      "Level 1, fold 1/5 , model 5 , f1===0.889045 \n",
      "Level 1, fold 1/5 , model 6 , f1===0.885281 \n",
      "Level 1, fold 1/5 , model 7 , f1===0.889667 \n",
      "Level 1, fold 1/5 , model 8 , f1===0.872401 \n",
      "Level 1, fold 1/5 , model 9 , f1===0.875433 \n",
      "=========== end of fold 1 in level 1 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1, fold 2/5 , model 0 , f1===0.867573 \n",
      "Level 1, fold 2/5 , model 1 , f1===0.886605 \n",
      "Level 1, fold 2/5 , model 2 , f1===0.889516 \n",
      "Level 1, fold 2/5 , model 3 , f1===0.888889 \n",
      "Level 1, fold 2/5 , model 4 , f1===0.888653 \n",
      "Level 1, fold 2/5 , model 5 , f1===0.890454 \n",
      "Level 1, fold 2/5 , model 6 , f1===0.886766 \n",
      "Level 1, fold 2/5 , model 7 , f1===0.890915 \n",
      "Level 1, fold 2/5 , model 8 , f1===0.880799 \n",
      "Level 1, fold 2/5 , model 9 , f1===0.883495 \n",
      "=========== end of fold 2 in level 1 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1, fold 3/5 , model 0 , f1===0.881878 \n",
      "Level 1, fold 3/5 , model 1 , f1===0.890830 \n",
      "Level 1, fold 3/5 , model 2 , f1===0.896727 \n",
      "Level 1, fold 3/5 , model 3 , f1===0.893254 \n",
      "Level 1, fold 3/5 , model 4 , f1===0.894718 \n",
      "Level 1, fold 3/5 , model 5 , f1===0.897781 \n",
      "Level 1, fold 3/5 , model 6 , f1===0.888564 \n",
      "Level 1, fold 3/5 , model 7 , f1===0.897075 \n",
      "Level 1, fold 3/5 , model 8 , f1===0.881965 \n",
      "Level 1, fold 3/5 , model 9 , f1===0.881766 \n",
      "=========== end of fold 3 in level 1 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1, fold 4/5 , model 0 , f1===0.885000 \n",
      "Level 1, fold 4/5 , model 1 , f1===0.897236 \n",
      "Level 1, fold 4/5 , model 2 , f1===0.899258 \n",
      "Level 1, fold 4/5 , model 3 , f1===0.893754 \n",
      "Level 1, fold 4/5 , model 4 , f1===0.896527 \n",
      "Level 1, fold 4/5 , model 5 , f1===0.900882 \n",
      "Level 1, fold 4/5 , model 6 , f1===0.894906 \n",
      "Level 1, fold 4/5 , model 7 , f1===0.899192 \n",
      "Level 1, fold 4/5 , model 8 , f1===0.887777 \n",
      "Level 1, fold 4/5 , model 9 , f1===0.882496 \n",
      "=========== end of fold 4 in level 1 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1, fold 5/5 , model 0 , f1===0.879828 \n",
      "Level 1, fold 5/5 , model 1 , f1===0.887234 \n",
      "Level 1, fold 5/5 , model 2 , f1===0.891173 \n",
      "Level 1, fold 5/5 , model 3 , f1===0.888809 \n",
      "Level 1, fold 5/5 , model 4 , f1===0.890071 \n",
      "Level 1, fold 5/5 , model 5 , f1===0.890619 \n",
      "Level 1, fold 5/5 , model 6 , f1===0.883970 \n",
      "Level 1, fold 5/5 , model 7 , f1===0.889203 \n",
      "Level 1, fold 5/5 , model 8 , f1===0.864535 \n",
      "Level 1, fold 5/5 , model 9 , f1===0.868835 \n",
      "=========== end of fold 5 in level 1 ===========\n",
      "Level 1, model 0 , f1===0.876256 \n",
      "Level 1, model 1 , f1===0.889326 \n",
      "Level 1, model 2 , f1===0.893409 \n",
      "Level 1, model 3 , f1===0.889332 \n",
      "Level 1, model 4 , f1===0.892006 \n",
      "Level 1, model 5 , f1===0.893756 \n",
      "Level 1, model 6 , f1===0.887897 \n",
      "Level 1, model 7 , f1===0.893210 \n",
      "Level 1, model 8 , f1===0.877495 \n",
      "Level 1, model 9 , f1===0.878405 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimensionality of level 1 is 10 \n",
      "====================== End of Level 1 ======================\n",
      " level 1 lasted 372.217851 seconds \n",
      "====================== Start of Level 2 ======================\n",
      "Input Dimensionality 10 at Level 2 \n",
      "1 models included in Level 2 \n",
      "Level 2, fold 1/5 , model 0 , f1===0.887091 \n",
      "=========== end of fold 1 in level 2 ===========\n",
      "Level 2, fold 2/5 , model 0 , f1===0.889201 \n",
      "=========== end of fold 2 in level 2 ===========\n",
      "Level 2, fold 3/5 , model 0 , f1===0.898034 \n",
      "=========== end of fold 3 in level 2 ===========\n",
      "Level 2, fold 4/5 , model 0 , f1===0.896283 \n",
      "=========== end of fold 4 in level 2 ===========\n",
      "Level 2, fold 5/5 , model 0 , f1===0.891720 \n",
      "=========== end of fold 5 in level 2 ===========\n",
      "Level 2, model 0 , f1===0.892466 \n",
      "Output dimensionality of level 2 is 1 \n",
      "====================== End of Level 2 ======================\n",
      " level 2 lasted 123.807996 seconds \n",
      "====================== End of fit ======================\n",
      " fit() lasted 5950.945961 seconds \n",
      "====================== Start of Level 0 ======================\n",
      "1 estimators included in Level 0 \n",
      "====================== Start of Level 1 ======================\n",
      "1 estimators included in Level 1 \n",
      "====================== Start of Level 2 ======================\n",
      "1 estimators included in Level 2 \n"
     ]
    }
   ],
   "source": [
    "from pystacknet.pystacknet import StackNetClassifier\n",
    "\n",
    "model = StackNetClassifier(\n",
    "    models, metric=\"f1\", \n",
    "    folds=5,\n",
    "    restacking=False, \n",
    "    use_retraining=True, \n",
    "    use_proba=True, \n",
    "    random_state=12345, n_jobs=1, verbose=1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds=model.predict_proba(X_test) ## 893951 893741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cls = np.argmax(preds, axis=1)\n",
    "pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission['label'] = pred_cls\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb1 = lgb.LGBMClassifier(max_depth=16, n_estimators=400,learning_rate=0.05,colsample_bytree=0.3,num_leaves=120,\n",
    "                          metric='f1',objective='binary', n_jobs=-1)\n",
    "lgb2 = lgb.LGBMClassifier(max_depth=17, n_estimators=400,learning_rate=0.04,colsample_bytree=0.3,num_leaves=200,\n",
    "                          metric='f1',objective='binary', n_jobs=-1)\n",
    "xgb1 = XGBClassifier(learning_rate =0.05,n_estimators=400,max_depth = 10,min_child_weight=3,gamma=0,subsample=0.8,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=-1,scale_pos_weight=1,seed=13,\n",
    "                     metric='f1')\n",
    "lr1 = LogisticRegression(C = 1.2, random_state=1)\n",
    "lr2 = LogisticRegression(C = 1.3, random_state=2019)\n",
    "rf1 = RandomForestClassifier (n_estimators=400, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=3)\n",
    "rf2 = RandomForestClassifier(n_estimators=400, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1)\n",
    "et1 = ExtraTreesClassifier(n_estimators=400, criterion=\"entropy\", max_depth=9, max_features=0.5, random_state=2, n_jobs=-1)\n",
    "gbc1 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=13, max_features=0.5, random_state=1)\n",
    "gbc2 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=12, max_features=0.5, random_state=2019)\n",
    "dt1 = DecisionTreeClassifier(criterion='gini', max_depth=13, min_samples_split=2, max_features=0.5, random_state=1)\n",
    "cat1 = CatBoostClassifier(learning_rate=0.1, iterations=400, random_seed=1, logging_level='Silent', random_state=1)\n",
    "knn1 = KNeighborsClassifier(n_neighbors=5, leaf_size=100, p=2, n_jobs=-1) \n",
    "svm1 = svm.SVC(C = 20.0, kernel='rbf', random_state = 1, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_models = [dt1, gbc1, rf1, lgb1, xgb1, lr1, et1, lgb2, lr2, gbc2]\n",
    "layer1_names = ['dt1', 'gbc1', 'rf1', 'lgb1', 'xgb1', 'lr1', 'et1', 'lgb2', 'lr2', 'gbc2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_models = [et1]\n",
    "layer1_names = ['et1']\n",
    "\n",
    "# X_train = hstack([trainDataVecs, train_word_features, csr_matrix(X_train_static)]).tocsr()\n",
    "# X_test = hstack([testDataVecs, test_word_features, csr_matrix(X_test_static)]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_train = np.zeros(shape=(len(train_df),len(layer1_models)))\n",
    "oof_test = np.zeros(shape=(len(test_df),len(layer1_models)))\n",
    "\n",
    "# Recording results\n",
    "layer1_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training et1\n",
      "Fold no 1\n",
      "Fold no 2\n",
      "Fold no 3\n",
      "Fold no 4\n",
      "Fold no 5\n",
      "Training CV score: 0.77526\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(layer1_models)):\n",
    "    print('\\n')\n",
    "    name = layer1_names[i]\n",
    "    model = layer1_models[i]\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "    print('Training %s' %name)\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "        print('Fold no %i'%(fold_+1))\n",
    "        trn_data = X_train[trn_idx]\n",
    "        trn_label = y_train[trn_idx]\n",
    "        val_data = X_train[val_idx]\n",
    "        val_label = y_train[val_idx]\n",
    "        if ('lgb' in name) or ('xgb' in name):\n",
    "            model.fit(X=trn_data, y=trn_label,\n",
    "                     eval_set=[(trn_data, trn_label), (val_data, val_label)],\n",
    "                     verbose=200)\n",
    "        else:\n",
    "            model.fit(X=trn_data, y=trn_label)\n",
    "        \n",
    "        oof_train[val_idx,i] = model.predict(val_data)\n",
    "        oof_test[:,i] += model.predict(X_test)/5\n",
    "        \n",
    "    score = f1_score(oof_train[:,i], y_train)\n",
    "    layer1_score.append(score)\n",
    "    print('Training CV score: %.5f' %score) #7603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2_models = [gbc1]\n",
    "layer2_names = ['gbc1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to record result\n",
    "train_pred = np.zeros(shape=(len(train_df),2))\n",
    "test_pred = np.zeros(shape=(len(test_df),2))\n",
    "\n",
    "layer2_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training gbc1\n",
      "Training score: 0.89246\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(layer2_models)):\n",
    "    print('\\n')\n",
    "    name = layer2_names[i]\n",
    "    model = layer2_models[i]\n",
    "    print('Training %s' %name)\n",
    "    model.fit(oof_train, y_train)\n",
    "    score = f1_score(model.predict_proba(oof_train).argmax(axis=1), y_train)\n",
    "    train_pred += model.predict_proba(oof_train)/len(layer2_models)\n",
    "    test_pred += model.predict_proba(oof_test)/len(layer2_models)\n",
    "    layer2_score.append(score)\n",
    "    print('Training score: %.5f' % score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission['label'] = test_pred\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "models = [\n",
    "    RandomForestClassifier (n_estimators=300, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n",
    "    ExtraTreesClassifier (n_estimators=300, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n",
    "    GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=5, max_features=0.5, random_state=1),\n",
    "    LogisticRegression(random_state=1)\n",
    "]\n",
    "\n",
    "def cross_val_and_predict(clf, X, y, X_test, nfolds):\n",
    "    kf = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=42)\n",
    "    \n",
    "    oof_preds = np.zeros((X.shape[0], 2))\n",
    "    sub_preds = np.zeros((X_test.shape[0], 2))\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        oof_preds[valid_idx] = clf.predict_proba(X_valid)\n",
    "        sub_preds += clf.predict_proba(X_test) / kf.n_splits\n",
    "        \n",
    "    return oof_preds, sub_preds\n",
    "\n",
    "sub_preds = []\n",
    "\n",
    "for clf in models:\n",
    "    oof_pred, sub_pred = cross_val_and_predict(clf, X_train, y_train, X_test, nfolds=5)\n",
    "    oof_pred_cls = oof_pred.argmax(axis=1)\n",
    "    oof_f1 = f1_score(y_pred=oof_pred_cls, y_true=y_train)\n",
    "    \n",
    "    print(clf.__class__)\n",
    "    print(f\"F1 CV: {oof_f1}\")\n",
    "    \n",
    "    sub_preds.append(sub_pred)\n",
    "\n",
    "sub_preds = np.asarray(sub_preds)\n",
    "sub_preds = sub_preds.mean(axis=0)\n",
    "sub_pred_cls = sub_preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.where((np.round(preds.ravel()) + sub_pred_cls + pred_cls) >= 2, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.65, 'learning_rate': 0.05, 'max_depth': 10, 'metric': 'f1', 'n_estimators': 600, 'num_leaves': 80, 'objective': 'binary', 'random_state': 1, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# Create parameters to search\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'binary',\n",
    "          'nthread': 3, # Updated from nthread\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5,\n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'binary_error'}\n",
    "\n",
    "gridParams = {\n",
    "    'max_depth':[10, 15, 20, 25],\n",
    "    'learning_rate': [0.02, 0.04, 0.05, 0.06],\n",
    "    'n_estimators': [500, 600],\n",
    "    'num_leaves': [80],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'random_state' : [1], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65],\n",
    "    'subsample' : [0.75],\n",
    "    'reg_alpha' : [1],\n",
    "    'reg_lambda' : [1],\n",
    "    'metric':['f1']\n",
    "    }\n",
    "\n",
    "# Create classifier to use. Note that parameters have to be input manually\n",
    "# not as a dict!\n",
    "mdl = lgb.LGBMClassifier(boosting_type= 'gbdt',\n",
    "          objective = 'binary',\n",
    "          n_jobs = -1, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'],\n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'],\n",
    "          subsample_freq = params['subsample_freq'],\n",
    "          min_split_gain = params['min_split_gain'],\n",
    "          min_child_weight = params['min_child_weight'],\n",
    "          min_child_samples = params['min_child_samples'],\n",
    "          scale_pos_weight = params['scale_pos_weight'])\n",
    "\n",
    "# To view the default model params:\n",
    "mdl.get_params().keys()\n",
    "\n",
    "# Create the grid\n",
    "grid = GridSearchCV(mdl, gridParams,\n",
    "                    verbose=0,\n",
    "                    cv=4,\n",
    "                    n_jobs=-1)\n",
    "# Run the grid\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 38.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best hyperparameters:\n",
      "{'max_depth': 17}\n"
     ]
    }
   ],
   "source": [
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        'max_depth': [16, 18, 20, 22]\n",
    "        }\n",
    "\n",
    "xgb1 = XGBClassifier(learning_rate =0.05,n_estimators=300,max_depth = 16,min_child_weight=3,gamma=0.5,subsample=0.8,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=1,scale_pos_weight=1,seed=13,\n",
    "                     metric='f1')\n",
    "\n",
    "folds = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "grid = GridSearchCV(estimator=xgb1, param_grid=params, scoring='f1', n_jobs=5, cv=skf.split(X_train,y_train), verbose=1 )\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('\\n Best hyperparameters:')\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
