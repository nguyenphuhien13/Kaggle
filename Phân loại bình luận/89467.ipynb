{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix, vstack\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.linear_model import *\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost.sklearn  import XGBClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3dda4c980d54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1440\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1080\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloud\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "cloud = WordCloud(width=1440, height=1080).generate(\" \".join(df.comment.astype(str)))\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def extract_emojis(str):\n",
    "    return [c for c in str if c in emoji.UNICODE_EMOJI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_df = train_df[train_df['label'] == 0]\n",
    "good_comment = good_df['comment'].values\n",
    "good_emoji = []\n",
    "for c in good_comment:\n",
    "    good_emoji += extract_emojis(c)\n",
    "\n",
    "good_emoji = np.unique(np.asarray(good_emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_df = train_df[train_df['label'] == 1]\n",
    "bad_comment = bad_df['comment'].values\n",
    "\n",
    "bad_emoji = []\n",
    "for c in bad_comment:\n",
    "    bad_emoji += extract_emojis(c)\n",
    "\n",
    "bad_emoji = np.unique(np.asarray(bad_emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['‚Üñ', '‚Üó', '‚òÄ', '‚ò∫', '‚ôÄ', '‚ô•', '‚úå', '‚ú®', '‚ùå', '‚ù£', '‚ù§', '‚≠ê', 'üÜó',\n",
       "       'üåù', 'üåü', 'üåß', 'üå∑', 'üå∏', 'üå∫', 'üåº', 'üçì', 'üéà', 'üéâ', 'üêÖ', 'üêæ', 'üëâ',\n",
       "       'üëå', 'üëç', 'üëè', 'üíã', 'üíå', 'üíê', 'üíì', 'üíï', 'üíñ', 'üíó', 'üíô', 'üíö', 'üíõ',\n",
       "       'üíú', 'üíû', 'üíü', 'üí•', 'üí™', 'üíÆ', 'üíØ', 'üí∞', 'üìë', 'üñ§', 'üòÄ', 'üòÅ', 'üòÇ',\n",
       "       'üòÉ', 'üòÑ', 'üòÖ', 'üòÜ', 'üòá', 'üòâ', 'üòä', 'üòã', 'üòå', 'üòç', 'üòé', 'üòë', 'üòì',\n",
       "       'üòî', 'üòñ', 'üòó', 'üòò', 'üòô', 'üòö', 'üòõ', 'üòú', 'üòù', 'üòû', 'üòü', 'üò°', 'üò¢',\n",
       "       'üò£', 'üò•', 'üò©', 'üò™', 'üò´', 'üò¨', 'üò≠', 'üòØ', 'üò∞', 'üò±', 'üò≤', 'üò≥', 'üòª',\n",
       "       'üòø', 'üôÅ', 'üôÇ', 'üôÉ', 'üôÑ', 'üôÜ', 'üôå', 'ü§ë', 'ü§î', 'ü§ó', 'ü§ô', 'ü§ù', 'ü§£',\n",
       "       'ü§§', 'ü§®', 'ü§™', 'ü§≠'], dtype='<U2')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_emoji_fix = [\n",
    "    '‚Üñ', '‚Üó', '‚òÄ', '‚ò∫', '‚ôÄ', '‚ô•', '‚úå', '‚ú®', '‚ù£', '‚ù§', '‚≠ê', 'üÜó', '^^',\n",
    "       'üåù', 'üåü', 'üåß', 'üå∑', 'üå∏', 'üå∫', 'üåº', 'üçì', 'üéà', 'üéâ', 'üêÖ', 'üêæ', 'üëâ',\n",
    "       'üëå', 'üëç', 'üëè', 'üíã', 'üíå', 'üíê', 'üíì', 'üíï', 'üíñ', 'üíó', 'üíô', 'üíö', 'üíõ',\n",
    "       'üíú', 'üíû', 'üíü', 'üí•', 'üí™', 'üíÆ', 'üíØ', 'üí∞', 'üìë', 'üñ§', 'üòÄ', 'üòÅ', 'üòÇ',\n",
    "       'üòÉ', 'üòÑ', 'üòÖ', 'üòÜ', 'üòá', 'üòâ', 'üòä', 'üòã', 'üòå', 'üòç', 'üòé', 'üòë', 'üòì', 'üòî', \n",
    "    'üòñ', 'üòó', 'üòò', 'üòô', 'üòö', 'üòõ', 'üòú', 'üòù', 'üòû', 'üòü', 'üòØ', 'üò∞', 'üò±', 'üò≤', 'üò≥', 'üòª', 'üôÇ', 'üôÉ', 'üôÑ', 'üôÜ', 'üôå', 'ü§ë', 'ü§î', 'ü§ó',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['‚òπ', '‚úã', '‚ùå', '‚ùì', '‚ù§', '‚≠ê', 'üéÉ', 'üëå', 'üëç', 'üëé', 'üë∂', 'üíÄ', 'üíã',\n",
       "       'üòÅ', 'üòÇ', 'üòà', 'üòä', 'üòå', 'üòè', 'üòê', 'üòë', 'üòí', 'üòì', 'üòî', 'üòñ', 'üòö',\n",
       "       'üòû', 'üòü', 'üò†', 'üò°', 'üò¢', 'üò£', 'üò§', 'üò•', 'üòß', 'üò©', 'üò™', 'üò´', 'üò¨',\n",
       "       'üò≠', 'üò≥', 'üòµ', 'üò∂', 'üôÅ', 'üôÇ', 'üôÑ', 'ü§î', 'ü§ö', 'ü§§'], dtype='<U2')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just remove \"good\" emoji :D\n",
    "bad_emoji_fix = [\n",
    "    '‚òπ', '‚úã', '‚ùå', '‚ùì', 'üëé', 'üë∂', 'üíÄ',\n",
    "       'üòê', 'üòë', 'üòí', 'üòì', 'üòî', ':(', 't^t', 't ^ t',\n",
    "       'üòû', 'üòü', 'üò†', 'üò°', 'üò¢', 'üò£', 'üò§', 'üò•', 'üòß', 'üò©', 'üò™', 'üò´', 'üò¨',\n",
    "       'üò≠', 'üò≥', 'üòµ', 'üò∂', 'üôÅ', 'üôÑ', 'ü§î',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_good_bad_emoji(row):\n",
    "    comment = row['comment']\n",
    "    n_good_emoji = 0\n",
    "    n_bad_emoji = 0\n",
    "    for c in comment:\n",
    "        if c in good_emoji_fix:\n",
    "            n_good_emoji += 1\n",
    "        if c in bad_emoji_fix:\n",
    "            n_bad_emoji += 1\n",
    "    \n",
    "    row['n_good_emoji'] = n_good_emoji\n",
    "    row['n_bad_emoji'] = n_bad_emoji\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].astype(str).fillna(' ')\n",
    "df = df.apply(count_good_bad_emoji, axis=1)\n",
    "\n",
    "df['count_sent']=df[\"comment\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "#Word count in each comment:\n",
    "df['count_word']=df[\"comment\"].apply(lambda x: len(str(x).split()))\n",
    "#Unique word count\n",
    "df['count_unique_word']=df[\"comment\"].apply(lambda x: len(set(str(x).split())))\n",
    "#Letter count\n",
    "df['count_letters']=df[\"comment\"].apply(lambda x: len(str(x)))\n",
    "#punctuation count\n",
    "df[\"count_punctuations\"] =df[\"comment\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "#upper case words count\n",
    "df[\"count_words_upper\"] = df[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "df['words_vs_unique'] = df['count_unique_word'] / df['count_word'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_mapping = {\n",
    "    \"ship\": \"v·∫≠n chuy·ªÉn\",\n",
    "    \"shop\": \"c·ª≠a h√†ng\",\n",
    "    \"mik\": \" m√¨nh \",\n",
    "    \"ko\": \"kh√¥ng\",\n",
    "    \"tl\": \"tr·∫£ l·ªùi\",\n",
    "    \"fb\": \"m·∫°ng x√£ h·ªôi\", \n",
    "    \"face\": \"m·∫°ng x√£ h·ªôi\",\n",
    "    \"thanks\": \"c·∫£m ∆°n\",\n",
    "    \"thank\": \"c·∫£m ∆°n\",\n",
    "    \"tks\": \"c·∫£m ∆°n\", \n",
    "    \"tk\": \"c·∫£m ∆°n\",\n",
    "    \"sp\": \"s·∫£n ph·∫©m\",\n",
    "    \"dc\": \"ƒë∆∞·ª£c\",\n",
    "    \"ƒëc\": \"ƒë∆∞·ª£c\",\n",
    "    \"ƒëx\": \"ƒë∆∞·ª£c\",\n",
    "    \"nhjet\": \"nhi·ªát\",\n",
    "    \"inb\":\"inbox\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['comment']\n",
    "tokenizer=TweetTokenizer()\n",
    "lem = WordNetLemmatizer()\n",
    "def clean(comment):\n",
    "    \"\"\"\n",
    "    This function receives comments and returns clean word-list\n",
    "    \"\"\"\n",
    "    #Convert to lower case , so that Hi and hi are the same\n",
    "    comment=comment.lower()\n",
    "    #remove \\n\n",
    "    comment=re.sub(\"\\\\n\",\"\",comment)\n",
    "    # remove leaky elements like ip,user\n",
    "    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n",
    "    #removing usernames\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    \n",
    "    #Split the sentences into words\n",
    "    words=tokenizer.tokenize(comment)\n",
    "    \n",
    "    # (')aphostophe  replacement (ie)   you're --> you are  \n",
    "    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n",
    "    words=[correct_mapping[word] if word in correct_mapping else word for word in words]\n",
    "    words=[lem.lemmatize(word, \"v\") for word in words]\n",
    "    words=[re.sub(r'(^r$)|(^r`$)', r'r·ªìi', word) for word in words]\n",
    "    words=[re.sub(r'(^k$)|(^kh$)', r'kh√¥ng', word) for word in words]\n",
    "    words=[re.sub(r'(^m$)', r'm√¨nh', word) for word in words]\n",
    "\n",
    "    clean_sent = \" \".join(words)\n",
    "    # remove any non alphanum,digit character\n",
    "    #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n",
    "    #clean_sent=re.sub(\"  \",\" \",clean_sent)\n",
    "    return(clean_sent)\n",
    "\n",
    "clean_corpus=corpus.apply(lambda x :clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    min_df = 5,\n",
    "    sublinear_tf=True,\n",
    "    analyzer='char',\n",
    "    token_pattern=r'\\S',\n",
    "    ngram_range=(1, 6),\n",
    "    max_features=10000)\n",
    "char_vectorizer.fit(clean_corpus)\n",
    "features_char = np.array(char_vectorizer.get_feature_names())\n",
    "train_char_features = char_vectorizer.transform(clean_corpus.iloc[:train_df.shape[0]])\n",
    "test_char_features = char_vectorizer.transform(clean_corpus.iloc[train_df.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.words.txt', 'r', encoding = 'utf_8') as my_file:\n",
    "    all_words = {word.strip() for word in my_file.read().split('\\n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    min_df = 5,\n",
    "    max_df = 0.8,\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(1, 3),\n",
    "#    stop_words = all_words,\n",
    "    max_features=30000)\n",
    "word_vectorizer.fit(clean_corpus)\n",
    "features = np.array(word_vectorizer.get_feature_names())\n",
    "train_word_features = word_vectorizer.transform(clean_corpus.iloc[:train_df.shape[0]])\n",
    "test_word_features = word_vectorizer.transform(clean_corpus.iloc[train_df.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22717,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skew in numerical features: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count_words_upper</th>\n",
       "      <td>34.086162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_bad_emoji</th>\n",
       "      <td>32.608676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_good_emoji</th>\n",
       "      <td>20.515688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_punctuations</th>\n",
       "      <td>19.358625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_sent</th>\n",
       "      <td>8.956432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_letters</th>\n",
       "      <td>6.018465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_word</th>\n",
       "      <td>5.932763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_unique_word</th>\n",
       "      <td>3.806975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>0.311152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words_vs_unique</th>\n",
       "      <td>-2.544749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Skew\n",
       "count_words_upper   34.086162\n",
       "n_bad_emoji         32.608676\n",
       "n_good_emoji        20.515688\n",
       "count_punctuations  19.358625\n",
       "count_sent           8.956432\n",
       "count_letters        6.018465\n",
       "count_word           5.932763\n",
       "count_unique_word    3.806975\n",
       "label                0.311152\n",
       "words_vs_unique     -2.544749"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 skewed numerical features to Box Cox transform\n"
     ]
    }
   ],
   "source": [
    "skewness = skewness.loc[abs(skewness.Skew) > 0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    if 'label' not in feat:\n",
    "        df[feat] = boxcox1p(df[feat], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:train_df.shape[0]]\n",
    "test_df = df.iloc[train_df.shape[0]:]\n",
    "\n",
    "y_train = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUED_COLS = ['id', 'comment', 'label']\n",
    "static_cols = [c for c in train_df.columns if not c in EXCLUED_COLS]\n",
    "X_train_static = train_df[static_cols].values\n",
    "X_test_static = test_df[static_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack([train_word_features, csr_matrix(X_train_static)]).tocsr()\n",
    "X_test = hstack([test_word_features, csr_matrix(X_test_static)]).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "\n",
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location):\n",
    "    \"\"\"Returns trained word2vec\n",
    "\n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found w2vmodel\n"
     ]
    }
   ],
   "source": [
    "w2vec = get_word2vec(\n",
    "    MySentences(\n",
    "        train_df['comment'].values, \n",
    "    ),\n",
    "    'w2vmodel'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\n",
    "trainDataVecs = mean_embedding_vectorizer.fit_transform(train_df['comment'])\n",
    "testDataVecs = mean_embedding_vectorizer.fit_transform(test_df['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest to training data....\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "    \n",
    "print(\"Fitting random forest to training data....\")    \n",
    "forest = forest.fit(trainDataVecs, train_df[\"label\"])\n",
    "\n",
    "result = forest.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack([trainDataVecs, csr_matrix(X_train_static)]).tocsr()\n",
    "X_test = hstack([testDataVecs, csr_matrix(X_test_static)]).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StackNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "x = X_train\n",
    "test_x = X_test\n",
    "\n",
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=2, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r\n",
    "\n",
    "preds = np.zeros((len(test_df), 1))\n",
    "\n",
    "label_cols = ['label']\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    m,r = get_mdl(train_df[j])\n",
    "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb1 = lgb.LGBMClassifier(max_depth=16, n_estimators=400,learning_rate=0.05,colsample_bytree=0.3,num_leaves=250,\n",
    "                          metric='auc',objective='binary', n_jobs=-1)\n",
    "lgb2 = lgb.LGBMClassifier(max_depth=17, n_estimators=400,learning_rate=0.05,colsample_bytree=0.3,num_leaves=80,\n",
    "                          metric='f1',objective='binary', n_jobs=-1)\n",
    "xgb1 = XGBClassifier(learning_rate =0.05,n_estimators=400,max_depth = 16,min_child_weight=3,gamma=0.5,subsample=0.8,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=13,\n",
    "                     metric='f1')\n",
    "xgb2 = XGBClassifier(learning_rate =0.04,n_estimators=400,max_depth = 15,min_child_weight=3,gamma=0.7,subsample=1,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=13,\n",
    "                     metric='f1')\n",
    "lr1 = LogisticRegression(C = 1.2, random_state=1)\n",
    "lr2 = LogisticRegression(C = 1.3, random_state=2019)\n",
    "rf1 = RandomForestClassifier (n_estimators=400, criterion=\"entropy\", max_depth=7, max_features=0.5, random_state=3)\n",
    "rf2 = RandomForestClassifier(n_estimators=400, criterion=\"entropy\", max_depth=6, max_features=0.5, random_state=1)\n",
    "et1 = ExtraTreesClassifier(n_estimators=400, criterion=\"entropy\", max_depth=6, max_features=0.5, random_state=2)\n",
    "gbc1 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=5, max_features=0.5, random_state=1)\n",
    "gbc2 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=6, max_features=0.5, random_state=2019)\n",
    "dt1 = DecisionTreeClassifier(criterion='gini', max_depth=15, min_samples_split=2, max_features=0.5, random_state=1)\n",
    "cat1 = CatBoostClassifier(learning_rate=0.1, iterations=400, random_seed=1, logging_level='Silent')\n",
    "knn1 = KNeighborsClassifier(n_neighbors=5, leaf_size=30, p=2, n_jobs=-1,) \n",
    "svm1 = svm.SVC(C = 20.0, kernel='rbf', random_state = 1)\n",
    "\n",
    "models=[[lgb1, xgb1, lr1, rf1, et1, dt1, knn1],[rf2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Start of Level 0 ======================\n",
      "Input Dimensionality 22726 at Level 0 \n",
      "7 models included in Level 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 1/5 , model 0 , f1===0.876556 \n",
      "Level 0, fold 1/5 , model 1 , f1===0.876574 \n",
      "Level 0, fold 1/5 , model 2 , f1===0.879972 \n",
      "Level 0, fold 1/5 , model 3 , f1===0.822979 \n",
      "Level 0, fold 1/5 , model 4 , f1===0.735008 \n",
      "Level 0, fold 1/5 , model 5 , f1===0.820343 \n",
      "Level 0, fold 1/5 , model 6 , f1===0.768705 \n",
      "=========== end of fold 1 in level 0 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 2/5 , model 0 , f1===0.881164 \n",
      "Level 0, fold 2/5 , model 1 , f1===0.878658 \n",
      "Level 0, fold 2/5 , model 2 , f1===0.889281 \n",
      "Level 0, fold 2/5 , model 3 , f1===0.830664 \n",
      "Level 0, fold 2/5 , model 4 , f1===0.751608 \n",
      "Level 0, fold 2/5 , model 5 , f1===0.826424 \n",
      "Level 0, fold 2/5 , model 6 , f1===0.768283 \n",
      "=========== end of fold 2 in level 0 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 3/5 , model 0 , f1===0.889460 \n",
      "Level 0, fold 3/5 , model 1 , f1===0.887826 \n",
      "Level 0, fold 3/5 , model 2 , f1===0.891892 \n",
      "Level 0, fold 3/5 , model 3 , f1===0.827824 \n",
      "Level 0, fold 3/5 , model 4 , f1===0.758845 \n",
      "Level 0, fold 3/5 , model 5 , f1===0.827799 \n",
      "Level 0, fold 3/5 , model 6 , f1===0.763894 \n",
      "=========== end of fold 3 in level 0 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 4/5 , model 0 , f1===0.891806 \n",
      "Level 0, fold 4/5 , model 1 , f1===0.885704 \n",
      "Level 0, fold 4/5 , model 2 , f1===0.888571 \n",
      "Level 0, fold 4/5 , model 3 , f1===0.827285 \n",
      "Level 0, fold 4/5 , model 4 , f1===0.742903 \n",
      "Level 0, fold 4/5 , model 5 , f1===0.830315 \n",
      "Level 0, fold 4/5 , model 6 , f1===0.761868 \n",
      "=========== end of fold 4 in level 0 ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0, fold 5/5 , model 0 , f1===0.876830 \n",
      "Level 0, fold 5/5 , model 1 , f1===0.876033 \n",
      "Level 0, fold 5/5 , model 2 , f1===0.885887 \n",
      "Level 0, fold 5/5 , model 3 , f1===0.822575 \n",
      "Level 0, fold 5/5 , model 4 , f1===0.741039 \n",
      "Level 0, fold 5/5 , model 5 , f1===0.825670 \n",
      "Level 0, fold 5/5 , model 6 , f1===0.755827 \n",
      "=========== end of fold 5 in level 0 ===========\n",
      "Level 0, model 0 , f1===0.883163 \n",
      "Level 0, model 1 , f1===0.880959 \n",
      "Level 0, model 2 , f1===0.887121 \n",
      "Level 0, model 3 , f1===0.826266 \n",
      "Level 0, model 4 , f1===0.745881 \n",
      "Level 0, model 5 , f1===0.826110 \n",
      "Level 0, model 6 , f1===0.763716 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimensionality of level 0 is 7 \n",
      "====================== End of Level 0 ======================\n",
      " level 0 lasted 1790.168904 seconds \n",
      "====================== Start of Level 1 ======================\n",
      "Input Dimensionality 7 at Level 1 \n",
      "1 models included in Level 1 \n",
      "Level 1, fold 1/5 , model 0 , f1===0.889514 \n",
      "=========== end of fold 1 in level 1 ===========\n",
      "Level 1, fold 2/5 , model 0 , f1===0.888264 \n",
      "=========== end of fold 2 in level 1 ===========\n",
      "Level 1, fold 3/5 , model 0 , f1===0.899202 \n",
      "=========== end of fold 3 in level 1 ===========\n",
      "Level 1, fold 4/5 , model 0 , f1===0.900812 \n",
      "=========== end of fold 4 in level 1 ===========\n",
      "Level 1, fold 5/5 , model 0 , f1===0.890780 \n",
      "=========== end of fold 5 in level 1 ===========\n",
      "Level 1, model 0 , f1===0.893715 \n",
      "Output dimensionality of level 1 is 1 \n",
      "====================== End of Level 1 ======================\n",
      " level 1 lasted 69.714178 seconds \n",
      "====================== End of fit ======================\n",
      " fit() lasted 1859.894078 seconds \n",
      "====================== Start of Level 0 ======================\n",
      "1 estimators included in Level 0 \n",
      "====================== Start of Level 1 ======================\n",
      "1 estimators included in Level 1 \n"
     ]
    }
   ],
   "source": [
    "from pystacknet.pystacknet import StackNetClassifier\n",
    "\n",
    "model = StackNetClassifier(\n",
    "    models, metric=\"f1\", \n",
    "    folds=5,\n",
    "    restacking=False, \n",
    "    use_retraining=True, \n",
    "    use_proba=True, \n",
    "    random_state=12345, n_jobs=1, verbose=1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds=model.predict_proba(X_test) ## 888370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cls = np.argmax(preds, axis=1)\n",
    "pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission['label'] = pred_cls\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb1 = lgb.LGBMClassifier(max_depth=16, n_estimators=400,learning_rate=0.05,colsample_bytree=0.3,num_leaves=120,\n",
    "                          metric='f1',objective='binary', n_jobs=-1)\n",
    "lgb2 = lgb.LGBMClassifier(max_depth=17, n_estimators=400,learning_rate=0.04,colsample_bytree=0.3,num_leaves=200,\n",
    "                          metric='f1',objective='binary', n_jobs=-1)\n",
    "xgb1 = XGBClassifier(learning_rate =0.05,n_estimators=400,max_depth = 10,min_child_weight=3,gamma=0,subsample=0.8,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=13,\n",
    "                     metric='f1')\n",
    "lr1 = LogisticRegression(C = 1.2, random_state=1)\n",
    "lr2 = LogisticRegression(C = 1.3, random_state=2019)\n",
    "rf1 = RandomForestClassifier (n_estimators=400, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=3)\n",
    "rf2 = RandomForestClassifier(n_estimators=400, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1)\n",
    "et1 = ExtraTreesClassifier(n_estimators=400, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=2)\n",
    "gbc1 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=5, max_features=0.5, random_state=1)\n",
    "gbc2 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=6, max_features=0.5, random_state=2019)\n",
    "dt1 = DecisionTreeClassifier(criterion='gini', max_depth=15, min_samples_split=2, max_features=0.5, random_state=1)\n",
    "cat1 = CatBoostClassifier(learning_rate=0.1, iterations=400, random_seed=1, logging_level='Silent')\n",
    "knn1 = KNeighborsClassifier(n_neighbors=5, leaf_size=100, p=2, n_jobs=-1,) \n",
    "svm1 = svm.SVC(C = 20.0, kernel='rbf', random_state = 1, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_models = [dt1, gbc1, rf1, lgb1, xgb1, lr1, et1, lgb2, lr2, gbc2]\n",
    "layer1_names = ['dt1', 'gbc1', 'rf1', 'lgb1', 'xgb1', 'lr1', 'et1', 'lgb2', 'lr2', 'gbc2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer1_models = [lgb1, xgb1]\n",
    "# layer1_names = ['lgb1','xgb1']\n",
    "\n",
    "X_train = hstack([trainDataVecs, train_word_features, csr_matrix(X_train_static)]).tocsr()\n",
    "X_test = hstack([testDataVecs, test_word_features, csr_matrix(X_test_static)]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_train = np.zeros(shape=(len(train_df),len(layer1_models)))\n",
    "oof_test = np.zeros(shape=(len(test_df),len(layer1_models)))\n",
    "\n",
    "# Recording results\n",
    "layer1_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training dt1\n",
      "Fold no 1\n",
      "Fold no 2\n",
      "Fold no 3\n",
      "Fold no 4\n",
      "Fold no 5\n",
      "Training CV score: 0.80461\n",
      "\n",
      "\n",
      "Training gbc1\n",
      "Fold no 1\n",
      "Fold no 2\n",
      "Fold no 3\n",
      "Fold no 4\n",
      "Fold no 5\n",
      "Training CV score: 0.87400\n",
      "\n",
      "\n",
      "Training rf1\n",
      "Fold no 1\n",
      "Fold no 2\n",
      "Fold no 3\n",
      "Fold no 4\n",
      "Fold no 5\n",
      "Training CV score: 0.83836\n",
      "\n",
      "\n",
      "Training lgb1\n",
      "Fold no 1\n",
      "Fold no 2\n",
      "Fold no 3\n",
      "Fold no 4\n",
      "Fold no 5\n",
      "Training CV score: 0.87859\n",
      "\n",
      "\n",
      "Training xgb1\n",
      "Fold no 1\n",
      "[0]\tvalidation_0-error:0.12332\tvalidation_1-error:0.168738\n",
      "[200]\tvalidation_0-error:0.028674\tvalidation_1-error:0.108763\n",
      "[399]\tvalidation_0-error:0.003419\tvalidation_1-error:0.106899\n",
      "Fold no 2\n",
      "[0]\tvalidation_0-error:0.118191\tvalidation_1-error:0.176507\n",
      "[200]\tvalidation_0-error:0.026653\tvalidation_1-error:0.10752\n",
      "[399]\tvalidation_0-error:0.00272\tvalidation_1-error:0.106899\n",
      "Fold no 3\n",
      "[0]\tvalidation_0-error:0.126962\tvalidation_1-error:0.178427\n",
      "[200]\tvalidation_0-error:0.028904\tvalidation_1-error:0.104134\n",
      "[399]\tvalidation_0-error:0.004196\tvalidation_1-error:0.104756\n",
      "Fold no 4\n",
      "[0]\tvalidation_0-error:0.12129\tvalidation_1-error:0.171899\n",
      "[200]\tvalidation_0-error:0.028361\tvalidation_1-error:0.105378\n",
      "[399]\tvalidation_0-error:0.003263\tvalidation_1-error:0.102891\n",
      "Fold no 5\n",
      "[0]\tvalidation_0-error:0.130925\tvalidation_1-error:0.181225\n",
      "[200]\tvalidation_0-error:0.027195\tvalidation_1-error:0.11004\n",
      "[399]\tvalidation_0-error:0.002953\tvalidation_1-error:0.109108\n",
      "Training CV score: 0.87813\n",
      "\n",
      "\n",
      "Training lr1\n",
      "Fold no 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold no 2\n",
      "Fold no 3\n",
      "Fold no 4\n",
      "Fold no 5\n",
      "Training CV score: 0.88087\n",
      "\n",
      "\n",
      "Training et1\n",
      "Fold no 1\n",
      "Fold no 2\n",
      "Fold no 3\n",
      "Fold no 4\n",
      "Fold no 5\n",
      "Training CV score: 0.83705\n",
      "\n",
      "\n",
      "Training lgb2\n",
      "Fold no 1\n",
      "Fold no 2\n",
      "Fold no 3\n",
      "Fold no 4\n",
      "Fold no 5\n",
      "Training CV score: 0.87853\n",
      "\n",
      "\n",
      "Training lr2\n",
      "Fold no 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen phu hien\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold no 2\n",
      "Fold no 3\n",
      "Fold no 4\n",
      "Fold no 5\n",
      "Training CV score: 0.88128\n",
      "\n",
      "\n",
      "Training gbc2\n",
      "Fold no 1\n",
      "Fold no 2\n",
      "Fold no 3\n",
      "Fold no 4\n",
      "Fold no 5\n",
      "Training CV score: 0.87318\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(layer1_models)):\n",
    "    print('\\n')\n",
    "    name = layer1_names[i]\n",
    "    model = layer1_models[i]\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "    print('Training %s' %name)\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "        print('Fold no %i'%(fold_+1))\n",
    "        trn_data = X_train[trn_idx]\n",
    "        trn_label = y_train[trn_idx]\n",
    "        val_data = X_train[val_idx]\n",
    "        val_label = y_train[val_idx]\n",
    "        if ('lgb' in name) or ('xgb' in name):\n",
    "            model.fit(X=trn_data, y=trn_label,\n",
    "                     eval_set=[(trn_data, trn_label), (val_data, val_label)],\n",
    "                     verbose=200)\n",
    "        else:\n",
    "            model.fit(X=trn_data, y=trn_label)\n",
    "        \n",
    "        oof_train[val_idx,i] = model.predict(val_data)\n",
    "        oof_test[:,i] += model.predict(X_test)/5\n",
    "        \n",
    "    score = f1_score(oof_train[:,i], y_train)\n",
    "    layer1_score.append(score)\n",
    "    print('Training CV score: %.5f' %score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2_models = [gbc1]\n",
    "layer2_names = ['gbc1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to record result\n",
    "train_pred = np.zeros(shape=(len(train_df),2))\n",
    "test_pred = np.zeros(shape=(len(test_df),2))\n",
    "\n",
    "layer2_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training gbc1\n",
      "Training score: 0.89246\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(layer2_models)):\n",
    "    print('\\n')\n",
    "    name = layer2_names[i]\n",
    "    model = layer2_models[i]\n",
    "    print('Training %s' %name)\n",
    "    model.fit(oof_train, y_train)\n",
    "    score = f1_score(model.predict_proba(oof_train).argmax(axis=1), y_train)\n",
    "    train_pred += model.predict_proba(oof_train)/len(layer2_models)\n",
    "    test_pred += model.predict_proba(oof_test)/len(layer2_models)\n",
    "    layer2_score.append(score)\n",
    "    print('Training score: %.5f' % score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission['label'] = test_pred\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "models = [\n",
    "    RandomForestClassifier (n_estimators=300, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n",
    "    ExtraTreesClassifier (n_estimators=300, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n",
    "    GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=5, max_features=0.5, random_state=1),\n",
    "    LogisticRegression(random_state=1)\n",
    "]\n",
    "\n",
    "def cross_val_and_predict(clf, X, y, X_test, nfolds):\n",
    "    kf = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=42)\n",
    "    \n",
    "    oof_preds = np.zeros((X.shape[0], 2))\n",
    "    sub_preds = np.zeros((X_test.shape[0], 2))\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        oof_preds[valid_idx] = clf.predict_proba(X_valid)\n",
    "        sub_preds += clf.predict_proba(X_test) / kf.n_splits\n",
    "        \n",
    "    return oof_preds, sub_preds\n",
    "\n",
    "sub_preds = []\n",
    "\n",
    "for clf in models:\n",
    "    oof_pred, sub_pred = cross_val_and_predict(clf, X_train, y_train, X_test, nfolds=5)\n",
    "    oof_pred_cls = oof_pred.argmax(axis=1)\n",
    "    oof_f1 = f1_score(y_pred=oof_pred_cls, y_true=y_train)\n",
    "    \n",
    "    print(clf.__class__)\n",
    "    print(f\"F1 CV: {oof_f1}\")\n",
    "    \n",
    "    sub_preds.append(sub_pred)\n",
    "\n",
    "sub_preds = np.asarray(sub_preds)\n",
    "sub_preds = sub_preds.mean(axis=0)\n",
    "sub_pred_cls = sub_preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.where((np.round(preds.ravel()) + sub_pred_cls + pred_cls) >= 2, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.65, 'learning_rate': 0.05, 'max_depth': 10, 'metric': 'f1', 'n_estimators': 600, 'num_leaves': 80, 'objective': 'binary', 'random_state': 1, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# Create parameters to search\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'binary',\n",
    "          'nthread': 3, # Updated from nthread\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5,\n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'binary_error'}\n",
    "\n",
    "gridParams = {\n",
    "    'max_depth':[10, 15, 20, 25],\n",
    "    'learning_rate': [0.02, 0.04, 0.05, 0.06],\n",
    "    'n_estimators': [500, 600],\n",
    "    'num_leaves': [80],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'random_state' : [1], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65],\n",
    "    'subsample' : [0.75],\n",
    "    'reg_alpha' : [1],\n",
    "    'reg_lambda' : [1],\n",
    "    'metric':['f1']\n",
    "    }\n",
    "\n",
    "# Create classifier to use. Note that parameters have to be input manually\n",
    "# not as a dict!\n",
    "mdl = lgb.LGBMClassifier(boosting_type= 'gbdt',\n",
    "          objective = 'binary',\n",
    "          n_jobs = -1, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'],\n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'],\n",
    "          subsample_freq = params['subsample_freq'],\n",
    "          min_split_gain = params['min_split_gain'],\n",
    "          min_child_weight = params['min_child_weight'],\n",
    "          min_child_samples = params['min_child_samples'],\n",
    "          scale_pos_weight = params['scale_pos_weight'])\n",
    "\n",
    "# To view the default model params:\n",
    "mdl.get_params().keys()\n",
    "\n",
    "# Create the grid\n",
    "grid = GridSearchCV(mdl, gridParams,\n",
    "                    verbose=0,\n",
    "                    cv=4,\n",
    "                    n_jobs=-1)\n",
    "# Run the grid\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best hyperparameters:\n",
      "{'max_depth': 16}\n"
     ]
    }
   ],
   "source": [
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        'max_depth': [16, 18, 20, 22]\n",
    "        }\n",
    "\n",
    "xgb1 = XGBClassifier(learning_rate =0.05,n_estimators=300,max_depth = 16,min_child_weight=3,gamma=0.5,subsample=0.8,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=1,scale_pos_weight=1,seed=13,\n",
    "                     metric='f1')\n",
    "\n",
    "folds = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "grid = GridSearchCV(estimator=xgb1, param_grid=params, scoring='f1', n_jobs=4, cv=skf.split(X_train,y_train), verbose=1 )\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('\\n Best hyperparameters:')\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
